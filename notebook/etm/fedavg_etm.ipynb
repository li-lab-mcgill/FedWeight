{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db612acbe5a33d2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:46:12.668924Z",
     "start_time": "2024-12-08T07:46:12.126580Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mikezhu\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84ede4655acc492e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:47:33.761606Z",
     "start_time": "2024-12-08T07:46:12.641628Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change the directory to your scratch folder\n",
    "os.chdir('/scratch/mikezhu/fed_weight_jupyter/notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0fc06e1e8c7f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:47:33.777302Z",
     "start_time": "2024-12-08T07:47:33.776885Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/mikezhu/fed_weight_jupyter/notebook\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8342b9517e5842",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:47:33.778939Z",
     "start_time": "2024-12-08T07:47:33.777104Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f00c7ce3947d69f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:47:33.779852Z",
     "start_time": "2024-12-08T07:47:33.777169Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_hidden_size = 512\n",
    "rho_size = 512\n",
    "num_topics = 64\n",
    "enc_drop = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b091c2826a7dfb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:47:38.506972Z",
     "start_time": "2024-12-08T07:47:33.777242Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x14a08e40d050>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mikezhu/venv/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m icd_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/eicu_mimic_patient_diagnosis.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m hospital_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2001\u001b[39m, \u001b[38;5;241m1001\u001b[39m]\n\u001b[1;32m     14\u001b[0m readmit_interval_threshold \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m2001\u001b[39m: \u001b[38;5;241m180\u001b[39m, \u001b[38;5;66;03m# Whether MIMIC patients will readmit to hospital within 180 days\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m1001\u001b[39m: \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# Whether eICU patients will readmit to ICU within 2 days\u001b[39;00m\n\u001b[1;32m     17\u001b[0m }\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "icd_data = pd.read_csv(\"../data/eicu_mimic_patient_diagnosis.csv\")\n",
    "\n",
    "hospital_ids = [2001, 1001]\n",
    "\n",
    "readmit_interval_threshold = {\n",
    "    2001: 180, # Whether MIMIC patients will readmit to hospital within 180 days\n",
    "    1001: 2 # Whether eICU patients will readmit to ICU within 2 days\n",
    "}\n",
    "\n",
    "train_loaders = {}\n",
    "train_icds = {}\n",
    "test_icds = {}\n",
    "x_bow_tests = {}\n",
    "\n",
    "train_readmit_row_ids = {}\n",
    "test_readmit_row_ids = {}\n",
    "\n",
    "train_label_deaths = {}\n",
    "test_label_deaths = {}\n",
    "\n",
    "train_label_readmit = {}\n",
    "test_label_readmit = {}\n",
    "\n",
    "for hospital_id in hospital_ids:\n",
    "    \n",
    "    hospital_data = icd_data[icd_data[\"hospitalid\"] == hospital_id]\n",
    "    train_data, test_data = train_test_split(hospital_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    x_train = train_data.iloc[:, 4:].to_numpy()\n",
    "    x_test = test_data.iloc[:, 4:].to_numpy()\n",
    "    \n",
    "    print(\"Hospital ID:\", hospital_id)\n",
    "    print(\"Train data shape:\", x_train.shape)\n",
    "    \n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(x_train_tensor, x_train_tensor)  # Use the same tensor for inputs and targets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "    train_loaders[hospital_id] = train_loader\n",
    "    \n",
    "    train_icds[hospital_id] = x_train_tensor\n",
    "    test_icds[hospital_id] = x_test_tensor\n",
    "    \n",
    "    # Bag of words\n",
    "    x_bow_test = []\n",
    "    for row in x_test:\n",
    "        word_id = list(np.where(row == 1)[0])\n",
    "        x_bow_test.append(word_id)\n",
    "    x_bow_tests[hospital_id] = x_bow_test\n",
    "    \n",
    "    # Readmission patients row ids\n",
    "    train_data_np = train_data.to_numpy()\n",
    "    train_readmit_patients_row_ids = np.where(train_data_np[:, 3] == 1)[0]\n",
    "    train_readmit_row_ids[hospital_id] = train_readmit_patients_row_ids\n",
    "    \n",
    "    test_data_np = test_data.to_numpy()\n",
    "    test_readmit_patients_row_ids = np.where(test_data_np[:, 3] == 1)[0]\n",
    "    test_readmit_row_ids[hospital_id] = test_readmit_patients_row_ids\n",
    "    \n",
    "    # Label death in readmission\n",
    "    y_death_train = train_data.iloc[train_readmit_patients_row_ids, 2].to_numpy()\n",
    "    y_death_test = test_data.iloc[test_readmit_patients_row_ids, 2].to_numpy()\n",
    "    train_label_deaths[hospital_id] = y_death_train\n",
    "    test_label_deaths[hospital_id] = y_death_test\n",
    "    \n",
    "    # # Label readmission interval\n",
    "    # y_readmit_train = train_data.iloc[train_readmit_patients_row_ids, 3].to_numpy()\n",
    "    # y_readmit_test = test_data.iloc[test_readmit_patients_row_ids, 3].to_numpy()\n",
    "    # \n",
    "    # threshold = readmit_interval_threshold[hospital_id]\n",
    "    # \n",
    "    # y_readmit_train = np.where(y_readmit_train <= threshold, 1, 0)\n",
    "    # y_readmit_test = np.where(y_readmit_test <= threshold, 1, 0)\n",
    "    # \n",
    "    # train_label_readmit[hospital_id] = y_readmit_train\n",
    "    # test_label_readmit[hospital_id] = y_readmit_test\n",
    "\n",
    "\n",
    "icd_code_names = icd_data.columns[4:]\n",
    "icd_code_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049fc421c35add1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:47:38.894621Z",
     "start_time": "2024-12-08T07:47:38.510186Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_data_total = icd_data\n",
    "x_train = train_data_total.iloc[:, 4:].to_numpy()\n",
    "x_bow_train = []\n",
    "for row in x_train:\n",
    "    word_id = list(np.where(row == 1)[0])\n",
    "    x_bow_train.append(word_id)\n",
    "\n",
    "# Train Word2Vec embeddings\n",
    "# word2vec_model = Word2Vec(sentences=x_bow_train, vector_size=rho_size, window=5, min_count=1, sg=1)\n",
    "# pretrained_rho = word2vec_model.wv.vectors\n",
    "# pretrained_rho_tensor = torch.tensor(pretrained_rho, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19173aedff8bde0a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.522535Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_icd9_to_disease(icd_9):\n",
    "    if pd.isna(icd_9):\n",
    "        return \"Others\"\n",
    "    primary_icd9 = icd_9.split(',')[0].strip()\n",
    "    try:\n",
    "        # Convert the input to a float to handle both numeric and decimal ICD-9 codes\n",
    "        icd_9_float = float(primary_icd9)\n",
    "\n",
    "        # Check the ICD-9 code against the known ranges\n",
    "        if 1 <= icd_9_float <= 139.9:\n",
    "            return \"Infection\"\n",
    "        elif 140 <= icd_9_float <= 239.9:\n",
    "            return \"Neoplasms\"\n",
    "        elif 240 <= icd_9_float <= 279.9:\n",
    "            return \"Endocrine\"\n",
    "        elif 280 <= icd_9_float <= 289.9:\n",
    "            return \"Blood\"\n",
    "        elif 290 <= icd_9_float <= 319:\n",
    "            return \"Mental\"\n",
    "        elif 320 <= icd_9_float <= 389.9:\n",
    "            return \"Nervous\"\n",
    "        elif 390 <= icd_9_float <= 459.9:\n",
    "            return \"Circulatory\"\n",
    "        elif 460 <= icd_9_float <= 519.9:\n",
    "            return \"Respiratory\"\n",
    "        elif 520 <= icd_9_float <= 579.9:\n",
    "            return \"Digestive\"\n",
    "        elif 580 <= icd_9_float <= 629.9:\n",
    "            return \"Genitourinary\"\n",
    "        elif 630 <= icd_9_float <= 676.9:\n",
    "            return \"Pregnancy\"\n",
    "        elif 680 <= icd_9_float <= 709.9:\n",
    "            return \"Skin\"\n",
    "        elif 710 <= icd_9_float <= 739.9:\n",
    "            return \"Musculoskeletal\"\n",
    "        elif 740 <= icd_9_float <= 759.9:\n",
    "            return \"Congenital\"\n",
    "        elif 760 <= icd_9_float <= 799.9:\n",
    "            return \"Perinatal\"\n",
    "        elif 800 <= icd_9_float <= 1000:\n",
    "            return \"Poisoning\"\n",
    "        elif icd_9.startswith(\"V\"):\n",
    "            return \"Others\"\n",
    "        else:\n",
    "            return \"Others\"\n",
    "    \n",
    "    except ValueError:\n",
    "        return \"Others\"\n",
    "\n",
    "disease_color_map = {\n",
    "    \"Infection\": \"#005896\",\n",
    "    \"Neoplasms\": \"#dc5f00\",      # SteelBlue\n",
    "    \"Endocrine\": \"#008002\",      # LimeGreen\n",
    "    \"Blood\": \"#b40005\",          # Crimson\n",
    "    \"Mental\": \"#74499c\",         # DarkViolet\n",
    "    \"Nervous\": \"#6c382e\",        # Gold\n",
    "    \"Circulatory\": \"#ab3db3\",    # OrangeRed\n",
    "    \"Respiratory\": \"#2e2e2e\",    # DarkTurquoise\n",
    "    \"Digestive\": \"#9c9c00\",      # DeepPink\n",
    "    \"Genitourinary\": \"#009eac\",  # MediumSlateBlue\n",
    "    \"Pregnancy\": \"#abcc25\",      # HotPink\n",
    "    \"Skin\": \"#f06e60\",           # SaddleBrown\n",
    "    \"Musculoskeletal\": \"#3bd156\",# DarkOliveGreen\n",
    "    \"Congenital\": \"#c7b228\",     # BlueViolet\n",
    "    \"Perinatal\": \"#ff5c7c\",      # IndianRed\n",
    "    \"Poisoning\": \"#1268fd\",      # DarkOrange\n",
    "    \"Others\": \"#696969\",         # DimGray\n",
    "    \"Unknown\": \"#808080\"         # Gray\n",
    "}\n",
    "\n",
    "hospital_color_map = {\n",
    "    1001: \"#1268fd\",\n",
    "    2001: \"#ff5c7c\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301a501c36f68d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.524148Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "icd_code_dict = dict() # Key: disease category, Value: list of ICD codes\n",
    "for icd_code in icd_code_names:\n",
    "    disease = convert_icd9_to_disease(icd_code)\n",
    "    if disease in icd_code_dict:\n",
    "        icd_code_dict[disease].append(icd_code)\n",
    "    else:\n",
    "        icd_code_dict[disease] = [icd_code]\n",
    "\n",
    "icd_code_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae80820b96d31ce2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.525707Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patient_icd_data = icd_data.iloc[:, 4:]\n",
    "\n",
    "total_feature_sum_dict = {}\n",
    "for feature in patient_icd_data.columns:\n",
    "    \n",
    "    feature_sum = patient_icd_data[feature].sum()\n",
    "    feature_name = convert_icd9_to_disease(feature)\n",
    "    print(f\"{feature_name}: {feature_sum}\")\n",
    "    \n",
    "    if feature_name in total_feature_sum_dict:\n",
    "        total_feature_sum_dict[feature_name] += feature_sum\n",
    "    else:\n",
    "        total_feature_sum_dict[feature_name] = feature_sum\n",
    "        \n",
    "print(total_feature_sum_dict)\n",
    "\n",
    "total_feature_sum_list = []\n",
    "for feature in patient_icd_data.columns:\n",
    "    \n",
    "    feature_name = convert_icd9_to_disease(feature)\n",
    "    feature_sum = total_feature_sum_dict[feature_name]\n",
    "    total_feature_sum_list.append(feature_sum)\n",
    "    \n",
    "feature_sums_tensor = torch.tensor(total_feature_sum_list)\n",
    "feature_sums_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add7c76ef993262",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.525850Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_common_icds(input, feature_sums_tensor):\n",
    "    \n",
    "    most_common_icd_names = []\n",
    "    least_common_icd_names = []\n",
    "\n",
    "    for row in input:\n",
    "        \n",
    "        active_indices = (row == 1).nonzero(as_tuple=True)[0].cpu().numpy()\n",
    "\n",
    "        if len(active_indices) == 0:\n",
    "            least_common_icd_names.append(\"Others\")\n",
    "        else:\n",
    "            active_sums = feature_sums_tensor[active_indices]\n",
    "            _, max_idx = torch.max(active_sums, dim=0)\n",
    "            most_common_feature_idx = active_indices[max_idx]\n",
    "            most_common_feature_icd = patient_icd_data.columns[most_common_feature_idx.item()]\n",
    "            most_common_feature_name = convert_icd9_to_disease(most_common_feature_icd)\n",
    "\n",
    "            most_common_icd_names.append(most_common_feature_name)\n",
    "\n",
    "            _, min_idx = torch.min(active_sums, dim=0)\n",
    "            least_common_feature_idx = active_indices[min_idx]\n",
    "            least_common_feature_icd = patient_icd_data.columns[least_common_feature_idx.item()]\n",
    "            least_common_feature_name = convert_icd9_to_disease(least_common_feature_icd)\n",
    "\n",
    "            least_common_icd_names.append(least_common_feature_name)\n",
    "\n",
    "    return most_common_icd_names, least_common_icd_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac2336eb00f77d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.543111Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_topic_diversity(beta, topk):\n",
    "    num_topics = beta.shape[0]\n",
    "    list_w = np.zeros((num_topics, topk))\n",
    "    for k in range(num_topics):\n",
    "        idx = beta[k, :].argsort()[-topk:][::-1]\n",
    "        list_w[k, :] = idx\n",
    "    n_unique = len(np.unique(list_w))\n",
    "    TD = n_unique / (topk * num_topics)\n",
    "    return TD\n",
    "\n",
    "def get_topic_coherence(beta, data, topk):\n",
    "    D = len(data)  ## number of docs...data is list of documents\n",
    "    TC = []\n",
    "    num_topics = len(beta)\n",
    "    counter = 0\n",
    "    for k in range(num_topics):\n",
    "        top_10 = list(beta[k].argsort()[-topk:][::-1])\n",
    "        TC_k = 0\n",
    "        for i, word in enumerate(top_10):\n",
    "            # get D(w_i)\n",
    "            D_wi = get_document_frequency(data, word)\n",
    "            j = i + 1\n",
    "            tmp = 0\n",
    "            while j < len(top_10) and j > i:\n",
    "                # get D(w_j) and D(w_i, w_j)\n",
    "                D_wj, D_wi_wj = get_document_frequency(data, word, top_10[j])\n",
    "                # get f(w_i, w_j)\n",
    "                if D_wi_wj == 0:\n",
    "                    f_wi_wj = -1\n",
    "                else:\n",
    "                    f_wi_wj = -1 + (np.log(D_wi) + np.log(D_wj) - 2.0 * np.log(D)) / (np.log(D_wi_wj) - np.log(D))\n",
    "                # update tmp:\n",
    "                tmp += f_wi_wj\n",
    "                j += 1\n",
    "                counter += 1\n",
    "            # update TC_k\n",
    "            TC_k += tmp\n",
    "        TC.append(TC_k)\n",
    "    TC = np.mean(TC) / counter\n",
    "    TC = (TC + 1) / 2\n",
    "    return TC\n",
    "\n",
    "def get_document_frequency(data, wi, wj=None):\n",
    "    if wj is None:\n",
    "        D_wi = 0\n",
    "        for l in range(len(data)):\n",
    "            doc = data[l]\n",
    "            if wi in doc:\n",
    "                D_wi += 1\n",
    "        return D_wi\n",
    "    D_wj = 0\n",
    "    D_wi_wj = 0\n",
    "    for l in range(len(data)):\n",
    "        doc = data[l]\n",
    "        if wj in doc:\n",
    "            D_wj += 1\n",
    "            if wi in doc:\n",
    "                D_wi_wj += 1\n",
    "    return D_wj, D_wi_wj\n",
    "\n",
    "\n",
    "def top_k_precision(y_true, y_probs, k=3):\n",
    "    top_k_indices = np.argsort(y_probs)[-k:]\n",
    "    true_positives_in_top_k = np.sum(y_true[top_k_indices])\n",
    "    top_k_precision = true_positives_in_top_k / k\n",
    "    return top_k_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5596777575686",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.548948Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def need_aggregate(key):\n",
    "    if key.startswith(\"rho\") or key.startswith(\"q_theta\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def average_weights(client_model_states, client_data_sizes):\n",
    "    \"\"\"Returns the weighted average of the model states provided by each client.\n",
    "\n",
    "    Args:\n",
    "    client_model_states (list): List of model states (dictionaries) from each client.\n",
    "    client_data_sizes (list): List of data sizes for each client, used as weights.\n",
    "\n",
    "    Returns:\n",
    "    dict: The averaged model state dictionary.\n",
    "    \"\"\"\n",
    "    total_data_points = sum(client_data_sizes)\n",
    "    \n",
    "    avg_state = {}\n",
    "    for key, value in client_model_states[0].items():\n",
    "        if need_aggregate(key):\n",
    "            avg_state[key] = torch.zeros_like(value)\n",
    "    \n",
    "    for i, client_state in enumerate(client_model_states):\n",
    "        weight = client_data_sizes[i] / total_data_points\n",
    "        for key in avg_state.keys():\n",
    "            avg_state[key] += client_state[key] * weight\n",
    "    \n",
    "    return avg_state\n",
    "\n",
    "\n",
    "def average_evaluation(evaluations, client_data_sizes):\n",
    "    \"\"\"Returns the weighted average of the evaluation metrics provided by each client.\n",
    "\n",
    "    Args:\n",
    "    evaluations (list): List of evaluation metrics from each client.\n",
    "    client_data_sizes (list): List of data sizes for each client, used as weights.\n",
    "\n",
    "    Returns:\n",
    "    float: The averaged evaluation metric.\n",
    "    \"\"\"\n",
    "    total_data_points = sum(client_data_sizes)\n",
    "    \n",
    "    avg_evaluation = 0.0\n",
    "    \n",
    "    for i, evaluation in enumerate(evaluations):\n",
    "        weight = client_data_sizes[i] / total_data_points\n",
    "        avg_evaluation += evaluation * weight\n",
    "    \n",
    "    return avg_evaluation\n",
    "\n",
    "\n",
    "def load_weights(model, updated_state_dict):\n",
    "    original_state = model.state_dict()\n",
    "\n",
    "    current_state_dict = {}\n",
    "    for key, value in original_state.items():\n",
    "        if need_aggregate(key) and key in updated_state_dict:\n",
    "            # If need_aggregate is True, use the value from state_dict\n",
    "            current_state_dict[key] = updated_state_dict[key]\n",
    "        else:\n",
    "            # Otherwise, use the original model state\n",
    "            current_state_dict[key] = original_state[key]\n",
    "            \n",
    "    model.load_state_dict(current_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bf1a65189fa48",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.554307Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Based on https://arxiv.org/pdf/1706.00359\n",
    "# def topic_diversity_regularizer(topic_matrix):\n",
    "#     num_topics = topic_matrix.size(0)\n",
    "#     \n",
    "#     total_angles = []\n",
    "#     for i in range(num_topics):\n",
    "#         for j in range(i + 1, num_topics):\n",
    "#             \n",
    "#             similarity = F.cosine_similarity(topic_matrix[i], topic_matrix[j], dim=0)\n",
    "#             similarity = torch.clamp(similarity, -1.0, 1.0)\n",
    "#             angles = torch.arccos(similarity)\n",
    "#             \n",
    "#             total_angles.append(angles)\n",
    "#     \n",
    "#     mean_angles = torch.mean(torch.stack(total_angles))\n",
    "#     variance = torch.var(torch.stack(total_angles))\n",
    "#     diversity_penalty = variance - mean_angles\n",
    "#     # diversity_penalty = eps * variance - lambda_ * mean_angles\n",
    "#     \n",
    "#     return diversity_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aca190b106b402",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.555423Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def topic_diversity_regularizer(topic_matrix, threshold=0.1):\n",
    "    # num_topics = topic_matrix.size(0)\n",
    "    # diversity_penalty = 0\n",
    "    # for i in range(num_topics):\n",
    "    #     for j in range(i + 1, num_topics):\n",
    "    #         similarity = F.cosine_similarity(topic_matrix[i], topic_matrix[j], dim=0)\n",
    "    #         diversity_penalty += torch.max(torch.tensor(0.0), similarity - threshold)\n",
    "    # return diversity_penalty\n",
    "    normalized_topic_matrix = F.normalize(topic_matrix, p=2, dim=1)\n",
    "    cosine_sim_matrix = torch.matmul(normalized_topic_matrix, normalized_topic_matrix.t())\n",
    "    mask = torch.eye(cosine_sim_matrix.size(0), device=cosine_sim_matrix.device).bool()\n",
    "    cosine_sim_matrix = cosine_sim_matrix.masked_fill(mask, 0)\n",
    "    diversity_penalty = torch.sum(torch.relu(cosine_sim_matrix - threshold)) / 2\n",
    "    return diversity_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8a6862ff630d2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.580494Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disease_labels = [convert_icd9_to_disease(x) for x in icd_code_names]\n",
    "disease_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed65907f90550b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.584854Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def federated_learning(client_models, client_optimizers, rounds=10, epochs=1, enc_drop=0.5, beta_=0.05, lambda_=0.1):\n",
    "    \n",
    "    elbo_hist = {}\n",
    "    kld_hist = {}\n",
    "    recon_hist = {}\n",
    "    \n",
    "    tc_hist = {}\n",
    "    td_hist = {}\n",
    "    tq_hist = {}\n",
    "    \n",
    "    for client_id in hospital_ids:\n",
    "        \n",
    "        elbo_hist[client_id] = []\n",
    "        kld_hist[client_id] = []\n",
    "        recon_hist[client_id] = []\n",
    "        \n",
    "        tc_hist[client_id] = []\n",
    "        td_hist[client_id] = []\n",
    "        tq_hist[client_id] = []\n",
    "    \n",
    "    for round in range(rounds):\n",
    "\n",
    "        client_model_states = []\n",
    "        client_data_sizes = []\n",
    "\n",
    "        for client_id in hospital_ids:\n",
    "            \n",
    "            client_loader = train_loaders[client_id]\n",
    "            client_model = client_models[client_id]\n",
    "            client_optimizer = client_optimizers[client_id]\n",
    "            \n",
    "            client_recon_likelihood = 0.0\n",
    "            client_kld = 0.0\n",
    "            client_elbo = 0.0\n",
    "            \n",
    "            client_model.train()\n",
    "\n",
    "            for local_epoch in range(epochs):  # Local training epochs (can increase as needed)\n",
    "                \n",
    "                epoch_recon_likelihood = 0.0\n",
    "                epoch_kld = 0.0\n",
    "                epoch_elbo = 0.0\n",
    "                \n",
    "                for batch_idx, (bows, normalized_bows) in enumerate(client_loader):\n",
    "                    bows = bows.to(device)\n",
    "                    normalized_bows = normalized_bows.to(device)\n",
    "\n",
    "                    client_optimizer.zero_grad()\n",
    "                    recon_loss, kld_theta = client_model(bows, normalized_bows)\n",
    "                    \n",
    "                    # Increase topic diversity\n",
    "                    # Based on paper: https://arxiv.org/pdf/1706.00359\n",
    "                    diversity_penalty = topic_diversity_regularizer(client_model.get_beta())\n",
    "                    \n",
    "                    # Beta-VAE KL Annealing to prevent posterior collapse\n",
    "                    kl_weight = min(1.0, round * beta_)\n",
    "                    elbo_term = recon_loss + kld_theta\n",
    "                    loss = recon_loss + kl_weight * kld_theta + lambda_ * diversity_penalty\n",
    "                    loss.backward()\n",
    "                    client_optimizer.step()\n",
    "                    \n",
    "                    epoch_recon_likelihood += -recon_loss.item()\n",
    "                    epoch_kld += kld_theta.item()\n",
    "                    epoch_elbo += -elbo_term.item()\n",
    "                    \n",
    "                epoch_recon_likelihood /= len(client_loader)\n",
    "                epoch_kld /= len(client_loader)\n",
    "                epoch_elbo /= len(client_loader)\n",
    "                \n",
    "                client_recon_likelihood += epoch_recon_likelihood\n",
    "                client_kld += epoch_kld\n",
    "                client_elbo += epoch_elbo\n",
    "                \n",
    "            client_recon_likelihood /= epochs\n",
    "            client_kld /= epochs\n",
    "            client_elbo /= epochs\n",
    "\n",
    "            client_model_states.append(client_model.state_dict())  # Save client model state\n",
    "            client_data_sizes.append(len(client_loader.dataset))\n",
    "\n",
    "            elbo_hist[client_id].append(client_elbo)\n",
    "            kld_hist[client_id].append(client_kld)\n",
    "            recon_hist[client_id].append(client_recon_likelihood)\n",
    "\n",
    "        # Aggregate the parameters from each client\n",
    "        avg_model_state = average_weights(client_model_states, client_data_sizes)\n",
    "\n",
    "        for client_id in hospital_ids:\n",
    "            \n",
    "            # Load the averaged model back into the client model\n",
    "            client_model = client_models[client_id]\n",
    "            load_weights(client_model, avg_model_state)\n",
    "        \n",
    "            # Evaluate\n",
    "            client_model.eval()\n",
    "            \n",
    "            beta = client_model.get_beta()\n",
    "            beta = beta.data.cpu().numpy()\n",
    "            \n",
    "            x_bow_test = x_bow_tests[client_id]\n",
    "            coherence = get_topic_coherence(beta, x_bow_test, 5)\n",
    "            diversity = get_topic_diversity(beta, 5)\n",
    "            quality = coherence * diversity\n",
    "            \n",
    "            tc_hist[client_id].append(coherence)\n",
    "            td_hist[client_id].append(diversity)\n",
    "            tq_hist[client_id].append(quality)\n",
    "    \n",
    "            print(f\"Round {round + 1}/{rounds} - Client: {client_id} - ELBO: {elbo_hist[client_id][-1]:.4f} - Recon likelihood: {recon_hist[client_id][-1]:.4f} - KLD: {kld_hist[client_id][-1]:.4f}, TC: {coherence:.4f}, TD: {diversity:.4f}, TQ: {quality:.4f}\")\n",
    "        \n",
    "    return client_models, elbo_hist, kld_hist, recon_hist, tc_hist, td_hist, tq_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.587113Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ETM(nn.Module):\n",
    "    def __init__(self, num_topics, vocab_size, t_hidden_size, rho_size, enc_drop=0.5):\n",
    "        super(ETM, self).__init__()\n",
    "\n",
    "        ## define hyperparameters\n",
    "        self.num_topics = num_topics\n",
    "        self.vocab_size = vocab_size\n",
    "        self.t_hidden_size = t_hidden_size\n",
    "        self.rho_size = rho_size\n",
    "        self.enc_drop = enc_drop\n",
    "        self.t_drop = nn.Dropout(enc_drop)\n",
    "        \n",
    "        ## define the word embedding matrix \\rho\n",
    "        self.rho = nn.Linear(rho_size, vocab_size, bias=False)\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        #     self.rho.weight = nn.Parameter(pretrained_rho_tensor.T)\n",
    "        #     self.rho.weight.requires_grad = False\n",
    "\n",
    "        ## define the matrix containing the topic embeddings\n",
    "        self.alphas = nn.Linear(rho_size, num_topics, bias=False)\n",
    "    \n",
    "        ## define variational distribution for \\theta_{1:D} via amortizartion\n",
    "        # print(vocab_size, \" THE Vocabulary size is here \")\n",
    "        self.q_theta = nn.Sequential(\n",
    "                nn.Linear(vocab_size, t_hidden_size),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "        self.mu_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
    "        self.logsigma_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Returns a sample from a Gaussian distribution via reparameterization.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar) \n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul_(std).add_(mu)\n",
    "        else:\n",
    "            # During inference time, there is no need for random sampling. \n",
    "            # Instead, the model can use the mean directly, which is a point estimate of the latent variable\n",
    "            # This avoids unnecessary randomness during inference or testing.\n",
    "            return mu\n",
    "\n",
    "    def encode(self, bows):\n",
    "        \"\"\"Returns paramters of the variational distribution for \\theta.\n",
    "\n",
    "        input: bows\n",
    "                batch of bag-of-words...tensor of shape bsz x V\n",
    "        output: mu_theta, log_sigma_theta\n",
    "        \"\"\"\n",
    "        q_theta = self.q_theta(bows)\n",
    "        if self.enc_drop > 0:\n",
    "            q_theta = self.t_drop(q_theta)\n",
    "        mu_theta = self.mu_q_theta(q_theta)\n",
    "        logsigma_theta = self.logsigma_q_theta(q_theta)\n",
    "        kl_theta = -0.5 * torch.sum(1 + logsigma_theta - mu_theta.pow(2) - logsigma_theta.exp(), dim=-1).mean()\n",
    "        \n",
    "        return mu_theta, logsigma_theta, kl_theta\n",
    "\n",
    "    def get_beta(self):\n",
    "        \"\"\"\n",
    "        This generate the description as a defintion over words\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logit = self.alphas(self.rho.weight) # torch.mm(self.rho, self.alphas)\n",
    "        except:\n",
    "            logit = self.alphas(self.rho)\n",
    "        # logit = self.alphas(self.rho.weight.T)\n",
    "        beta = F.softmax(logit, dim=0).transpose(1, 0) ## softmax over vocab dimension\n",
    "        return beta\n",
    "\n",
    "    def get_theta(self, normalized_bows, is_train=True, d=1.0):\n",
    "        \"\"\"\n",
    "        getting the topic poportion for the document passed in the normalixe bow or tf-idf\"\"\"\n",
    "        mu_theta, logsigma_theta, kld_theta = self.encode(normalized_bows)\n",
    "        z = self.reparameterize(mu_theta, logsigma_theta)\n",
    "        theta = F.softmax(z, dim=-1)\n",
    "        if not is_train:\n",
    "            theta = F.softmax(z / d, dim=-1)\n",
    "        return z, theta, kld_theta\n",
    "\n",
    "    def decode(self, theta, beta):\n",
    "        \"\"\"compute the probability of topic given the document which is equal to theta^T ** B\n",
    "\n",
    "        Args:\n",
    "            theta ([type]): [description]\n",
    "            beta ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        res = torch.mm(theta, beta)\n",
    "        \n",
    "        almost_zeros = torch.full_like(res, 1e-6)\n",
    "        results_without_zeros = res.add(almost_zeros)\n",
    "        predictions = torch.log(results_without_zeros)\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, bows, normalized_bows, theta=None, aggregate=True):\n",
    "        ## get \\theta\n",
    "        if theta is None:\n",
    "            _, theta, kld_theta = self.get_theta(normalized_bows)\n",
    "        else:\n",
    "            kld_theta = None\n",
    "\n",
    "        ## get \\beta\n",
    "        beta = self.get_beta()\n",
    "\n",
    "        ## get prediction loss\n",
    "        preds = self.decode(theta, beta)\n",
    "        recon_loss = -(preds * bows).sum(1)\n",
    "        if aggregate:\n",
    "            recon_loss = recon_loss.mean()\n",
    "        return recon_loss, kld_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92129a05cf04305a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.622622Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client_models = {}\n",
    "client_optimizers = {}\n",
    "\n",
    "for client_id in hospital_ids:\n",
    "\n",
    "    client_model = ETM(num_topics=num_topics,\n",
    "                       vocab_size=len(icd_code_names),\n",
    "                       t_hidden_size=t_hidden_size,\n",
    "                       rho_size=rho_size,\n",
    "                       enc_drop=enc_drop).to(device)\n",
    "    client_models[client_id] = client_model\n",
    "\n",
    "    optimizer_fn = torch.optim.Adam(client_model.parameters(), lr=1e-4, weight_decay=5e-6)\n",
    "    client_optimizers[client_id] = optimizer_fn\n",
    "\n",
    "client_models, elbo_hist, kld_hist, recon_hist, tc_hist, td_hist, tq_hist = federated_learning(client_models, client_optimizers, rounds=20, epochs=5, enc_drop=enc_drop, beta_=0.05, lambda_=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c8591f08fa5340",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.660521Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot ELBO\n",
    "plt.title(\"ELBO of ETM FedAvg (eICU)\")\n",
    "plt.plot(elbo_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3637d26e5d53fea4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.701877Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot ELBO\n",
    "plt.title(\"ELBO of ETM FedAvg of (MIMIC-III)\")\n",
    "plt.plot(elbo_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e28f3894cbf33",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.866752Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Reconstruction Likelihood\n",
    "plt.title(\"Reconstruction Likelihood of ETM FedAvg (eICU)\")\n",
    "plt.plot(recon_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"E[logp(x|z)]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d98956a277be09d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.869006Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Reconstruction Likelihood\n",
    "plt.title(\"Reconstruction Likelihood of ETM FedAvg (MIMIC-III)\")\n",
    "plt.plot(recon_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"E[logp(x|z)]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c3dbf9487a64f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:47:38.994308Z",
     "start_time": "2024-12-08T07:47:38.901843Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot KL\n",
    "plt.title(\"KL[q(z|x) || p(z)] of ETM FedAvg (eICU)\")\n",
    "plt.plot(kld_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"KL[q(z|x) || p(z)]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af848eeeebf30a27",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.913762Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot KL\n",
    "plt.title(\"KL[q(z|x) || p(z)] of ETM FedAvg (MIMIC-III)\")\n",
    "plt.plot(kld_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"KL[q(z|x) || p(z)]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec47f2c3719cbc1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.935323Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Topic Coherence\n",
    "plt.title(\"Topic Coherence of ETM FedAvg (eICU)\")\n",
    "plt.plot(tc_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Coherence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea834cf4c4460e70",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.939028Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Topic Coherence\n",
    "plt.title(\"Topic Coherence of ETM FedAvg (MIMIC-III)\")\n",
    "plt.plot(tc_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Coherence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d359eedccde3dea",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.967706Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Topic Diversity\n",
    "plt.title(\"Topic Diversity of ETM FedAvg (eICU)\")\n",
    "plt.plot(td_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Diversity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25315863dc35043",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:47:39.005331Z",
     "start_time": "2024-12-08T07:47:38.995508Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Topic Diversity\n",
    "plt.title(\"Topic Diversity of ETM FedAvg (MIMIC-III)\")\n",
    "plt.plot(td_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Diversity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b7f77da148117",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:38.997019Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Topic Quality\n",
    "plt.title(\"Topic Quality (Coherence x Diversity) of ETM FedAvg (eICU)\")\n",
    "plt.plot(tq_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Quality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45b86f023c37d3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.022545Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Topic Quality\n",
    "plt.title(\"Topic Quality (Coherence x Diversity) of ETM FedAvg (MIMIC-III)\")\n",
    "plt.plot(tq_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Quality\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1e158035f4d3d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.029501Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "eicu_client_model = client_models[1001]\n",
    "eicu_topic_word_distribution = eicu_client_model.get_beta()\n",
    "eicu_topic_word_distribution = eicu_topic_word_distribution.data.cpu().numpy()\n",
    "\n",
    "total_top_icd_idx = np.zeros((eicu_topic_word_distribution.shape[0], 5))  # K x 5\n",
    "\n",
    "for topic in range(eicu_topic_word_distribution.shape[0]):\n",
    "    topic_icds = eicu_topic_word_distribution[topic, :]\n",
    "    top_icd_idx = np.flip(np.argsort(topic_icds))[:5]  # Top 5 ICD codes\n",
    "    total_top_icd_idx[topic] = top_icd_idx\n",
    "\n",
    "total_top_icd_idx = np.ravel(total_top_icd_idx).astype(int)\n",
    "\n",
    "eicu_total_top_icd = eicu_topic_word_distribution[:, total_top_icd_idx]\n",
    "eicu_total_top_icd = eicu_total_top_icd.T\n",
    "\n",
    "total_top_icd_names = icd_code_names[total_top_icd_idx]\n",
    "disease = [convert_icd9_to_disease(x) for x in total_top_icd_names]\n",
    "disease_label = [f\"{disease[i]} - {total_top_icd_names[i]}\" for i in range(len(disease))]\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "\n",
    "# Plot heatmap\n",
    "plt.title(\"Heatmap of the Top 5 ICD Codes per Topic using ETM FedAvg (eICU)\")\n",
    "ax = sns.heatmap(eicu_total_top_icd,\n",
    "            yticklabels=disease_label,\n",
    "            cmap='Reds', vmax=0.2)\n",
    "\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "y_labels = plt.gca().get_yticklabels()\n",
    "for i, label in enumerate(y_labels):\n",
    "    color = disease_color_map[disease[i]]\n",
    "    label.set_color(color)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eff4335b3646f27",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.035141Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mimic_client_model = client_models[2001]\n",
    "mimic_topic_word_distribution = mimic_client_model.get_beta()\n",
    "mimic_topic_word_distribution = mimic_topic_word_distribution.data.cpu().numpy()\n",
    "\n",
    "total_top_icd_idx = np.zeros((mimic_topic_word_distribution.shape[0], 5))  # K x 5\n",
    "\n",
    "for topic in range(mimic_topic_word_distribution.shape[0]):\n",
    "    topic_icds = mimic_topic_word_distribution[topic, :]\n",
    "    top_icd_idx = np.flip(np.argsort(topic_icds))[:5]  # Top 5 ICD codes\n",
    "    total_top_icd_idx[topic] = top_icd_idx\n",
    "\n",
    "total_top_icd_idx = np.ravel(total_top_icd_idx).astype(int)\n",
    "\n",
    "mimic_total_top_icd = mimic_topic_word_distribution[:, total_top_icd_idx]\n",
    "mimic_total_top_icd = mimic_total_top_icd.T\n",
    "\n",
    "total_top_icd_names = icd_code_names[total_top_icd_idx]\n",
    "disease = [convert_icd9_to_disease(x) for x in total_top_icd_names]\n",
    "disease_label = [f\"{disease[i]} - {total_top_icd_names[i]}\" for i in range(len(disease))]\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "\n",
    "# Plot heatmap\n",
    "plt.title(\"Heatmap of the Top 5 ICD Codes per Topic using ETM FedAvg (MIMIC-III)\")\n",
    "ax = sns.heatmap(mimic_total_top_icd,\n",
    "            yticklabels=disease_label,\n",
    "            cmap='Reds', vmax=0.2)\n",
    "\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "y_labels = plt.gca().get_yticklabels()\n",
    "for i, label in enumerate(y_labels):\n",
    "    color = disease_color_map[disease[i]]\n",
    "    label.set_color(color)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e950347c678f18",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.037261Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eicu_x_bow_test = x_bow_tests[1001]\n",
    "eicu_coherence = get_topic_coherence(eicu_topic_word_distribution, eicu_x_bow_test, 3)\n",
    "eicu_diversity = get_topic_diversity(eicu_topic_word_distribution, 3)\n",
    "eicu_quality = eicu_coherence * eicu_diversity\n",
    "\n",
    "print(\"eICU ETM FedAvg Topic Coherence: \", eicu_coherence)\n",
    "print(\"eICU ETM FedAvg Topic Diversity: \", eicu_diversity)\n",
    "print(\"eICU ETM FedAvg Topic Quality: \", eicu_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59a513d07ef1b3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.054786Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mimic_x_bow_test = x_bow_tests[2001]\n",
    "mimic_coherence = get_topic_coherence(mimic_topic_word_distribution, mimic_x_bow_test, 3)\n",
    "mimic_diversity = get_topic_diversity(mimic_topic_word_distribution, 3)\n",
    "mimic_quality = mimic_coherence * mimic_diversity\n",
    "\n",
    "print(\"MIMIC ETM FedAvg Topic Coherence: \", mimic_coherence)\n",
    "print(\"MIMIC ETM FedAvg Topic Diversity: \", mimic_diversity)\n",
    "print(\"MIMIC ETM FedAvg Topic Quality: \", mimic_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833226adfc64260",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.063408Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "icd_code_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93473bfd99c2d3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.064914Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disease_labels = [convert_icd9_to_disease(x) for x in icd_code_names]\n",
    "disease_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06b25dc74f9363",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.093635Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "mimic_client_model = client_models[2001]\n",
    "eicu_x_test_tensor = test_icds[1001]\n",
    "\n",
    "_, eicu_test_theta_unweighted, _ = mimic_client_model.get_theta(eicu_x_test_tensor)\n",
    "eicu_test_theta_unweighted = eicu_test_theta_unweighted.data.cpu().numpy()\n",
    "\n",
    "eicu_test_readmit_row_ids = test_readmit_row_ids[1001]\n",
    "X_eicu_test = eicu_test_theta_unweighted[eicu_test_readmit_row_ids]\n",
    "eicu_icd_input = eicu_x_test_tensor[eicu_test_readmit_row_ids]\n",
    "\n",
    "eicu_most_common_icd_names, eicu_least_common_icd_names = find_common_icds(eicu_icd_input, feature_sums_tensor)\n",
    "\n",
    "unique_diseases = np.unique(eicu_least_common_icd_names)\n",
    "row_colors = pd.Series(eicu_least_common_icd_names).map(disease_color_map).to_numpy()\n",
    "\n",
    "# Create a seaborn clustermap\n",
    "plt.clf()\n",
    "row_clusters = linkage(X_eicu_test, method='ward')\n",
    "col_clusters = linkage(X_eicu_test.T, method='ward')\n",
    "g = sns.clustermap(X_eicu_test, row_linkage=row_clusters, col_linkage=col_clusters, \n",
    "                   figsize=(5, 6),\n",
    "                   yticklabels=False, cmap='rocket_r',\n",
    "                   cbar_kws={'orientation': 'horizontal', 'pad': 0.1, 'shrink': 0.6},\n",
    "                   cbar_pos=(0.45, -0.05, 0.3, 0.02),\n",
    "                   row_colors=row_colors)\n",
    "\n",
    "g.fig.suptitle(f'Heatmap FedAvg ETM Patient-Topic Mixture (MIMIC on eICU)',\n",
    "               fontsize=12, x=0.6, y=1.02)\n",
    "g.ax_heatmap.set_xlabel('Latent Dimension')\n",
    "g.ax_heatmap.set_ylabel('Patients')\n",
    "\n",
    "legend_patches = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=disease_color_map[disease],\n",
    "                             markersize=10, label=disease) for disease in unique_diseases]\n",
    "plt.legend(handles=legend_patches, title='Disease', bbox_to_anchor=(2.0, 12), loc='lower left', borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26335ddad3ef1852",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.094612Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "\n",
    "unique_diseases = icd_code_dict.keys()\n",
    "eicu_disease_p_values = {}\n",
    "\n",
    "eicu_disease_topic_p_values = np.zeros((len(unique_diseases), num_topics))\n",
    "\n",
    "for disease_idx, disease in enumerate(unique_diseases):\n",
    "    \n",
    "    # ICD codes for disease category\n",
    "    icd_codes = icd_code_dict[disease]\n",
    "    icd_code_idx = [np.where(icd_code_names == icd_code)[0][0] for icd_code in icd_codes]\n",
    "    eicu_icd_input_disease = eicu_icd_input[:, icd_code_idx]\n",
    "    \n",
    "    # Find the patients if the disease is present\n",
    "    patient_has_disease_indices = (eicu_icd_input_disease.sum(dim=1) > 0).nonzero(as_tuple=True)[0]\n",
    "    patient_no_disease_indices = (eicu_icd_input_disease.sum(dim=1) == 0).nonzero(as_tuple=True)[0]\n",
    "    patient_has_disease_indices = patient_has_disease_indices.cpu().numpy()\n",
    "    patient_no_disease_indices = patient_no_disease_indices.cpu().numpy()\n",
    "    \n",
    "    if len(patient_has_disease_indices) == 0 or len(patient_no_disease_indices) == 0:\n",
    "        continue\n",
    "    \n",
    "    smallest_p_value = np.inf\n",
    "    for topic in range(num_topics):\n",
    "        theta_topic = X_eicu_test[:, topic]\n",
    "        theta_topic_has_disease = theta_topic[patient_has_disease_indices]\n",
    "        theta_topic_no_disease = theta_topic[patient_no_disease_indices]\n",
    "        _, p_value = ranksums(theta_topic_has_disease, theta_topic_no_disease)\n",
    "        eicu_disease_topic_p_values[disease_idx][topic] = -np.log10(p_value)\n",
    "        if p_value < smallest_p_value:\n",
    "            smallest_p_value = p_value\n",
    "    \n",
    "    eicu_disease_p_values[disease] = -np.log10(smallest_p_value)\n",
    "    \n",
    "# MIMIC on eICU\n",
    "eicu_disease_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c634baeeb96630",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.095064Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preg_other_idx = [idx for idx in range(len(unique_diseases)) if list(unique_diseases)[idx] == \"Pregnancy\" or list(unique_diseases)[idx] == \"Others\"]\n",
    "\n",
    "eicu_disease_topic_p_values = np.delete(eicu_disease_topic_p_values, preg_other_idx, axis=0)\n",
    "unique_diseases = np.delete(list(unique_diseases), preg_other_idx)\n",
    "\n",
    "plt.figure(figsize=(5, 6))\n",
    "ax = sns.heatmap(eicu_disease_topic_p_values, cmap='Reds', cbar=True, cbar_kws={'orientation': 'horizontal', 'pad': 0.12, 'shrink': 0.8})\n",
    "\n",
    "ax.set_yticklabels(unique_diseases, rotation=0)\n",
    "\n",
    "colorbar = ax.collections[0].colorbar\n",
    "colorbar.set_label(\"-log10(p-value)\")\n",
    "\n",
    "plt.xlabel('Topics')\n",
    "plt.title('FedAvg ETM Significance of Disease-Topic Associations (MIMIC on eICU)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ebddb781d953a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:47:39.121355Z",
     "start_time": "2024-12-08T07:47:39.113393Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "eicu_client_model = client_models[1001]\n",
    "mimic_x_test_tensor = test_icds[2001]\n",
    "\n",
    "_, mimic_test_theta_unweighted, _ = eicu_client_model.get_theta(mimic_x_test_tensor)\n",
    "mimic_test_theta_unweighted = mimic_test_theta_unweighted.data.cpu().numpy()\n",
    "mimic_test_readmit_row_ids = test_readmit_row_ids[2001]\n",
    "\n",
    "X_mimic_test = mimic_test_theta_unweighted[mimic_test_readmit_row_ids]\n",
    "mimic_icd_input = mimic_x_test_tensor[mimic_test_readmit_row_ids]\n",
    "\n",
    "mimic_most_common_icd_names, mimic_least_common_icd_names = find_common_icds(mimic_icd_input, feature_sums_tensor)\n",
    "\n",
    "unique_diseases = np.unique(mimic_least_common_icd_names)\n",
    "row_colors = pd.Series(mimic_least_common_icd_names).map(disease_color_map).to_numpy()\n",
    "\n",
    "# Create a seaborn clustermap\n",
    "plt.clf()\n",
    "row_clusters = linkage(X_mimic_test, method='ward')\n",
    "col_clusters = linkage(X_mimic_test.T, method='ward')\n",
    "g = sns.clustermap(X_mimic_test, row_linkage=row_clusters, col_linkage=col_clusters, \n",
    "                   figsize=(5, 6),\n",
    "                   yticklabels=False, cmap='rocket_r',\n",
    "                   cbar_kws={'orientation': 'horizontal', 'pad': 0.1, 'shrink': 0.6},\n",
    "                   cbar_pos=(0.45, -0.05, 0.3, 0.02),\n",
    "                   row_colors=row_colors)\n",
    "\n",
    "g.fig.suptitle(f'Heatmap FedAvg ETM Patient-Topic Mixture (eICU on MIMIC)',\n",
    "               fontsize=12, x=0.6, y=1.02)\n",
    "g.ax_heatmap.set_xlabel('Latent Dimension')\n",
    "g.ax_heatmap.set_ylabel('Patients')\n",
    "\n",
    "legend_patches = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=disease_color_map[disease],\n",
    "                             markersize=10, label=disease) for disease in unique_diseases]\n",
    "plt.legend(handles=legend_patches, title='Disease', bbox_to_anchor=(2.0, 12), loc='lower left', borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0144bfb58e700c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.116758Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "\n",
    "unique_diseases = icd_code_dict.keys()\n",
    "mimic_disease_p_values = {}\n",
    "\n",
    "mimic_disease_topic_p_values = np.zeros((len(unique_diseases), num_topics))\n",
    "\n",
    "for disease_idx, disease in enumerate(unique_diseases):\n",
    "    \n",
    "    # ICD codes for disease category\n",
    "    icd_codes = icd_code_dict[disease]\n",
    "    icd_code_idx = [np.where(icd_code_names == icd_code)[0][0] for icd_code in icd_codes]\n",
    "    mimic_icd_input_disease = mimic_icd_input[:, icd_code_idx]\n",
    "    \n",
    "    # Find the patients if the disease is present\n",
    "    patient_has_disease_indices = (mimic_icd_input_disease.sum(dim=1) > 0).nonzero(as_tuple=True)[0]\n",
    "    patient_no_disease_indices = (mimic_icd_input_disease.sum(dim=1) == 0).nonzero(as_tuple=True)[0]\n",
    "    patient_has_disease_indices = patient_has_disease_indices.cpu().numpy()\n",
    "    patient_no_disease_indices = patient_no_disease_indices.cpu().numpy()\n",
    "    \n",
    "    if len(patient_has_disease_indices) == 0 or len(patient_no_disease_indices) == 0:\n",
    "        continue\n",
    "    \n",
    "    smallest_p_value = np.inf\n",
    "    for topic in range(num_topics):\n",
    "        theta_topic = X_mimic_test[:, topic]\n",
    "        theta_topic_has_disease = theta_topic[patient_has_disease_indices]\n",
    "        theta_topic_no_disease = theta_topic[patient_no_disease_indices]\n",
    "        _, p_value = ranksums(theta_topic_has_disease, theta_topic_no_disease)\n",
    "        mimic_disease_topic_p_values[disease_idx][topic] = -np.log10(p_value)\n",
    "        if p_value < smallest_p_value:\n",
    "            smallest_p_value = p_value\n",
    "    \n",
    "    mimic_disease_p_values[disease] = -np.log10(smallest_p_value)\n",
    "    \n",
    "# eICU on MIMIC\n",
    "mimic_disease_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407449617daff1a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.119559Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preg_other_idx = [idx for idx in range(len(unique_diseases)) if list(unique_diseases)[idx] == \"Pregnancy\" or list(unique_diseases)[idx] == \"Others\"]\n",
    "\n",
    "mimic_disease_topic_p_values = np.delete(mimic_disease_topic_p_values, preg_other_idx, axis=0)\n",
    "unique_diseases = np.delete(list(unique_diseases), preg_other_idx)\n",
    "\n",
    "plt.figure(figsize=(5, 6))\n",
    "ax = sns.heatmap(mimic_disease_topic_p_values, cmap='Reds', cbar=True, cbar_kws={'orientation': 'horizontal', 'pad': 0.12, 'shrink': 0.8})\n",
    "\n",
    "ax.set_yticklabels(unique_diseases, rotation=0)\n",
    "\n",
    "colorbar = ax.collections[0].colorbar\n",
    "colorbar.set_label(\"-log10(p-value)\")\n",
    "\n",
    "plt.xlabel('Topics')\n",
    "plt.title('FedAvg ETM Significance of Disease-Topic Associations (eICU on MIMIC)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313985d1c647210",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.144102Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, average_precision_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mimic_client_model = client_models[2001]\n",
    "\n",
    "mimic_x_train_tensor = train_icds[2001]\n",
    "eicu_x_test_tensor = test_icds[1001]\n",
    "\n",
    "_, mimic_train_theta_unweighted, _ = mimic_client_model.get_theta(mimic_x_train_tensor)\n",
    "_, eicu_test_theta_unweighted, _ = mimic_client_model.get_theta(eicu_x_test_tensor)\n",
    "\n",
    "mimic_train_theta_unweighted = mimic_train_theta_unweighted.data.cpu().numpy()\n",
    "eicu_test_theta_unweighted = eicu_test_theta_unweighted.data.cpu().numpy()\n",
    "\n",
    "mimic_train_readmit_row_ids = train_readmit_row_ids[2001]\n",
    "eicu_test_readmit_row_ids = test_readmit_row_ids[1001]\n",
    "\n",
    "X_mimic_train = mimic_train_theta_unweighted[mimic_train_readmit_row_ids]\n",
    "X_eicu_test = eicu_test_theta_unweighted[eicu_test_readmit_row_ids]\n",
    "\n",
    "y_mimic_death_train = train_label_deaths[2001]\n",
    "y_eicu_death_test = test_label_deaths[1001]\n",
    "\n",
    "# log_reg = LogisticRegression()\n",
    "# log_reg.fit(X_mimic_train, y_mimic_death_train)\n",
    "# \n",
    "# y_eicu_death_cross_lr_scores = log_reg.predict_proba(X_eicu_test)[:, 1]\n",
    "# \n",
    "# precision_eicu_death_cross_lr, recall_eicu_death_cross_lr, _ = precision_recall_curve(y_eicu_death_test, y_eicu_death_cross_lr_scores)\n",
    "# auprc_eicu_death_cross_lr = average_precision_score(y_eicu_death_test, y_eicu_death_cross_lr_scores)\n",
    "# top_k_precision_death_cross_lr = top_k_precision(y_eicu_death_test, y_eicu_death_cross_lr_scores, k=100)\n",
    "# \n",
    "# fpr_eicu_death_cross_lr, tpr_eicu_death_cross_lr, _ = roc_curve(y_eicu_death_test, y_eicu_death_cross_lr_scores)\n",
    "# auroc_eicu_death_cross_lr = roc_auc_score(y_eicu_death_test, y_eicu_death_cross_lr_scores)\n",
    "# \n",
    "# # Print the AUPRC and AUROC values\n",
    "# print(f\"AUPRC of Mortality Prediction after Re-admission (MIMIC on eICU): {auprc_eicu_death_cross_lr:.4f}\")\n",
    "# print(f\"AUROC of Mortality Prediction after Re-admission (MIMIC on eICU): {auroc_eicu_death_cross_lr:.4f}\")\n",
    "# print(f\"Top k Precision of Mortality Prediction after Re-admission (MIMIC on eICU): {top_k_precision_death_cross_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be762016df1deebb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.148844Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=120)\n",
    "knn.fit(X_mimic_train, y_mimic_death_train)\n",
    "\n",
    "y_eicu_death_cross_knn_scores = knn.predict_proba(X_eicu_test)[:, 1]\n",
    "\n",
    "precision_eicu_death_cross_knn, recall_eicu_death_cross_knn, _ = precision_recall_curve(y_eicu_death_test, y_eicu_death_cross_knn_scores)\n",
    "auprc_eicu_death_cross_knn = average_precision_score(y_eicu_death_test, y_eicu_death_cross_knn_scores)\n",
    "top_k_precision_death_cross_knn = top_k_precision(y_eicu_death_test, y_eicu_death_cross_knn_scores, k=100)\n",
    "\n",
    "fpr_eicu_death_cross_knn, tpr_eicu_death_cross_knn, _ = roc_curve(y_eicu_death_test, y_eicu_death_cross_knn_scores)\n",
    "auroc_eicu_death_cross_knn = roc_auc_score(y_eicu_death_test, y_eicu_death_cross_knn_scores)\n",
    "\n",
    "# Print the AUPRC and AUROC values\n",
    "print(f\"AUPRC of Mortality Prediction after Re-admission (MIMIC on eICU): {auprc_eicu_death_cross_knn:.4f}\")\n",
    "print(f\"AUROC of Mortality Prediction after Re-admission (MIMIC on eICU): {auroc_eicu_death_cross_knn:.4f}\")\n",
    "print(f\"Top k Precision of Mortality Prediction after Re-admission (MIMIC on eICU): {top_k_precision_death_cross_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4f9cd3b0a548c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.148928Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, average_precision_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eicu_client_model = client_models[1001]\n",
    "\n",
    "eicu_x_train_tensor = train_icds[1001]\n",
    "mimic_x_test_tensor = test_icds[2001]\n",
    "\n",
    "_, eicu_train_theta_unweighted, _ = eicu_client_model.get_theta(eicu_x_train_tensor)\n",
    "_, mimic_test_theta_unweighted, _ = eicu_client_model.get_theta(mimic_x_test_tensor)\n",
    "\n",
    "eicu_train_theta_unweighted = eicu_train_theta_unweighted.data.cpu().numpy()\n",
    "mimic_test_theta_unweighted = mimic_test_theta_unweighted.data.cpu().numpy()\n",
    "\n",
    "eicu_train_readmit_row_ids = train_readmit_row_ids[1001]\n",
    "mimic_test_readmit_row_ids = test_readmit_row_ids[2001]\n",
    "\n",
    "X_eicu_train = eicu_train_theta_unweighted[eicu_train_readmit_row_ids]\n",
    "X_mimic_test = mimic_test_theta_unweighted[mimic_test_readmit_row_ids]\n",
    "\n",
    "y_eicu_death_train = train_label_deaths[1001]\n",
    "y_mimic_death_test = test_label_deaths[2001]\n",
    "\n",
    "# log_reg = LogisticRegression()\n",
    "# log_reg.fit(X_eicu_train, y_eicu_death_train)\n",
    "# \n",
    "# y_mimic_death_cross_lr_scores = log_reg.predict_proba(X_mimic_test)[:, 1]\n",
    "# \n",
    "# precision_mimic_death_cross_lr, recall_mimic_death_cross_lr, _ = precision_recall_curve(y_mimic_death_test, y_mimic_death_cross_lr_scores)\n",
    "# auprc_mimic_death_cross_lr = average_precision_score(y_mimic_death_test, y_mimic_death_cross_lr_scores)\n",
    "# top_k_precision_mimic_cross_lr = top_k_precision(y_mimic_death_test, y_mimic_death_cross_lr_scores, k=100)\n",
    "# \n",
    "# fpr_mimic_death_cross_lr, tpr_mimic_death_cross_lr, _ = roc_curve(y_mimic_death_test, y_mimic_death_cross_lr_scores)\n",
    "# auroc_mimic_death_cross_lr = roc_auc_score(y_mimic_death_test, y_mimic_death_cross_lr_scores)\n",
    "# \n",
    "# # Print the AUPRC and AUROC values\n",
    "# print(f\"AUPRC of Mortality Prediction after Re-admission (eICU on MIMIC): {auprc_mimic_death_cross_lr:.4f}\")\n",
    "# print(f\"AUROC of Mortality Prediction after Re-admission (eICU on MIMIC): {auroc_mimic_death_cross_lr:.4f}\")\n",
    "# print(f\"Top k Precision of Mortality Prediction after Re-admission (eICU on MIMIC): {top_k_precision_mimic_cross_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb9cf8b3222d0b1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.151294Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=120)\n",
    "knn.fit(X_eicu_train, y_eicu_death_train)\n",
    "\n",
    "y_mimic_death_cross_knn_scores = knn.predict_proba(X_mimic_test)[:, 1]\n",
    "\n",
    "precision_mimic_death_cross_knn, recall_mimic_death_cross_knn, _ = precision_recall_curve(y_mimic_death_test, y_mimic_death_cross_knn_scores)\n",
    "auprc_mimic_death_cross_knn = average_precision_score(y_mimic_death_test, y_mimic_death_cross_knn_scores)\n",
    "top_k_precision_mimic_cross_knn = top_k_precision(y_mimic_death_test, y_mimic_death_cross_knn_scores, k=100)\n",
    "\n",
    "fpr_mimic_death_cross_knn, tpr_mimic_death_cross_knn, _ = roc_curve(y_mimic_death_test, y_mimic_death_cross_knn_scores)\n",
    "auroc_mimic_death_cross_knn = roc_auc_score(y_mimic_death_test, y_mimic_death_cross_knn_scores)\n",
    "\n",
    "# Print the AUPRC and AUROC values\n",
    "print(f\"AUPRC of Mortality Prediction after Re-admission (eICU on MIMIC): {auprc_mimic_death_cross_knn:.4f}\")\n",
    "print(f\"AUROC of Mortality Prediction after Re-admission (eICU on MIMIC): {auroc_mimic_death_cross_knn:.4f}\")\n",
    "print(f\"Top k Precision of Mortality Prediction after Re-admission (eICU on MIMIC): {top_k_precision_mimic_cross_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623e94d3c78b6d9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.151459Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"unweighted_recall_eicu_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for r in recall_eicu_death_cross_knn:\n",
    "        f.write(f\"{r}\\n\")\n",
    "        \n",
    "with open(\"unweighted_precision_eicu_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for p in precision_eicu_death_cross_knn:\n",
    "        f.write(f\"{p}\\n\")\n",
    "    \n",
    "with open(\"unweighted_auprc_eicu_death_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{auprc_eicu_death_cross_knn}\\n\")\n",
    "\n",
    "with open(\"unweighted_top_k_precision_eicu_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{top_k_precision_death_cross_knn}\\n\")\n",
    "    \n",
    "with open(\"unweighted_fpr_eicu_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for fp in fpr_eicu_death_cross_knn:\n",
    "        f.write(f\"{fp}\\n\")\n",
    "        \n",
    "with open(\"unweighted_tpr_eicu_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for tp in tpr_eicu_death_cross_knn:\n",
    "        f.write(f\"{tp}\\n\")\n",
    "        \n",
    "with open(\"unweighted_auroc_eicu_death_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{auroc_eicu_death_cross_knn}\\n\")\n",
    "    \n",
    "with open(\"unweighted_eicu_disease_p_values.txt\", 'w') as file:\n",
    "    for key, value in eicu_disease_p_values.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "        \n",
    "with open(\"unweighted_eicu_disease_topic_p_values.txt\", 'w') as file:\n",
    "    for row in eicu_disease_topic_p_values:\n",
    "        file.write(\" \".join(f\"{value:.5f}\" for value in row) + \"\\n\")\n",
    "        \n",
    "with open(\"unweighted_recall_mimic_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for r in recall_mimic_death_cross_knn:\n",
    "        f.write(f\"{r}\\n\")\n",
    "        \n",
    "with open(\"unweighted_precision_mimic_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for p in precision_mimic_death_cross_knn:\n",
    "        f.write(f\"{p}\\n\")\n",
    "    \n",
    "with open(\"unweighted_auprc_mimic_death_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{auprc_mimic_death_cross_knn}\\n\")\n",
    "    \n",
    "with open(\"unweighted_top_k_precision_mimic_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{top_k_precision_mimic_cross_knn}\\n\")\n",
    "    \n",
    "with open(\"unweighted_mimic_disease_p_values.txt\", 'w') as file:\n",
    "    for key, value in mimic_disease_p_values.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "        \n",
    "with open(\"unweighted_fpr_mimic_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for fp in fpr_mimic_death_cross_knn:\n",
    "        f.write(f\"{fp}\\n\")\n",
    "        \n",
    "with open(\"unweighted_tpr_mimic_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for tp in tpr_mimic_death_cross_knn:\n",
    "        f.write(f\"{tp}\\n\")\n",
    "        \n",
    "with open(\"unweighted_auroc_mimic_death_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{auroc_mimic_death_cross_knn}\\n\")\n",
    "\n",
    "with open(\"unweighted_mimic_disease_topic_p_values.txt\", 'w') as file:\n",
    "    for row in mimic_disease_topic_p_values:\n",
    "        file.write(\" \".join(f\"{value:.5f}\" for value in row) + \"\\n\")\n",
    "        \n",
    "with open(\"unweighted_unique_disease_names.txt\", 'w') as file:\n",
    "    for disease in unique_diseases:\n",
    "        file.write(f'{disease}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d0764a99e56f8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.154091Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    " \n",
    "torch.save(client_models[2001].state_dict(), 'unweighted_mimic_client_model.pth')\n",
    "torch.save(client_models[1001].state_dict(), 'unweighted_eicu_client_model.pth')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "mimic_train_icds = train_icds[2001].cpu().numpy()\n",
    "np.save(\"unweighted_mimic_train_icds.npy\", mimic_train_icds)\n",
    "\n",
    "mimic_test_icds = test_icds[2001].cpu().numpy()\n",
    "np.save(\"unweighted_mimic_test_icds.npy\", mimic_test_icds)\n",
    "\n",
    "mimic_train_readmit_row_ids = train_readmit_row_ids[2001]\n",
    "np.save(\"unweighted_mimic_train_readmit_row_ids.npy\", mimic_train_readmit_row_ids)\n",
    "\n",
    "mimic_test_readmit_row_ids = test_readmit_row_ids[2001]\n",
    "np.save(\"unweighted_mimic_test_readmit_row_ids.npy\", mimic_test_readmit_row_ids)\n",
    "\n",
    "mimic_train_label_deaths = train_label_deaths[2001]\n",
    "np.save(\"unweighted_mimic_train_label_deaths.npy\", mimic_train_label_deaths)\n",
    "\n",
    "mimic_test_label_deaths = test_label_deaths[2001]\n",
    "np.save(\"unweighted_mimic_test_label_deaths.npy\", mimic_test_label_deaths)\n",
    "\n",
    "eicu_train_icds = train_icds[1001].cpu().numpy()\n",
    "np.save(\"unweighted_eicu_train_icds.npy\", eicu_train_icds)\n",
    "\n",
    "eicu_test_icds = test_icds[1001].cpu().numpy()\n",
    "np.save(\"unweighted_eicu_test_icds.npy\", eicu_test_icds)\n",
    "\n",
    "eicu_train_readmit_row_ids = train_readmit_row_ids[1001]\n",
    "np.save(\"unweighted_eicu_train_readmit_row_ids.npy\", eicu_train_readmit_row_ids)\n",
    "\n",
    "eicu_test_readmit_row_ids = test_readmit_row_ids[1001]\n",
    "np.save(\"unweighted_eicu_test_readmit_row_ids.npy\", eicu_test_readmit_row_ids)\n",
    "\n",
    "eicu_train_label_deaths = train_label_deaths[1001]\n",
    "np.save(\"unweighted_eicu_train_label_deaths.npy\", eicu_train_label_deaths)\n",
    "\n",
    "eicu_test_label_deaths = test_label_deaths[1001]\n",
    "np.save(\"unweighted_eicu_test_label_deaths.npy\", eicu_test_label_deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964462c6008c1caf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Baseline LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8f8fe857fbe94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T07:47:39.157946Z",
     "start_time": "2024-12-08T07:47:39.156196Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# \n",
    "# lda_model = LatentDirichletAllocation(n_components=num_topics,\n",
    "#                                       random_state=42)\n",
    "# \n",
    "# lda_model.fit(x_train_icd_data_np)\n",
    "# \n",
    "# document_topic_distribution = lda_model.transform(x_test_icd_data_np)\n",
    "# \n",
    "# reducer = umap.UMAP(n_neighbors=5, n_components=2, min_dist=0.8,\n",
    "#                                     random_state=np.random.RandomState(25),\n",
    "#                                     transform_seed=np.random.RandomState(25))\n",
    "# principal_components_lda = reducer.fit_transform(document_topic_distribution)\n",
    "# print(principal_components_lda.shape)\n",
    "# \n",
    "# respiratory_indices = [i for i, label in enumerate(disease_labels) if \"Respiratory\" == label]\n",
    "# respiratory_tensor = x_test_tensor[:, respiratory_indices]\n",
    "# patient_has_respiratory = (respiratory_tensor.sum(dim=1) > 0).int().numpy()\n",
    "# \n",
    "# # K-means clustering\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(principal_components_lda)\n",
    "# kmeans_labels = kmeans.labels_\n",
    "# \n",
    "# ari_score_diagnosis = adjusted_rand_score(patient_has_respiratory, kmeans_labels)\n",
    "# print(f'Adjusted Rand Index (ARI) for Hospitals: {ari_score_diagnosis}')\n",
    "# \n",
    "# palette = [\n",
    "#     '#1268fd',    # Grey\n",
    "#     '#ff5c7c',   # Red\n",
    "# ]\n",
    "# colors = sns.color_palette(palette, n_colors=2)\n",
    "# \n",
    "# final_np = np.hstack((principal_components_lda, patient_has_respiratory[:, None]))\n",
    "# \n",
    "# scatter_rows_0 = final_np[np.where(final_np[:, 2] == 0.0)]\n",
    "# plt.scatter(scatter_rows_0[:, 0], scatter_rows_0[:, 1], color=colors[0], s=0.2, alpha=0.2)\n",
    "# \n",
    "# scatter_rows_1 = final_np[np.where(final_np[:, 2] == 1.0)]\n",
    "# plt.scatter(scatter_rows_1[:, 0], scatter_rows_1[:, 1], color=colors[1], s=0.2, alpha=0.5)\n",
    "# \n",
    "# legend_elements = []\n",
    "# legend_element_0 = Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[0], markersize=8, label=\"No respiratory\")\n",
    "# legend_element_1 = Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[1], markersize=8, label=\"Has respiratory\")\n",
    "# legend_elements.append(legend_element_0)\n",
    "# legend_elements.append(legend_element_1)\n",
    "# \n",
    "# plt.title(\"K-Means Clustering of Respiratory Disease in LDA Patient-Topic Distribution\")\n",
    "# plt.text(15, 20, f'ARI: {ari_score_diagnosis:.4f}', color='red', fontsize=12, bbox=dict(facecolor='yellow', alpha=1.0))\n",
    "# plt.legend(title=\"Disease\", handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531fbf9343b5250",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.156295Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # K-means clustering\n",
    "# kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "# kmeans.fit(principal_components_lda)\n",
    "# kmeans_labels = kmeans.labels_\n",
    "# \n",
    "# ari_score_diagnosis = adjusted_rand_score(patients_all_df_disease, kmeans_labels)\n",
    "# print(f'Adjusted Rand Index (ARI) for Diagnosis: {ari_score_diagnosis}')\n",
    "# \n",
    "# final_np = np.hstack((principal_components_lda, patients_all_df_disease[:, None]))\n",
    "# \n",
    "# legend_elements = []\n",
    "# for label in np.unique(patients_all_df_disease):\n",
    "#     scatter_rows = final_np[np.where(final_np[:, 2] == label)]\n",
    "#     plt.scatter(scatter_rows[:, 0].astype(float), scatter_rows[:, 1].astype(float), color=disease_color_map[label], s=0.2, alpha=0.2, label=label)\n",
    "#     legend_element = Line2D([0], [0], marker='o', color='w', markerfacecolor=disease_color_map[label], markersize=8, label=label)\n",
    "#     legend_elements.append(legend_element)\n",
    "# \n",
    "# plt.title(\"K-Means Clustering of Diagnosis in LDA Patient-Topic Distribution\")\n",
    "# plt.text(15, 20, f'ARI: {ari_score_diagnosis:.4f}', color='red', fontsize=12, bbox=dict(facecolor='yellow', alpha=1.0))\n",
    "# plt.legend(title=\"Disease\", handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1845784a56994c5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.389627Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for target_label in np.unique(disease_labels):\n",
    "#     \n",
    "#     disease_indices = [i for i, label in enumerate(disease_labels) if target_label == label]\n",
    "#     disease_tensor = x_test_tensor[:, disease_indices]\n",
    "#     patient_has_disease = (disease_tensor.sum(dim=1) > 0).int().numpy()\n",
    "#     \n",
    "#     # K-means clustering\n",
    "#     kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "#     kmeans.fit(principal_components_lda)\n",
    "#     kmeans_labels = kmeans.labels_\n",
    "#     \n",
    "#     ari_score_diagnosis = adjusted_rand_score(patient_has_disease, kmeans_labels)\n",
    "#     print(f'Adjusted Rand Index (ARI) for {target_label}: {ari_score_diagnosis}')\n",
    "#     \n",
    "#     palette = [\n",
    "#         '#1268fd',    # Grey\n",
    "#         '#ff5c7c',   # Red\n",
    "#     ]\n",
    "#     colors = sns.color_palette(palette, n_colors=2)\n",
    "#     \n",
    "#     final_np = np.hstack((principal_components_lda, patient_has_disease[:, None]))\n",
    "#     \n",
    "#     scatter_rows_0 = final_np[np.where(final_np[:, 2] == 0.0)]\n",
    "#     plt.scatter(scatter_rows_0[:, 0], scatter_rows_0[:, 1], color=colors[0], s=0.2, alpha=0.2)\n",
    "#     \n",
    "#     scatter_rows_1 = final_np[np.where(final_np[:, 2] == 1.0)]\n",
    "#     plt.scatter(scatter_rows_1[:, 0], scatter_rows_1[:, 1], color=colors[1], s=0.2, alpha=0.2)\n",
    "#     \n",
    "#     legend_elements = []\n",
    "#     legend_element_0 = Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[0], markersize=8, label=f\"No {target_label}\")\n",
    "#     legend_element_1 = Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[1], markersize=8, label=f\"Has {target_label}\")\n",
    "#     legend_elements.append(legend_element_0)\n",
    "#     legend_elements.append(legend_element_1)\n",
    "#     \n",
    "#     plt.title(f\"K-Means Clustering of {target_label} Disease in LDA Patient-Topic Distribution\")\n",
    "#     plt.text(15, 20, f'ARI: {ari_score_diagnosis:.4f}', color='red', fontsize=12, bbox=dict(facecolor='yellow', alpha=1.0))\n",
    "#     plt.legend(title=\"Disease\", handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b46a4b88e76e5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.389753Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# \n",
    "# topic_word_distribution = lda_model.components_ / \\\n",
    "#                           lda_model.components_.sum(axis=1)[:, np.newaxis]\n",
    "# \n",
    "# total_top_icd_idx = np.zeros((topic_word_distribution.shape[0], 5))  # K x 5\n",
    "# \n",
    "# for topic in range(topic_word_distribution.shape[0]):\n",
    "#     topic_icds = topic_word_distribution[topic, :]\n",
    "#     top_icd_idx = np.flip(np.argsort(topic_icds))[:5]  # Top 5 ICD codes\n",
    "#     total_top_icd_idx[topic] = top_icd_idx\n",
    "# \n",
    "# total_top_icd_idx = np.ravel(total_top_icd_idx).astype(int)\n",
    "# \n",
    "# total_top_icd = topic_word_distribution[:, total_top_icd_idx]\n",
    "# total_top_icd = total_top_icd.T\n",
    "# \n",
    "# total_top_icd_names = icd_code_names[total_top_icd_idx]\n",
    "# disease = [convert_icd9_to_disease(x) for x in total_top_icd_names]\n",
    "# disease_label = [f\"{disease[i]} - {total_top_icd_names[i]}\" for i in range(len(disease))]\n",
    "# \n",
    "# plt.figure(figsize=(8, 10))\n",
    "# \n",
    "# # Plot heatmap\n",
    "# plt.title(\"Heatmap of the Top 5 ICD Codes per Topic using LDA\")\n",
    "# ax = sns.heatmap(total_top_icd,\n",
    "#             yticklabels=disease_label,\n",
    "#             cmap='Reds', vmax=0.2)\n",
    "# \n",
    "# ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)\n",
    "# \n",
    "# y_labels = plt.gca().get_yticklabels()\n",
    "# for i, label in enumerate(y_labels):\n",
    "#     color = disease_color_map[disease[i]]\n",
    "#     label.set_color(color)\n",
    "# \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a70fe83cd0f667",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.398146Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coherence = get_topic_coherence(topic_word_distribution, x_bow_test, 5)\n",
    "# diversity = get_topic_diversity(topic_word_distribution, 5)\n",
    "# quality = coherence * diversity\n",
    "# \n",
    "# print(\"ETM FedAvg Topic Coherence: \", coherence)\n",
    "# print(\"ETM FedAvg Topic Diversity: \", diversity)\n",
    "# print(\"ETM FedAvg Topic Quality: \", quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08074895018d8f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.399641Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document_topic_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d0921c7c68d2c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.399740Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.cluster.hierarchy import linkage\n",
    "# \n",
    "# # Create a seaborn clustermap\n",
    "# plt.clf()\n",
    "# row_clusters = linkage(document_topic_distribution, method='ward')\n",
    "# col_clusters = linkage(document_topic_distribution.T, method='ward')\n",
    "# cmap = sns.diverging_palette(260, 350, as_cmap=True)\n",
    "# g = sns.clustermap(document_topic_distribution, row_linkage=row_clusters, col_linkage=col_clusters, figsize=(5, 6),\n",
    "#                    yticklabels=False, cmap=cmap, center=0,\n",
    "#                    cbar_kws={'orientation': 'horizontal', 'pad': 0.1, 'shrink': 0.6},\n",
    "#                    cbar_pos=(0.45, -0.05, 0.3, 0.02))\n",
    "# \n",
    "# g.fig.suptitle(f'Heatmap LDA Theta',\n",
    "#                fontsize=12, x=0.6, y=1.02)\n",
    "# g.ax_heatmap.set_xlabel('Latent Dimension')\n",
    "# g.ax_heatmap.set_ylabel('Patients')\n",
    "# \n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ecdf4bc797bc94",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.457508Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# document_topic_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aada06a7e19c437",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Upper Bound scETM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3243be5dc29d3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.457664Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F \n",
    "# import numpy as np\n",
    "# \n",
    "# from torch import nn\n",
    "# \n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# \n",
    "# class ETM(nn.Module):\n",
    "#     def __init__(self, \n",
    "#                  num_topics, \n",
    "#                  vocab_size, \n",
    "#                  t_hidden_size, \n",
    "#                  rho_size, \n",
    "#                  enc_drop=0.5, \n",
    "#                  num_hospitals=None,\n",
    "#                  run_with_fl=True):\n",
    "#         super(ETM, self).__init__()\n",
    "# \n",
    "#         ## define hyperparameters\n",
    "#         self.num_topics = num_topics\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.t_hidden_size = t_hidden_size\n",
    "#         self.rho_size = rho_size\n",
    "#         self.enc_drop = enc_drop\n",
    "#         self.t_drop = nn.Dropout(enc_drop)\n",
    "#         self.run_with_fl = run_with_fl\n",
    "#         \n",
    "#         if not run_with_fl:\n",
    "#             # Centralized\n",
    "#             self.hospital_bias = nn.Parameter(\n",
    "#                 torch.randn(num_hospitals, vocab_size))  # H x D\n",
    "#             self.global_bias = nn.Parameter(\n",
    "#                 torch.randn(1, vocab_size))  # 1 x D\n",
    "#         \n",
    "#         ## define the word embedding matrix \\rho\n",
    "#         self.rho = nn.Linear(rho_size, vocab_size, bias=False)\n",
    "# \n",
    "#         ## define the matrix containing the topic embeddings\n",
    "#         self.alphas = nn.Linear(rho_size, num_topics, bias=False)\n",
    "#         # self.alpha = nn.Parameter(torch.randn(num_topics, rho_size)) # K x L\n",
    "#         # self.rho = nn.Parameter(torch.randn(rho_size, vocab_size)) # L x D\n",
    "#     \n",
    "#         ## define variational distribution for \\theta_{1:D} via amortizartion\n",
    "#         print(vocab_size, \" THE Vocabulary size is here \")\n",
    "#         q_theta_shape = vocab_size + num_hospitals if not run_with_fl else vocab_size\n",
    "#         self.q_theta = nn.Sequential(\n",
    "#                 nn.Linear(q_theta_shape, t_hidden_size), \n",
    "#                 # nn.ReLU(),\n",
    "#                 # nn.Linear(t_hidden_size, t_hidden_size),\n",
    "#                 # nn.ReLU(),\n",
    "#             )\n",
    "#         self.mu_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
    "#         self.logsigma_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
    "# \n",
    "#     def reparameterize(self, mu, logvar):\n",
    "#         \"\"\"Returns a sample from a Gaussian distribution via reparameterization.\n",
    "#         \"\"\"\n",
    "#         if self.training:\n",
    "#             std = torch.exp(0.5 * logvar) \n",
    "#             eps = torch.randn_like(std)\n",
    "#             return eps.mul_(std).add_(mu)\n",
    "#         else:\n",
    "#             return mu\n",
    "# \n",
    "#     def encode(self, bows):\n",
    "#         \"\"\"Returns paramters of the variational distribution for \\theta.\n",
    "# \n",
    "#         input: bows\n",
    "#                 batch of bag-of-words...tensor of shape bsz x V\n",
    "#         output: mu_theta, log_sigma_theta\n",
    "#         \"\"\"\n",
    "#         q_theta = self.q_theta(bows)\n",
    "#         if self.enc_drop > 0:\n",
    "#             q_theta = self.t_drop(q_theta)\n",
    "#         mu_theta = self.mu_q_theta(q_theta)\n",
    "#         logsigma_theta = self.logsigma_q_theta(q_theta)\n",
    "#         kl_theta = -0.5 * torch.sum(1 + logsigma_theta - mu_theta.pow(2) - logsigma_theta.exp(), dim=-1).mean()\n",
    "#         return mu_theta, logsigma_theta, kl_theta\n",
    "# \n",
    "#     def get_beta(self):\n",
    "#         \"\"\"\n",
    "#         This generate the description as a defintion over words\n",
    "# \n",
    "#         Returns:\n",
    "#             [type]: [description]\n",
    "#         \"\"\"\n",
    "#         # try:\n",
    "#         #     logit = self.alphas(self.rho.weight) # torch.mm(self.rho, self.alphas)\n",
    "#         # except:\n",
    "#         #     logit = self.alphas(self.rho)\n",
    "#         # beta = F.softmax(logit, dim=0).transpose(1, 0) ## softmax over vocab dimension\n",
    "#         beta = self.alphas(self.rho.weight) # D x K\n",
    "#         beta_normalized = F.softmax(beta, dim=0) # D x K ## softmax over vocab dimension\n",
    "#         return beta.transpose(1, 0), beta_normalized.transpose(1, 0)\n",
    "# \n",
    "#     def get_theta(self, normalized_bows):\n",
    "#         \"\"\"\n",
    "#         getting the topic poportion for the document passed in the normalixe bow or tf-idf\"\"\"\n",
    "#         mu_theta, logsigma_theta, kld_theta = self.encode(normalized_bows)\n",
    "#         z = self.reparameterize(mu_theta, logsigma_theta)\n",
    "#         theta = F.softmax(z, dim=-1) \n",
    "#         return z, theta, kld_theta\n",
    "# \n",
    "#     def decode(self, theta, beta, beta_norm, hospital_bias=None):\n",
    "#         \"\"\"compute the probability of topic given the document which is equal to theta^T ** B\n",
    "# \n",
    "#         Args:\n",
    "#             theta ([type]): [description]\n",
    "#             beta ([type]): [description]\n",
    "# \n",
    "#         Returns:\n",
    "#             [type]: [description]\n",
    "#         \"\"\"\n",
    "#         if hospital_bias is None:\n",
    "#             res = torch.mm(theta, beta_norm)\n",
    "#             almost_zeros = torch.full_like(res, 1e-6)\n",
    "#             results_without_zeros = res.add(almost_zeros)\n",
    "#             predictions = torch.log(results_without_zeros)\n",
    "#         else:\n",
    "#             res = torch.mm(theta, beta) + hospital_bias\n",
    "#             predictions = F.log_softmax(res, dim=-1) # Normalization\n",
    "#         return predictions\n",
    "# \n",
    "#     def forward(self, X, X_hospital_concat, theta=None, aggregate=True, hospital_ids=None):\n",
    "#         \n",
    "#         if theta is None:\n",
    "#             _, theta, kld_theta = self.get_theta(X_hospital_concat)\n",
    "#         else:\n",
    "#             kld_theta = None\n",
    "# \n",
    "#         ## get \\beta\n",
    "#         beta, beta_norm = self.get_beta()\n",
    "#             \n",
    "#         hospital_bias = None\n",
    "#         if not self.run_with_fl:\n",
    "#             # Centralized\n",
    "#             hospital_bias = self.hospital_bias[hospital_ids] if hospital_ids is not None else self.global_bias\n",
    "# \n",
    "#         ## get prediction loss\n",
    "#         preds = self.decode(theta, beta, beta_norm, hospital_bias)\n",
    "#         recon_loss = -(preds * X).sum(1)\n",
    "#         if aggregate:\n",
    "#             recon_loss = recon_loss.mean()\n",
    "#         return recon_loss, kld_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb67792674652c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.464294Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import pandas as pd\n",
    "# \n",
    "# dedicated_hospital_ids = [2001, 1001]\n",
    "# \n",
    "# x_train_icd_data_list = []\n",
    "# x_test_icd_data_list = []\n",
    "# \n",
    "# for idx, hospital_id in enumerate(dedicated_hospital_ids):\n",
    "#     hospital_icd_data = icd_data[icd_data[\"hospitalid\"] == hospital_id].iloc[:, 3:]\n",
    "#     x = hospital_icd_data.to_numpy()\n",
    "#     \n",
    "#     one_hot_hospital_ids = np.zeros((len(x), len(dedicated_hospital_ids)))\n",
    "#     one_hot_hospital_ids[:, idx] = 1.0\n",
    "#     cls_x = np.concatenate((one_hot_hospital_ids, x), axis=1)\n",
    "#     \n",
    "#     if hospital_id == 2001:\n",
    "#         x_test_icd_data_list.append(cls_x)\n",
    "#     \n",
    "#     x_train_icd_data_list.append(cls_x)\n",
    "# \n",
    "# x_train_icd_data_np = np.concatenate(x_train_icd_data_list, axis=0)\n",
    "# x_test_icd_data_np = np.concatenate(x_test_icd_data_list, axis=0)\n",
    "# \n",
    "# x_train_tensor = torch.tensor(x_train_icd_data_np, dtype=torch.float32)\n",
    "# x_test_tensor = torch.tensor(x_test_icd_data_np, dtype=torch.float32)\n",
    "# \n",
    "# print(\"x_train_tensor: \", x_train_tensor.shape)\n",
    "# print(\"x_test_tensor: \", x_test_tensor.shape)\n",
    "# \n",
    "# train_dataset = TensorDataset(x_train_tensor, x_train_tensor)\n",
    "# test_dataset = TensorDataset(x_test_tensor, x_test_tensor)\n",
    "# \n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "# \n",
    "# x_bow_test = []\n",
    "# x_test_icd_data_original_np = x_test_icd_data_np[:, len(dedicated_hospital_ids):]\n",
    "# print(x_test_icd_data_original_np.shape)\n",
    "# for row in x_test_icd_data_original_np:\n",
    "#     word_id = list(np.where(row == 1)[0])\n",
    "#     x_bow_test.append(word_id)\n",
    "# \n",
    "# model = ETM(num_topics=18, \n",
    "#             vocab_size=len(icd_code_names), \n",
    "#             t_hidden_size=64, \n",
    "#             rho_size=16,\n",
    "#             num_hospitals=len(dedicated_hospital_ids),\n",
    "#             run_with_fl=False)\n",
    "# model = model.to(device)\n",
    "# \n",
    "# model.train()\n",
    "# \n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "# \n",
    "# epochs = 30\n",
    "# \n",
    "# recon_hist = []\n",
    "# kld_hist = []\n",
    "# elbo_hist = []\n",
    "# tc_hist = []\n",
    "# td_hist = []\n",
    "# tq_hist = []\n",
    "# \n",
    "# for epoch in range(epochs):\n",
    "#     \n",
    "#     # Train\n",
    "#     epoch_train_loss = 0.0\n",
    "#     \n",
    "#     epoch_recon_likelihood = 0.0\n",
    "#     epoch_kld = 0.0\n",
    "#     epoch_elbo = 0.0\n",
    "#     \n",
    "#     model.train()\n",
    "#     \n",
    "#     for batch_idx, (bows, normalized_bows) in enumerate(train_loader):\n",
    "#         bows = bows.to(device)\n",
    "#         normalized_bows = normalized_bows.to(device)\n",
    "#         \n",
    "#         hospital_ids = normalized_bows[:, :len(dedicated_hospital_ids)]\n",
    "#         arg_hospital_ids = torch.argmax(hospital_ids, dim=1)\n",
    "# \n",
    "#         optimizer.zero_grad()\n",
    "#         # recon_loss, kld_theta = model(bows, normalized_bows, hospital_ids=arg_hospital_ids)\n",
    "#         X = normalized_bows[:, len(dedicated_hospital_ids):]\n",
    "#         X_hospital_concat = normalized_bows\n",
    "#         recon_loss, kld_theta = model(X, X_hospital_concat, hospital_ids=arg_hospital_ids)\n",
    "#         \n",
    "#         _, topic_word_distribution = model.get_beta()\n",
    "#         diversity_penalty = topic_diversity_regularizer(topic_word_distribution)\n",
    "#         \n",
    "#         kl_weight = min(1.0, epoch * 0.05)\n",
    "#         loss = recon_loss + kl_weight * kld_theta + 0.5 * diversity_penalty\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         \n",
    "#         epoch_recon_likelihood += -recon_loss.item()\n",
    "#         epoch_kld += kld_theta.item()\n",
    "#         epoch_elbo += -loss.item()\n",
    "# \n",
    "#         epoch_train_loss += loss.item()\n",
    "# \n",
    "#     epoch_train_loss /= len(train_loader)\n",
    "#     \n",
    "#     epoch_recon_likelihood /= len(train_loader)\n",
    "#     epoch_kld /= len(train_loader)\n",
    "#     epoch_elbo /= len(train_loader)\n",
    "#     \n",
    "#     recon_hist.append(epoch_recon_likelihood)\n",
    "#     kld_hist.append(epoch_kld)\n",
    "#     elbo_hist.append(epoch_elbo)\n",
    "#     \n",
    "#     # Evaluate\n",
    "#     model.eval()\n",
    "#     \n",
    "#     _, beta_norm = model.get_beta()\n",
    "#     beta_norm = beta_norm.data.cpu().numpy()\n",
    "#     \n",
    "#     coherence = get_topic_coherence(beta_norm, x_bow_test, 5)\n",
    "#     diversity = get_topic_diversity(beta_norm, 5)\n",
    "#     quality = coherence * diversity\n",
    "#     \n",
    "#     tc_hist.append(coherence)\n",
    "#     td_hist.append(diversity)\n",
    "#     tq_hist.append(quality)\n",
    "#     \n",
    "#     print(f'Epoch [{epoch + 1}/{epochs}], Train ELBO: {epoch_elbo:.4f}, Recon likelihood: {epoch_recon_likelihood:.4f}, KL: {epoch_kld:.4f}, Topic coherence: {coherence:.4f}, Topic diversity: {diversity:.4f}, Topic quality: {quality:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c047151d3b12572",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.464733Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Plot ELBO\n",
    "# plt.title(\"ELBO of scETM\")\n",
    "# plt.plot(elbo_hist)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"ELBO\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cbd695431f10e3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.469248Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Plot Reconstruction Likelihood\n",
    "# plt.title(\"Reconstruction Likelihood of scETM\")\n",
    "# plt.plot(recon_hist)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"E[logp(x|z)]\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473fa0b5c4a203c9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.490644Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Plot KL\n",
    "# plt.title(\"KL[q(z|x) || p(z)] of scETM\")\n",
    "# plt.plot(kld_hist)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"KL[q(z|x) || p(z)]\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832faedc5b3740e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.490840Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Plot Topic Coherence\n",
    "# plt.title(\"Topic Coherence of scETM\")\n",
    "# plt.plot(tc_hist)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Topic Coherence\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da97070a516fd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.497803Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Plot Topic Diversity\n",
    "# plt.title(\"Topic Diversity of scETM\")\n",
    "# plt.plot(td_hist)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Topic Diversity\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ecb9ab313d0a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.497913Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Plot Topic Quality\n",
    "# plt.title(\"Topic Quality (Coherence x Diversity) of scETM\")\n",
    "# plt.plot(tq_hist)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Topic Quality\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731842e8cdddd8d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.504363Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# \n",
    "# _, topic_word_distribution = model.get_beta()\n",
    "# topic_word_distribution = topic_word_distribution.data.cpu().numpy()\n",
    "# \n",
    "# total_top_icd_idx = np.zeros((topic_word_distribution.shape[0], 5))  # K x 5\n",
    "# \n",
    "# for topic in range(topic_word_distribution.shape[0]):\n",
    "#     topic_icds = topic_word_distribution[topic, :]\n",
    "#     top_icd_idx = np.flip(np.argsort(topic_icds))[:5]  # Top 5 ICD codes\n",
    "#     total_top_icd_idx[topic] = top_icd_idx\n",
    "# \n",
    "# total_top_icd_idx = np.ravel(total_top_icd_idx).astype(int)\n",
    "# \n",
    "# total_top_icd = topic_word_distribution[:, total_top_icd_idx]\n",
    "# total_top_icd = total_top_icd.T\n",
    "# \n",
    "# total_top_icd_names = icd_code_names[total_top_icd_idx]\n",
    "# disease = [convert_icd9_to_disease(x) for x in total_top_icd_names]\n",
    "# disease_label = [f\"{disease[i]} - {total_top_icd_names[i]}\" for i in range(len(disease))]\n",
    "# \n",
    "# plt.figure(figsize=(8, 10))\n",
    "# \n",
    "# # Plot heatmap\n",
    "# plt.title(\"Heatmap of the Top 5 ICD Codes per Topic using scETM\")\n",
    "# ax = sns.heatmap(total_top_icd,\n",
    "#             yticklabels=disease_label,\n",
    "#             cmap='Reds', vmax=0.2)\n",
    "# \n",
    "# ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)\n",
    "# \n",
    "# y_labels = plt.gca().get_yticklabels()\n",
    "# for i, label in enumerate(y_labels):\n",
    "#     color = disease_color_map[disease[i]]\n",
    "#     label.set_color(color)\n",
    "# \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7faf6d6fec32a6b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.558601Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_bow_test = []\n",
    "# x_test_icd_data_original_np = x_test_icd_data_np[:, len(dedicated_hospital_ids):]\n",
    "# print(x_test_icd_data_original_np.shape)\n",
    "# for row in x_test_icd_data_original_np:\n",
    "#     word_id = list(np.where(row == 1)[0])\n",
    "#     x_bow_test.append(word_id)\n",
    "#     \n",
    "# coherence = get_topic_coherence(topic_word_distribution, x_bow_test, 5)\n",
    "# diversity = get_topic_diversity(topic_word_distribution, 5)\n",
    "# quality = coherence * diversity\n",
    "# \n",
    "# print(\"scETM Topic Coherence: \", coherence)\n",
    "# print(\"scETM Topic Diversity: \", diversity)\n",
    "# print(\"scETM Topic Quality: \", quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567938ec5ec4d773",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.558802Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import umap.umap_ as umap\n",
    "# from matplotlib.lines import Line2D\n",
    "# \n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import adjusted_rand_score\n",
    "# \n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# z_sc, _, _ = model.get_theta(x_test_tensor)\n",
    "# latent_sc = z_sc.data.cpu().numpy()\n",
    "# \n",
    "# reducer = umap.UMAP(n_neighbors=10, n_components=2, min_dist=0.8,\n",
    "#                             random_state=np.random.RandomState(25),\n",
    "#                             transform_seed=np.random.RandomState(25))\n",
    "# principal_components_sc = reducer.fit_transform(latent_sc)\n",
    "# print(principal_components_sc.shape)\n",
    "# \n",
    "# # kmeans = KMeans(n_clusters=12, random_state=42)\n",
    "# # kmeans.fit(principal_components_sc)\n",
    "# # kmeans_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04d2fe6b72fc9a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.560542Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# disease_labels = np.array(disease_labels)\n",
    "# \n",
    "# x_test_tensor_icd = x_test_tensor[:, len(dedicated_hospital_ids):]\n",
    "# \n",
    "# active_disease = torch.argmax(x_test_tensor_icd, dim=1)\n",
    "# no_active_disease_mask = torch.all(x_test_tensor_icd == 0, dim=1)\n",
    "# \n",
    "# patients_all_df_disease = disease_labels[active_disease.numpy()]\n",
    "# patients_all_df_disease[no_active_disease_mask.numpy()] = \"Others\"\n",
    "# \n",
    "# # K-means clustering\n",
    "# kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "# kmeans.fit(principal_components_sc)\n",
    "# kmeans_labels = kmeans.labels_\n",
    "# \n",
    "# ari_score_diagnosis = adjusted_rand_score(patients_all_df_disease, kmeans_labels)\n",
    "# print(f'Adjusted Rand Index (ARI) for Diagnosis: {ari_score_diagnosis}')\n",
    "# \n",
    "# final_np = np.hstack((principal_components_sc, patients_all_df_disease[:, None]))\n",
    "# \n",
    "# legend_elements = []\n",
    "# for label in np.unique(patients_all_df_disease):\n",
    "#     scatter_rows = final_np[np.where(final_np[:, 2] == label)]\n",
    "#     plt.scatter(scatter_rows[:, 0].astype(float), scatter_rows[:, 1].astype(float), color=disease_color_map[label], s=1.0, alpha=0.5, label=label)\n",
    "#     legend_element = Line2D([0], [0], marker='o', color='w', markerfacecolor=disease_color_map[label], markersize=8, label=label)\n",
    "#     legend_elements.append(legend_element)\n",
    "# \n",
    "# plt.title(\"K-Means Clustering of Diagnosis in scETM Latent Space\")\n",
    "# plt.text(30, 17, f'ARI: {ari_score_diagnosis:.4f}', color='red', fontsize=12, bbox=dict(facecolor='yellow', alpha=1.0))\n",
    "# plt.legend(title=\"Disease\", handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08aa6bb9799aed5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.560642Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# respiratory_indices = [i for i, label in enumerate(disease_labels) if \"Respiratory\" == label]\n",
    "# respiratory_tensor = x_test_tensor_icd[:, respiratory_indices]\n",
    "# patient_has_respiratory = (respiratory_tensor.sum(dim=1) > 0).int().numpy()\n",
    "# \n",
    "# # K-means clustering\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(principal_components_sc)\n",
    "# kmeans_labels = kmeans.labels_\n",
    "# \n",
    "# ari_score_diagnosis = adjusted_rand_score(patient_has_respiratory, kmeans_labels)\n",
    "# print(f'Adjusted Rand Index (ARI) for Respiratory: {ari_score_diagnosis}')\n",
    "# \n",
    "# palette = [\n",
    "#     '#1268fd',    # Grey\n",
    "#     '#ff5c7c',   # Red\n",
    "# ]\n",
    "# colors = sns.color_palette(palette, n_colors=2)\n",
    "# \n",
    "# final_np = np.hstack((principal_components_sc, patient_has_respiratory[:, None]))\n",
    "# \n",
    "# scatter_rows_0 = final_np[np.where(final_np[:, 2] == 0.0)]\n",
    "# plt.scatter(scatter_rows_0[:, 0], scatter_rows_0[:, 1], color=colors[0], s=1.0, alpha=0.2)\n",
    "# \n",
    "# scatter_rows_1 = final_np[np.where(final_np[:, 2] == 1.0)]\n",
    "# plt.scatter(scatter_rows_1[:, 0], scatter_rows_1[:, 1], color=colors[1], s=1.0, alpha=0.5)\n",
    "# \n",
    "# legend_elements = []\n",
    "# legend_element_0 = Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[0], markersize=8, label=\"No respiratory\")\n",
    "# legend_element_1 = Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[1], markersize=8, label=\"Has respiratory\")\n",
    "# legend_elements.append(legend_element_0)\n",
    "# legend_elements.append(legend_element_1)\n",
    "# \n",
    "# plt.title(\"K-Means Clustering of Respiratory Disease in scETM Latent Space\")\n",
    "# plt.text(30, 17, f'ARI: {ari_score_diagnosis:.4f}', color='red', fontsize=12, bbox=dict(facecolor='yellow', alpha=1.0))\n",
    "# plt.legend(title=\"Disease\", handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9007b245fda407",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.565415Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8deb46110d2e81",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.592736Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.cluster.hierarchy import linkage\n",
    "# \n",
    "# # Create a seaborn clustermap\n",
    "# plt.clf()\n",
    "# row_clusters = linkage(latent_sc, method='ward')\n",
    "# col_clusters = linkage(latent_sc.T, method='ward')\n",
    "# cmap = sns.diverging_palette(260, 350, as_cmap=True)\n",
    "# g = sns.clustermap(latent_sc, row_linkage=row_clusters, col_linkage=col_clusters, figsize=(5, 6),\n",
    "#                    yticklabels=False, cmap=cmap, center=0,\n",
    "#                    cbar_kws={'orientation': 'horizontal', 'pad': 0.1, 'shrink': 0.6},\n",
    "#                    cbar_pos=(0.45, -0.05, 0.3, 0.02))\n",
    "# \n",
    "# g.fig.suptitle(f'Heatmap scETM Latent',\n",
    "#                fontsize=12, x=0.6, y=1.02)\n",
    "# g.ax_heatmap.set_xlabel('Latent Dimension')\n",
    "# g.ax_heatmap.set_ylabel('Patients')\n",
    "# \n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde726a790d7e8dc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# scETM - Test on All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bebebafd587b9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.593238Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import umap.umap_ as umap\n",
    "# from matplotlib.lines import Line2D\n",
    "# \n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import adjusted_rand_score\n",
    "# \n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# z_sc_all, _, _ = model.get_theta(x_train_tensor)\n",
    "# latent_sc_all = z_sc_all.data.cpu().numpy()\n",
    "# \n",
    "# reducer = umap.UMAP(n_neighbors=20, n_components=2, min_dist=0.8,\n",
    "#                             random_state=np.random.RandomState(25),\n",
    "#                             transform_seed=np.random.RandomState(25))\n",
    "# principal_components_sc_all = reducer.fit_transform(latent_sc_all)\n",
    "# print(principal_components_sc_all.shape)\n",
    "# \n",
    "# # kmeans = KMeans(n_clusters=12, random_state=42)\n",
    "# # kmeans.fit(principal_components_sc_all)\n",
    "# # kmeans_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0c9be7ac1a4910",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.595956Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# disease_labels = np.array(disease_labels)\n",
    "# \n",
    "# x_train_tensor_icd = x_train_tensor[:, len(dedicated_hospital_ids):]\n",
    "# \n",
    "# active_disease = torch.argmax(x_train_tensor_icd, dim=1)\n",
    "# no_active_disease_mask = torch.all(x_train_tensor_icd == 0, dim=1)\n",
    "# \n",
    "# patients_all_df_disease = disease_labels[active_disease.numpy()]\n",
    "# patients_all_df_disease[no_active_disease_mask.numpy()] = \"Others\"\n",
    "# \n",
    "# # K-means clustering\n",
    "# kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "# kmeans.fit(principal_components_sc_all)\n",
    "# kmeans_labels = kmeans.labels_\n",
    "# \n",
    "# ari_score_diagnosis = adjusted_rand_score(patients_all_df_disease, kmeans_labels)\n",
    "# print(f'Adjusted Rand Index (ARI) for Diagnosis: {ari_score_diagnosis}')\n",
    "# \n",
    "# final_np = np.hstack((principal_components_sc_all, patients_all_df_disease[:, None]))\n",
    "# \n",
    "# legend_elements = []\n",
    "# for label in np.unique(patients_all_df_disease):\n",
    "#     scatter_rows = final_np[np.where(final_np[:, 2] == label)]\n",
    "#     plt.scatter(scatter_rows[:, 0].astype(float), scatter_rows[:, 1].astype(float), color=disease_color_map[label], s=0.2, alpha=0.2, label=label)\n",
    "#     legend_element = Line2D([0], [0], marker='o', color='w', markerfacecolor=disease_color_map[label], markersize=8, label=label)\n",
    "#     legend_elements.append(legend_element)\n",
    "# \n",
    "# plt.title(\"K-Means Clustering of Diagnosis in scETM Latent Space\")\n",
    "# plt.text(20, 17, f'ARI: {ari_score_diagnosis:.4f}', color='red', fontsize=12, bbox=dict(facecolor='yellow', alpha=1.0))\n",
    "# plt.legend(title=\"Disease\", handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa12cad8fc849b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.596007Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# circulatory_indices = [i for i, label in enumerate(disease_labels) if \"Circulatory\" == label]\n",
    "# circulatory_tensor = x_train_tensor_icd[:, circulatory_indices]\n",
    "# patient_has_circulatory = (circulatory_tensor.sum(dim=1) > 0).int().numpy()\n",
    "# \n",
    "# # K-means clustering\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(principal_components_sc_all)\n",
    "# kmeans_labels = kmeans.labels_\n",
    "# \n",
    "# ari_score_diagnosis = adjusted_rand_score(patient_has_circulatory, kmeans_labels)\n",
    "# print(f'Adjusted Rand Index (ARI) for Circulatory: {ari_score_diagnosis}')\n",
    "# \n",
    "# palette = [\n",
    "#     '#1268fd',    # Grey\n",
    "#     '#ff5c7c',   # Red\n",
    "# ]\n",
    "# colors = sns.color_palette(palette, n_colors=2)\n",
    "# \n",
    "# final_np = np.hstack((principal_components_sc_all, patient_has_circulatory[:, None]))\n",
    "# \n",
    "# scatter_rows_0 = final_np[np.where(final_np[:, 2] == 0.0)]\n",
    "# plt.scatter(scatter_rows_0[:, 0], scatter_rows_0[:, 1], color=colors[0], s=0.2, alpha=0.2)\n",
    "# \n",
    "# scatter_rows_1 = final_np[np.where(final_np[:, 2] == 1.0)]\n",
    "# plt.scatter(scatter_rows_1[:, 0], scatter_rows_1[:, 1], color=colors[1], s=0.2, alpha=0.2)\n",
    "# \n",
    "# legend_elements = []\n",
    "# legend_element_0 = Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[0], markersize=8, label=\"No circulatory\")\n",
    "# legend_element_1 = Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[1], markersize=8, label=\"Has circulatory\")\n",
    "# legend_elements.append(legend_element_0)\n",
    "# legend_elements.append(legend_element_1)\n",
    "# \n",
    "# plt.title(\"K-Means Clustering of Circulatory Disease in scETM Latent Space\")\n",
    "# plt.text(20, 17, f'ARI: {ari_score_diagnosis:.4f}', color='red', fontsize=12, bbox=dict(facecolor='yellow', alpha=1.0))\n",
    "# plt.legend(title=\"Disease\", handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a845d95172db4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.596047Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.cluster.hierarchy import linkage\n",
    "# \n",
    "# # Create a seaborn clustermap\n",
    "# plt.clf()\n",
    "# row_clusters = linkage(latent_sc_all, method='ward')\n",
    "# col_clusters = linkage(latent_sc_all.T, method='ward')\n",
    "# cmap = sns.diverging_palette(260, 350, as_cmap=True)\n",
    "# g = sns.clustermap(latent_sc_all, row_linkage=row_clusters, col_linkage=col_clusters, figsize=(5, 6),\n",
    "#                    yticklabels=False, cmap=cmap, center=0,\n",
    "#                    cbar_kws={'orientation': 'horizontal', 'pad': 0.1, 'shrink': 0.6},\n",
    "#                    cbar_pos=(0.45, -0.05, 0.3, 0.02))\n",
    "# \n",
    "# g.fig.suptitle(f'Heatmap scETM Latent (Source + Target)',\n",
    "#                fontsize=12, x=0.6, y=1.02)\n",
    "# g.ax_heatmap.set_xlabel('Latent Dimension')\n",
    "# g.ax_heatmap.set_ylabel('Patients')\n",
    "# \n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f26cdce1c530e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56c4cc14fc9ed",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.596084Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# \n",
    "# icd_data = pd.read_csv(\"../data/eicu_mimic_patient_diagnosis.csv\")\n",
    "# \n",
    "# hospital_ids = [2001, 1001]\n",
    "# x_train_icd_data = icd_data[icd_data[\"hospitalid\"].isin(hospital_ids)].iloc[:, 3:]  \n",
    "# x_test_icd_data = icd_data[icd_data[\"hospitalid\"] == 2001].iloc[:, 3:]\n",
    "# \n",
    "# x_train_icd_data_np = x_train_icd_data.to_numpy()\n",
    "# x_test_icd_data_np = x_test_icd_data.to_numpy()\n",
    "# \n",
    "# x_train_tensor = torch.tensor(x_train_icd_data_np, dtype=torch.float32)\n",
    "# train_dataset = TensorDataset(x_train_tensor, x_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "# \n",
    "# x_test_tensor = torch.tensor(x_test_icd_data_np, dtype=torch.float32)\n",
    "# test_dataset = TensorDataset(x_test_tensor, x_test_tensor)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "# \n",
    "# icd_code_names = x_train_icd_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c707dc8dba820",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.617400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# \n",
    "# \n",
    "# class ModelAutoEncoder(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(ModelAutoEncoder, self).__init__()\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(input_size, 64),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(64, 16),\n",
    "#         )\n",
    "# \n",
    "#         # Decoder part\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             # nn.Linear(16, 32),\n",
    "#             # nn.ReLU(True),\n",
    "#             nn.Linear(16, 64),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(64, input_size),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         latent = self.encoder(x)\n",
    "#         reconstruct = self.decoder(latent)\n",
    "#         return reconstruct, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ed03dcb7fde65",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.622249Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# \n",
    "# input_size = x_train_tensor.shape[1]\n",
    "# model = ModelAutoEncoder(input_size)\n",
    "# \n",
    "# criterion = nn.BCELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "# \n",
    "# # Train the autoencoder\n",
    "# num_epochs = 100\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     \n",
    "#     for batch in train_loader:\n",
    "#         inputs, _ = batch\n",
    "#         \n",
    "#         optimizer.zero_grad()\n",
    "#         \n",
    "#         outputs, _ = model(inputs)\n",
    "#         \n",
    "#         loss = criterion(outputs, inputs)\n",
    "#         \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         \n",
    "#         running_loss += loss.item() * inputs.size(0)\n",
    "#     \n",
    "#     epoch_loss = running_loss / len(train_loader.dataset)\n",
    "# \n",
    "#     # Test the autoencoder\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_loader:\n",
    "#             inputs, _ = batch\n",
    "#             outputs, _ = model(inputs)\n",
    "#             loss = criterion(outputs, inputs)\n",
    "#             test_loss += loss.item() * inputs.size(0)\n",
    "#     \n",
    "#     test_loss = test_loss / len(test_loader.dataset)\n",
    "#     \n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20387a5a94dda6a7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.622352Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import umap.umap_ as umap\n",
    "# from matplotlib.lines import Line2D\n",
    "# \n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import adjusted_rand_score\n",
    "# \n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# \n",
    "# model.eval()\n",
    "# \n",
    "# _, latent_ae = model(x_test_tensor)\n",
    "# \n",
    "# latent_ae = latent_ae.data.cpu().numpy()\n",
    "# \n",
    "# disease_labels = np.array(disease_labels)\n",
    "# \n",
    "# dedicated_hospital_ids = [2001, 1001]\n",
    "# x_test_tensor_icd = x_test_tensor[:, len(dedicated_hospital_ids):]\n",
    "# \n",
    "# active_disease = torch.argmax(x_test_tensor_icd, dim=1)\n",
    "# no_active_disease_mask = torch.all(x_test_tensor_icd == 0, dim=1)\n",
    "# \n",
    "# patients_all_df_disease = disease_labels[active_disease.numpy()]\n",
    "# patients_all_df_disease[no_active_disease_mask.numpy()] = \"Others\"\n",
    "# \n",
    "# reducer = umap.UMAP(n_neighbors=10, n_components=2, min_dist=0.8, \n",
    "#                     # metric=\"cosine\",\n",
    "#                             random_state=np.random.RandomState(25),\n",
    "#                             transform_seed=np.random.RandomState(25))\n",
    "# principal_components = reducer.fit_transform(latent_ae)\n",
    "# print(principal_components.shape)\n",
    "# \n",
    "# # K-means clustering\n",
    "# kmeans = KMeans(n_clusters=num_topics, random_state=42)\n",
    "# kmeans.fit(principal_components)\n",
    "# kmeans_labels = kmeans.labels_\n",
    "# \n",
    "# ari_score_diagnosis = adjusted_rand_score(patients_all_df_disease, kmeans_labels)\n",
    "# print(f'Adjusted Rand Index (ARI) for Diagnosis: {ari_score_diagnosis}')\n",
    "# \n",
    "# final_np = np.hstack((principal_components, patients_all_df_disease[:, None]))\n",
    "# \n",
    "# legend_elements = []\n",
    "# for label in np.unique(patients_all_df_disease):\n",
    "#     scatter_rows = final_np[np.where(final_np[:, 2] == label)]\n",
    "#     plt.scatter(scatter_rows[:, 0].astype(float), scatter_rows[:, 1].astype(float), color=disease_color_map[label], s=0.5, alpha=0.5, label=label)\n",
    "#     legend_element = Line2D([0], [0], marker='o', color='w', markerfacecolor=disease_color_map[label], markersize=8, label=label)\n",
    "#     legend_elements.append(legend_element)\n",
    "# \n",
    "# plt.title(\"K-Means Clustering of Diagnosis in Autoencoder Latent Space\")\n",
    "# plt.text(10, 25, f'ARI: {ari_score_diagnosis:.4f}', color='red', fontsize=12, bbox=dict(facecolor='yellow', alpha=1.0))\n",
    "# plt.legend(title=\"Disease\", handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b275e6c32469fb3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.627593Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import umap.umap_ as umap\n",
    "# from matplotlib.lines import Line2D\n",
    "# \n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import adjusted_rand_score\n",
    "# \n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# \n",
    "# model.eval()\n",
    "# \n",
    "# _, latent_ae = model(x_test_tensor)\n",
    "# \n",
    "# latent_ae = latent_ae.data.cpu().numpy()\n",
    "# \n",
    "# # Change output_metric\n",
    "# reducer = umap.UMAP(n_neighbors=10, n_components=2, min_dist=0.8, \n",
    "#                     # metric=\"cosine\",\n",
    "#                             random_state=np.random.RandomState(25),\n",
    "#                             transform_seed=np.random.RandomState(25))\n",
    "# principal_components = reducer.fit_transform(latent_ae)\n",
    "# print(principal_components.shape)\n",
    "# \n",
    "# disease_labels = [convert_icd9_to_disease(x) for x in icd_code_names]\n",
    "# respiratory_indices = [i for i, label in enumerate(disease_labels) if \"Respiratory\" == label]\n",
    "# respiratory_tensor = x_test_tensor[:, respiratory_indices]\n",
    "# patient_has_respiratory = (respiratory_tensor.sum(dim=1) > 0).int().numpy()\n",
    "# \n",
    "# # K-means clustering\n",
    "# kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "# kmeans.fit(principal_components)\n",
    "# kmeans_labels = kmeans.labels_\n",
    "# \n",
    "# ari_score_diagnosis = adjusted_rand_score(patient_has_respiratory, kmeans_labels)\n",
    "# print(f'Adjusted Rand Index (ARI) for Respiratory: {ari_score_diagnosis}')\n",
    "# \n",
    "# palette = [\n",
    "#     '#1268fd',    # Grey\n",
    "#     '#ff5c7c',   # Red\n",
    "# ]\n",
    "# colors = sns.color_palette(palette, n_colors=2)\n",
    "# \n",
    "# final_np = np.hstack((principal_components, patient_has_respiratory[:, None]))\n",
    "# \n",
    "# scatter_rows_0 = final_np[np.where(final_np[:, 2] == 0.0)]\n",
    "# plt.scatter(scatter_rows_0[:, 0], scatter_rows_0[:, 1], color=colors[0], s=1.0, alpha=0.2)\n",
    "# \n",
    "# scatter_rows_1 = final_np[np.where(final_np[:, 2] == 1.0)]\n",
    "# plt.scatter(scatter_rows_1[:, 0], scatter_rows_1[:, 1], color=colors[1], s=1.0, alpha=0.5)\n",
    "# \n",
    "# legend_elements = []\n",
    "# legend_element_0 = Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[0], markersize=8, label=\"No respiratory\")\n",
    "# legend_element_1 = Line2D([0], [0], marker='o', color='w', markerfacecolor=colors[1], markersize=8, label=\"Has respiratory\")\n",
    "# legend_elements.append(legend_element_0)\n",
    "# legend_elements.append(legend_element_1)\n",
    "# \n",
    "# plt.title(\"K-Means Clustering of Respiratory Disease in Autoencoder Latent Space\")\n",
    "# plt.text(10, 25, f'ARI: {ari_score_diagnosis:.4f}', color='red', fontsize=12, bbox=dict(facecolor='yellow', alpha=1.0))\n",
    "# plt.legend(title=\"Disease\", handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2784c26a97e59568",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-08T07:47:39.627685Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.cluster.hierarchy import linkage\n",
    "# \n",
    "# # Create a seaborn clustermap\n",
    "# plt.clf()\n",
    "# row_clusters = linkage(latent_ae, method='ward')\n",
    "# col_clusters = linkage(latent_ae.T, method='ward')\n",
    "# cmap = sns.diverging_palette(260, 350, as_cmap=True)\n",
    "# g = sns.clustermap(latent_ae, row_linkage=row_clusters, col_linkage=col_clusters, figsize=(5, 6),\n",
    "#                    yticklabels=False, cmap=cmap, center=0,\n",
    "#                    cbar_kws={'orientation': 'horizontal', 'pad': 0.1, 'shrink': 0.6},\n",
    "#                    cbar_pos=(0.45, -0.05, 0.3, 0.02))\n",
    "# \n",
    "# g.fig.suptitle(f'Heatmap Autoencoder Latent',\n",
    "#                fontsize=12, x=0.6, y=1.02)\n",
    "# g.ax_heatmap.set_xlabel('Latent Dimension')\n",
    "# g.ax_heatmap.set_ylabel('Patients')\n",
    "# \n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
