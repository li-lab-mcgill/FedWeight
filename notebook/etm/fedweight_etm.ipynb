{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-29T20:49:45.192310Z",
     "start_time": "2025-01-29T20:49:44.187257Z"
    }
   },
   "id": "5d035fcb88f93dca",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mike/Documents/Project/FedWeight/FedWeight_eICU/notebook/etm\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-29T20:49:45.319704Z",
     "start_time": "2025-01-29T20:49:45.198782Z"
    }
   },
   "id": "bc82ca873ac7d5ce",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/mikezhu/fed_weight_jupyter/notebook'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Change the directory to your scratch folder\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchdir\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/scratch/mikezhu/fed_weight_jupyter/notebook\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/scratch/mikezhu/fed_weight_jupyter/notebook'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Change the directory to your scratch folder\n",
    "# os.chdir('/scratch/mikezhu/fed_weight_jupyter/notebook')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-29T20:49:45.479094Z",
     "start_time": "2025-01-29T20:49:45.315801Z"
    }
   },
   "id": "cba17a2610b1423",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pwd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-29T20:49:45.698265Z",
     "start_time": "2025-01-29T20:49:45.481861Z"
    }
   },
   "id": "1ea619042da3a071",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.483855Z"
    }
   },
   "id": "842e17845fd68ab3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "t_hidden_size = 512\n",
    "rho_size = 512\n",
    "num_topics = 64\n",
    "enc_drop = 0.2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.486267Z"
    }
   },
   "id": "39d96dbbddde476d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "icd_data = pd.read_csv(\"../data/eicu_mimic_patient_diagnosis.csv\")\n",
    "icd_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.488712Z"
    }
   },
   "id": "5ab61a2705f5b1c4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.491422Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "icd_data = pd.read_csv(\"../data/eicu_mimic_patient_diagnosis.csv\")\n",
    "\n",
    "hospital_ids = [2001, 1001]\n",
    "\n",
    "readmit_interval_threshold = {\n",
    "    2001: 180, # Whether MIMIC patients will readmit to hospital within 180 days\n",
    "    1001: 2 # Whether eICU patients will readmit to ICU within 2 days\n",
    "}\n",
    "\n",
    "train_loaders = {}\n",
    "train_icds = {}\n",
    "test_icds = {}\n",
    "x_bow_tests = {}\n",
    "\n",
    "train_readmit_row_ids = {}\n",
    "test_readmit_row_ids = {}\n",
    "\n",
    "train_label_deaths = {}\n",
    "test_label_deaths = {}\n",
    "\n",
    "train_label_readmit = {}\n",
    "test_label_readmit = {}\n",
    "\n",
    "for hospital_id in hospital_ids:\n",
    "    \n",
    "    hospital_data = icd_data[icd_data[\"hospitalid\"] == hospital_id]\n",
    "    train_data, test_data = train_test_split(hospital_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    x_train = train_data.iloc[:, 4:].to_numpy()\n",
    "    x_test = test_data.iloc[:, 4:].to_numpy()\n",
    "    \n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(x_train_tensor, x_train_tensor)  # Use the same tensor for inputs and targets\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "    train_loaders[hospital_id] = train_loader\n",
    "    \n",
    "    train_icds[hospital_id] = x_train_tensor\n",
    "    test_icds[hospital_id] = x_test_tensor\n",
    "    \n",
    "    # Bag of words\n",
    "    x_bow_test = []\n",
    "    for row in x_test:\n",
    "        word_id = list(np.where(row == 1)[0])\n",
    "        x_bow_test.append(word_id)\n",
    "    x_bow_tests[hospital_id] = x_bow_test\n",
    "    \n",
    "    # Readmission patients row ids\n",
    "    train_data_np = train_data.to_numpy()\n",
    "    train_readmit_patients_row_ids = np.where(train_data_np[:, 3] == 1)[0]\n",
    "    train_readmit_row_ids[hospital_id] = train_readmit_patients_row_ids\n",
    "    \n",
    "    test_data_np = test_data.to_numpy()\n",
    "    test_readmit_patients_row_ids = np.where(test_data_np[:, 3] == 1)[0]\n",
    "    test_readmit_row_ids[hospital_id] = test_readmit_patients_row_ids\n",
    "    \n",
    "    # Label death in readmission\n",
    "    y_death_train = train_data.iloc[train_readmit_patients_row_ids, 2].to_numpy()\n",
    "    y_death_test = test_data.iloc[test_readmit_patients_row_ids, 2].to_numpy()\n",
    "    train_label_deaths[hospital_id] = y_death_train\n",
    "    test_label_deaths[hospital_id] = y_death_test\n",
    "    \n",
    "    # # Label readmission interval\n",
    "    # y_readmit_train = train_data.iloc[train_readmit_patients_row_ids, 3].to_numpy()\n",
    "    # y_readmit_test = test_data.iloc[test_readmit_patients_row_ids, 3].to_numpy()\n",
    "    # \n",
    "    # threshold = readmit_interval_threshold[hospital_id]\n",
    "    # \n",
    "    # y_readmit_train = np.where(y_readmit_train <= threshold, 1, 0)\n",
    "    # y_readmit_test = np.where(y_readmit_test <= threshold, 1, 0)\n",
    "    # \n",
    "    # train_label_readmit[hospital_id] = y_readmit_train\n",
    "    # test_label_readmit[hospital_id] = y_readmit_test\n",
    "\n",
    "\n",
    "icd_code_names = icd_data.columns[4:]\n",
    "icd_code_names"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "train_data_total = icd_data\n",
    "x_train = train_data_total.iloc[:, 4:].to_numpy()\n",
    "x_bow_train = []\n",
    "for row in x_train:\n",
    "    word_id = list(np.where(row == 1)[0])\n",
    "    x_bow_train.append(word_id)\n",
    "\n",
    "# Train Word2Vec embeddings\n",
    "# word2vec_model = Word2Vec(sentences=x_bow_train, vector_size=rho_size, window=5, min_count=1, sg=1)\n",
    "# pretrained_rho = word2vec_model.wv.vectors\n",
    "# pretrained_rho_tensor = torch.tensor(pretrained_rho, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.495292Z"
    }
   },
   "id": "69abe8abeaf7beda"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_icd9_to_disease(icd_9):\n",
    "    if pd.isna(icd_9):\n",
    "        return \"Others\"\n",
    "    primary_icd9 = icd_9.split(',')[0].strip()\n",
    "    try:\n",
    "        # Convert the input to a float to handle both numeric and decimal ICD-9 codes\n",
    "        icd_9_float = float(primary_icd9)\n",
    "\n",
    "        # Check the ICD-9 code against the known ranges\n",
    "        if 1 <= icd_9_float <= 139.9:\n",
    "            return \"Infection\"\n",
    "        elif 140 <= icd_9_float <= 239.9:\n",
    "            return \"Neoplasms\"\n",
    "        elif 240 <= icd_9_float <= 279.9:\n",
    "            return \"Endocrine\"\n",
    "        elif 280 <= icd_9_float <= 289.9:\n",
    "            return \"Blood\"\n",
    "        elif 290 <= icd_9_float <= 319:\n",
    "            return \"Mental\"\n",
    "        elif 320 <= icd_9_float <= 389.9:\n",
    "            return \"Nervous\"\n",
    "        elif 390 <= icd_9_float <= 459.9:\n",
    "            return \"Circulatory\"\n",
    "        elif 460 <= icd_9_float <= 519.9:\n",
    "            return \"Respiratory\"\n",
    "        elif 520 <= icd_9_float <= 579.9:\n",
    "            return \"Digestive\"\n",
    "        elif 580 <= icd_9_float <= 629.9:\n",
    "            return \"Genitourinary\"\n",
    "        elif 630 <= icd_9_float <= 676.9:\n",
    "            return \"Pregnancy\"\n",
    "        elif 680 <= icd_9_float <= 709.9:\n",
    "            return \"Skin\"\n",
    "        elif 710 <= icd_9_float <= 739.9:\n",
    "            return \"Musculoskeletal\"\n",
    "        elif 740 <= icd_9_float <= 759.9:\n",
    "            return \"Congenital\"\n",
    "        elif 760 <= icd_9_float <= 799.9:\n",
    "            return \"Perinatal\"\n",
    "        elif 800 <= icd_9_float <= 999.9:\n",
    "            return \"Poisoning\"\n",
    "        elif icd_9.startswith(\"V\"):\n",
    "            return \"Others\"\n",
    "        else:\n",
    "            return \"Others\"\n",
    "    \n",
    "    except ValueError:\n",
    "        return \"Others\"\n",
    "\n",
    "disease_color_map = {\n",
    "    \"Infection\": \"#005896\",\n",
    "    \"Neoplasms\": \"#dc5f00\",      # SteelBlue\n",
    "    \"Endocrine\": \"#008002\",      # LimeGreen\n",
    "    \"Blood\": \"#b40005\",          # Crimson\n",
    "    \"Mental\": \"#74499c\",         # DarkViolet\n",
    "    \"Nervous\": \"#6c382e\",        # Gold\n",
    "    \"Circulatory\": \"#ab3db3\",    # OrangeRed\n",
    "    \"Respiratory\": \"#2e2e2e\",    # DarkTurquoise\n",
    "    \"Digestive\": \"#9c9c00\",      # DeepPink\n",
    "    \"Genitourinary\": \"#009eac\",  # MediumSlateBlue\n",
    "    \"Pregnancy\": \"#abcc25\",      # HotPink\n",
    "    \"Skin\": \"#f06e60\",           # SaddleBrown\n",
    "    \"Musculoskeletal\": \"#3bd156\",# DarkOliveGreen\n",
    "    \"Congenital\": \"#c7b228\",     # BlueViolet\n",
    "    \"Perinatal\": \"#ff5c7c\",      # IndianRed\n",
    "    \"Poisoning\": \"#1268fd\",      # DarkOrange\n",
    "    \"Others\": \"#696969\",         # DimGray\n",
    "    \"Unknown\": \"#808080\"         # Gray\n",
    "}\n",
    "\n",
    "hospital_color_map = {\n",
    "    1001: \"#1268fd\",\n",
    "    2001: \"#ff5c7c\",\n",
    "}\n",
    "\n",
    "target_hospital_ids = {\n",
    "    1001: 2001,\n",
    "    2001: 1001,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.498473Z"
    }
   },
   "id": "76dd8a0931387f29",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "icd_code_dict = dict() # Key: disease category, Value: list of ICD codes\n",
    "for icd_code in icd_code_names:\n",
    "    disease = convert_icd9_to_disease(icd_code)\n",
    "    if disease in icd_code_dict:\n",
    "        icd_code_dict[disease].append(icd_code)\n",
    "    else:\n",
    "        icd_code_dict[disease] = [icd_code]\n",
    "\n",
    "icd_code_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.501175Z"
    }
   },
   "id": "a1c07ee7fe5ac6c2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "patient_icd_data = icd_data.iloc[:, 4:]\n",
    "\n",
    "total_feature_sum_dict = {}\n",
    "for feature in patient_icd_data.columns:\n",
    "    \n",
    "    feature_sum = patient_icd_data[feature].sum()\n",
    "    feature_name = convert_icd9_to_disease(feature)\n",
    "    print(f\"{feature_name}: {feature_sum}\")\n",
    "    \n",
    "    if feature_name in total_feature_sum_dict:\n",
    "        total_feature_sum_dict[feature_name] += feature_sum\n",
    "    else:\n",
    "        total_feature_sum_dict[feature_name] = feature_sum\n",
    "        \n",
    "print(total_feature_sum_dict)\n",
    "\n",
    "total_feature_sum_list = []\n",
    "for feature in patient_icd_data.columns:\n",
    "    \n",
    "    feature_name = convert_icd9_to_disease(feature)\n",
    "    feature_sum = total_feature_sum_dict[feature_name]\n",
    "    total_feature_sum_list.append(feature_sum)\n",
    "    \n",
    "feature_sums_tensor = torch.tensor(total_feature_sum_list)\n",
    "feature_sums_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.503970Z"
    }
   },
   "id": "ec6c3dbfe7b1a388",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_common_icds(input, feature_sums_tensor):\n",
    "    \n",
    "    most_common_icd_names = []\n",
    "    least_common_icd_names = []\n",
    "\n",
    "    for row in input:\n",
    "        \n",
    "        active_indices = (row == 1).nonzero(as_tuple=True)[0].cpu().numpy()\n",
    "\n",
    "        if len(active_indices) == 0:\n",
    "            least_common_icd_names.append(\"Others\")\n",
    "        else:\n",
    "            active_sums = feature_sums_tensor[active_indices]\n",
    "            _, max_idx = torch.max(active_sums, dim=0)\n",
    "            most_common_feature_idx = active_indices[max_idx]\n",
    "            most_common_feature_icd = patient_icd_data.columns[most_common_feature_idx.item()]\n",
    "            most_common_feature_name = convert_icd9_to_disease(most_common_feature_icd)\n",
    "\n",
    "            most_common_icd_names.append(most_common_feature_name)\n",
    "\n",
    "            _, min_idx = torch.min(active_sums, dim=0)\n",
    "            least_common_feature_idx = active_indices[min_idx]\n",
    "            least_common_feature_icd = patient_icd_data.columns[least_common_feature_idx.item()]\n",
    "            least_common_feature_name = convert_icd9_to_disease(least_common_feature_icd)\n",
    "\n",
    "            least_common_icd_names.append(least_common_feature_name)\n",
    "\n",
    "    return most_common_icd_names, least_common_icd_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.506937Z"
    }
   },
   "id": "8d46bf9610192e31",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_topic_diversity(beta, topk):\n",
    "    num_topics = beta.shape[0]\n",
    "    list_w = np.zeros((num_topics, topk))\n",
    "    for k in range(num_topics):\n",
    "        idx = beta[k, :].argsort()[-topk:][::-1]\n",
    "        list_w[k, :] = idx\n",
    "    n_unique = len(np.unique(list_w))\n",
    "    TD = n_unique / (topk * num_topics)\n",
    "    return TD\n",
    "\n",
    "def get_topic_coherence(beta, data, topk):\n",
    "    D = len(data)  ## number of docs...data is list of documents\n",
    "    TC = []\n",
    "    num_topics = len(beta)\n",
    "    counter = 0\n",
    "    for k in range(num_topics):\n",
    "        top_10 = list(beta[k].argsort()[-topk:][::-1])\n",
    "        TC_k = 0\n",
    "        for i, word in enumerate(top_10):\n",
    "            # get D(w_i)\n",
    "            D_wi = get_document_frequency(data, word)\n",
    "            j = i + 1\n",
    "            tmp = 0\n",
    "            while j < len(top_10) and j > i:\n",
    "                # get D(w_j) and D(w_i, w_j)\n",
    "                D_wj, D_wi_wj = get_document_frequency(data, word, top_10[j])\n",
    "                # get f(w_i, w_j)\n",
    "                if D_wi_wj == 0:\n",
    "                    f_wi_wj = -1\n",
    "                else:\n",
    "                    f_wi_wj = -1 + (np.log(D_wi) + np.log(D_wj) - 2.0 * np.log(D)) / (np.log(D_wi_wj) - np.log(D))\n",
    "                # update tmp:\n",
    "                tmp += f_wi_wj\n",
    "                j += 1\n",
    "                counter += 1\n",
    "            # update TC_k\n",
    "            TC_k += tmp\n",
    "        TC.append(TC_k)\n",
    "    TC = np.mean(TC) / counter\n",
    "    TC = (TC + 1) / 2\n",
    "    return TC\n",
    "\n",
    "def get_document_frequency(data, wi, wj=None):\n",
    "    if wj is None:\n",
    "        D_wi = 0\n",
    "        for l in range(len(data)):\n",
    "            doc = data[l]\n",
    "            if wi in doc:\n",
    "                D_wi += 1\n",
    "        return D_wi\n",
    "    D_wj = 0\n",
    "    D_wi_wj = 0\n",
    "    for l in range(len(data)):\n",
    "        doc = data[l]\n",
    "        if wj in doc:\n",
    "            D_wj += 1\n",
    "            if wi in doc:\n",
    "                D_wi_wj += 1\n",
    "    return D_wj, D_wi_wj"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.508835Z"
    }
   },
   "id": "66385657e4308ae7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def top_k_precision(y_true, y_probs, k=3):\n",
    "    top_k_indices = np.argsort(y_probs)[-k:][::-1]\n",
    "    true_positives_in_top_k = np.sum(y_true[top_k_indices])\n",
    "    top_k_precision = true_positives_in_top_k / k\n",
    "    return top_k_precision"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.511782Z"
    }
   },
   "id": "348efd66f0119f2e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def need_aggregate(key):\n",
    "    if key.startswith(\"rho\") or key.startswith(\"q_theta\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def average_weights(client_model_states, client_data_sizes):\n",
    "    \"\"\"Returns the weighted average of the model states provided by each client.\n",
    "\n",
    "    Args:\n",
    "    client_model_states (list): List of model states (dictionaries) from each client.\n",
    "    client_data_sizes (list): List of data sizes for each client, used as weights.\n",
    "\n",
    "    Returns:\n",
    "    dict: The averaged model state dictionary.\n",
    "    \"\"\"\n",
    "    total_data_points = sum(client_data_sizes)\n",
    "    \n",
    "    avg_state = {}\n",
    "    for key, value in client_model_states[0].items():\n",
    "        if need_aggregate(key):\n",
    "            avg_state[key] = torch.zeros_like(value)\n",
    "    \n",
    "    for i, client_state in enumerate(client_model_states):\n",
    "        weight = client_data_sizes[i] / total_data_points\n",
    "        for key in avg_state.keys():\n",
    "            avg_state[key] += client_state[key] * weight\n",
    "    \n",
    "    return avg_state\n",
    "\n",
    "\n",
    "def average_evaluation(evaluations, client_data_sizes):\n",
    "    \"\"\"Returns the weighted average of the evaluation metrics provided by each client.\n",
    "\n",
    "    Args:\n",
    "    evaluations (list): List of evaluation metrics from each client.\n",
    "    client_data_sizes (list): List of data sizes for each client, used as weights.\n",
    "\n",
    "    Returns:\n",
    "    float: The averaged evaluation metric.\n",
    "    \"\"\"\n",
    "    total_data_points = sum(client_data_sizes)\n",
    "    \n",
    "    avg_evaluation = 0.0\n",
    "    \n",
    "    for i, evaluation in enumerate(evaluations):\n",
    "        weight = client_data_sizes[i] / total_data_points\n",
    "        avg_evaluation += evaluation * weight\n",
    "    \n",
    "    return avg_evaluation\n",
    "\n",
    "\n",
    "def load_weights(model, updated_state_dict):\n",
    "    original_state = model.state_dict()\n",
    "\n",
    "    current_state_dict = {}\n",
    "    for key, value in original_state.items():\n",
    "        if need_aggregate(key) and key in updated_state_dict:\n",
    "            # If need_aggregate is True, use the value from state_dict\n",
    "            current_state_dict[key] = updated_state_dict[key]\n",
    "        else:\n",
    "            # Otherwise, use the original model state\n",
    "            current_state_dict[key] = original_state[key]\n",
    "            \n",
    "    model.load_state_dict(current_state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.513520Z"
    }
   },
   "id": "6e62f016ab98554e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def topic_diversity_regularizer(topic_matrix, threshold=0.1):\n",
    "    # num_topics = topic_matrix.size(0)\n",
    "    # diversity_penalty = 0\n",
    "    # for i in range(num_topics):\n",
    "    #     for j in range(i + 1, num_topics):\n",
    "    #         similarity = F.cosine_similarity(topic_matrix[i], topic_matrix[j], dim=0)\n",
    "    #         diversity_penalty += torch.max(torch.tensor(0.0), similarity - threshold)\n",
    "    # return diversity_penalty\n",
    "    normalized_topic_matrix = F.normalize(topic_matrix, p=2, dim=1)\n",
    "    cosine_sim_matrix = torch.matmul(normalized_topic_matrix, normalized_topic_matrix.t())\n",
    "    mask = torch.eye(cosine_sim_matrix.size(0), device=cosine_sim_matrix.device).bool()\n",
    "    cosine_sim_matrix = cosine_sim_matrix.masked_fill(mask, 0)\n",
    "    diversity_penalty = torch.sum(torch.relu(cosine_sim_matrix - threshold)) / 2\n",
    "    return diversity_penalty"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.515310Z"
    }
   },
   "id": "4f44bb994b98be1d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "disease_labels = [convert_icd9_to_disease(x) for x in icd_code_names]\n",
    "disease_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.516979Z"
    }
   },
   "id": "a98c5e658859158",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# made_hiddens = \"860,860,860\"\n",
    "\n",
    "total_made_hiddens = {\n",
    "    1001: \"860,860,860\",\n",
    "    2001: \"860,860\",\n",
    "}\n",
    "\n",
    "total_made_learning_rate = {\n",
    "    1001: 5e-4,\n",
    "    2001: 5e-4,\n",
    "}\n",
    "\n",
    "total_made_epochs = {\n",
    "    1001: 150,\n",
    "    2001: 100,\n",
    "}\n",
    "\n",
    "num_masks = 1\n",
    "natural_ordering = False\n",
    "# made_learning_rate = 5e-4\n",
    "made_weight_decay = 1e-5\n",
    "made_batch_size = 64\n",
    "samples = 10\n",
    "resample_every = 20\n",
    "# made_epochs = 200"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.519115Z"
    }
   },
   "id": "7b1435f7e753de7c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    \"\"\" same as Linear except has a configurable mask on the weights \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.mask * self.weight, self.bias)\n",
    "\n",
    "\n",
    "class MADE(nn.Module):\n",
    "\n",
    "    def __init__(self, nin, hidden_sizes, nout, num_masks=1, natural_ordering=False):\n",
    "        \"\"\"\n",
    "        nin: integer; number of inputs\n",
    "        hidden sizes: a list of integers; number of units in hidden layers\n",
    "        nout: integer; number of outputs, which usually collectively parameterize some kind of 1D distribution\n",
    "              note: if nout is e.g. 2x larger than nin (perhaps the mean and std), then the first nin\n",
    "              will be all the means and the second nin will be stds. i.e. output dimensions depend on the\n",
    "              same input dimensions in \"chunks\" and should be carefully decoded downstream appropriately.\n",
    "              the output of running the tests for this file makes this a bit more clear with examples.\n",
    "        num_masks: can be used to train ensemble over orderings/connections\n",
    "        natural_ordering: force natural ordering of dimensions, don't use random permutations\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        assert self.nout % self.nin == 0, \"nout must be integer multiple of nin\"\n",
    "\n",
    "        # define a simple MLP neural net\n",
    "        self.net = []\n",
    "        hs = [nin] + hidden_sizes + [nout]\n",
    "        for h0, h1 in zip(hs, hs[1:]):\n",
    "            self.net.extend([\n",
    "                MaskedLinear(h0, h1),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self.net.pop()  # pop the last ReLU for the output layer\n",
    "        # self.net.append(nn.Sigmoid()) # Use categorical cross-entropy loss\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "        # seeds for orders/connectivities of the model ensemble\n",
    "        self.natural_ordering = natural_ordering\n",
    "        self.num_masks = num_masks\n",
    "        self.seed = 0  # for cycling through num_masks orderings\n",
    "\n",
    "        self.m = {}\n",
    "        self.update_masks()  # builds the initial self.m connectivity\n",
    "        # note, we could also precompute the masks and cache them, but this\n",
    "        # could get memory expensive for large number of masks.\n",
    "\n",
    "    def update_masks(self):\n",
    "        \n",
    "        L = len(self.hidden_sizes)\n",
    "\n",
    "        # fetch the next seed and construct a random stream\n",
    "        rng = np.random.RandomState(self.seed)\n",
    "        self.seed = (self.seed + 1) % self.num_masks\n",
    "\n",
    "        # sample the order of the inputs and the connectivity of all neurons\n",
    "        self.m[-1] = np.arange(\n",
    "            self.nin) if self.natural_ordering else rng.permutation(self.nin)\n",
    "        for l in range(L):\n",
    "            self.m[l] = rng.randint(\n",
    "                self.m[l - 1].min(), self.nin - 1, size=self.hidden_sizes[l])\n",
    "\n",
    "        # construct the mask matrices\n",
    "        masks = [self.m[l - 1][:, None] <= self.m[l][None, :]\n",
    "                 for l in range(L)]\n",
    "        masks.append(self.m[L - 1][:, None] < self.m[-1][None, :])\n",
    "\n",
    "        # handle the case where nout = nin * k, for integer k > 1\n",
    "        if self.nout > self.nin:\n",
    "            k = int(self.nout / self.nin)\n",
    "            # replicate the mask across the other outputs\n",
    "            masks[-1] = np.concatenate([masks[-1]] * k, axis=1)\n",
    "\n",
    "        # set the masks in all MaskedLinear layers\n",
    "        layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)]\n",
    "        for l, m in zip(layers, masks):\n",
    "            l.set_mask(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.521390Z"
    }
   },
   "id": "9bcfd4b6fdc9be1e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class ETM(nn.Module):\n",
    "    def __init__(self, num_topics, vocab_size, t_hidden_size, rho_size, enc_drop=0.5):\n",
    "        super(ETM, self).__init__()\n",
    "\n",
    "        ## define hyperparameters\n",
    "        self.num_topics = num_topics\n",
    "        self.vocab_size = vocab_size\n",
    "        self.t_hidden_size = t_hidden_size\n",
    "        self.rho_size = rho_size\n",
    "        self.enc_drop = enc_drop\n",
    "        self.t_drop = nn.Dropout(enc_drop)\n",
    "        \n",
    "        ## define the word embedding matrix \\rho\n",
    "        self.rho = nn.Linear(rho_size, vocab_size, bias=False)\n",
    "        \n",
    "        # with torch.no_grad():\n",
    "        #     self.rho.weight = nn.Parameter(pretrained_rho_tensor.T)\n",
    "        #     self.rho.weight.requires_grad = False\n",
    "\n",
    "        ## define the matrix containing the topic embeddings\n",
    "        self.alphas = nn.Linear(rho_size, num_topics, bias=False)\n",
    "    \n",
    "        ## define variational distribution for \\theta_{1:D} via amortizartion\n",
    "        # print(vocab_size, \" THE Vocabulary size is here \")\n",
    "        self.q_theta = nn.Sequential(\n",
    "                nn.Linear(vocab_size, t_hidden_size),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "        self.mu_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
    "        self.logsigma_q_theta = nn.Linear(t_hidden_size, num_topics, bias=True)\n",
    "        \n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Returns a sample from a Gaussian distribution via reparameterization.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar) \n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul_(std).add_(mu)\n",
    "        else:\n",
    "            # During inference time, there is no need for random sampling. \n",
    "            # Instead, the model can use the mean directly, which is a point estimate of the latent variable\n",
    "            # This avoids unnecessary randomness during inference or testing.\n",
    "            return mu\n",
    "\n",
    "    def encode(self, bows):\n",
    "        \"\"\"Returns paramters of the variational distribution for \\theta.\n",
    "\n",
    "        input: bows\n",
    "                batch of bag-of-words...tensor of shape bsz x V\n",
    "        output: mu_theta, log_sigma_theta\n",
    "        \"\"\"\n",
    "        q_theta = self.q_theta(bows)\n",
    "        if self.enc_drop > 0:\n",
    "            q_theta = self.t_drop(q_theta)\n",
    "        mu_theta = self.mu_q_theta(q_theta)\n",
    "        logsigma_theta = self.logsigma_q_theta(q_theta)\n",
    "        kl_theta = -0.5 * torch.sum(1 + logsigma_theta - mu_theta.pow(2) - logsigma_theta.exp(), dim=-1).mean()\n",
    "        \n",
    "        return mu_theta, logsigma_theta, kl_theta\n",
    "\n",
    "    def get_beta(self):\n",
    "        \"\"\"\n",
    "        This generate the description as a defintion over words\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logit = self.alphas(self.rho.weight) # torch.mm(self.rho, self.alphas)\n",
    "        except:\n",
    "            logit = self.alphas(self.rho)\n",
    "        # logit = self.alphas(self.rho.weight.T)\n",
    "        beta = F.softmax(logit, dim=0).transpose(1, 0) ## softmax over vocab dimension\n",
    "        return beta\n",
    "\n",
    "    def get_theta(self, normalized_bows, is_train=True, d=1.0):\n",
    "        \"\"\"\n",
    "        getting the topic poportion for the document passed in the normalixe bow or tf-idf\"\"\"\n",
    "        mu_theta, logsigma_theta, kld_theta = self.encode(normalized_bows)\n",
    "        z = self.reparameterize(mu_theta, logsigma_theta)\n",
    "        theta = F.softmax(z, dim=-1)\n",
    "        if not is_train:\n",
    "            theta = F.softmax(z / d, dim=-1)\n",
    "        return z, theta, kld_theta\n",
    "\n",
    "    def decode(self, theta, beta):\n",
    "        \"\"\"compute the probability of topic given the document which is equal to theta^T ** B\n",
    "\n",
    "        Args:\n",
    "            theta ([type]): [description]\n",
    "            beta ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        res = torch.mm(theta, beta)\n",
    "        \n",
    "        almost_zeros = torch.full_like(res, 1e-6)\n",
    "        results_without_zeros = res.add(almost_zeros)\n",
    "        predictions = torch.log(results_without_zeros)\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, bows, normalized_bows, theta=None, aggregate=True):\n",
    "        ## get \\theta\n",
    "        if theta is None:\n",
    "            _, theta, kld_theta = self.get_theta(normalized_bows)\n",
    "        else:\n",
    "            kld_theta = None\n",
    "\n",
    "        ## get \\beta\n",
    "        beta = self.get_beta()\n",
    "\n",
    "        ## get prediction loss\n",
    "        preds = self.decode(theta, beta)\n",
    "        recon_loss = -(preds * bows).sum(1)\n",
    "        if aggregate:\n",
    "            recon_loss = recon_loss.mean()\n",
    "        return recon_loss, kld_theta"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.523371Z"
    }
   },
   "id": "b3c916a49d19234d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import f1_score\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EvaluationUtils:\n",
    "    \"\"\" Utility class to evaluate models \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_bce(pred, y, reduction='mean'):\n",
    "        criterion = nn.BCELoss(reduction=reduction)\n",
    "        return criterion(pred, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_ce(pred, y, reduction='mean'):\n",
    "        criterion = nn.CrossEntropyLoss(reduction=reduction)\n",
    "        return criterion(pred, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_mse(pred, y, reduction='mean'):\n",
    "        criterion = nn.MSELoss(reduction=reduction)\n",
    "        return criterion(pred, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_accuracy(pred, y):\n",
    "        pred = (pred > 0.5).float()\n",
    "        return ((pred - y).abs() < 1e-2).float().mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_roc_auc(pred, y):\n",
    "        y = y.detach().cpu().numpy()\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        return roc_auc_score(y, pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pr_auc(pred, y):\n",
    "        y = y.detach().cpu().numpy()\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        precision, recall, _ = precision_recall_curve(y, pred)\n",
    "        return auc(recall, precision)\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score(pred, y):\n",
    "        y = y.detach().cpu().numpy()\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        pred_labels = np.where(pred >= 0.5, 1, 0)\n",
    "        return f1_score(y, pred_labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topic_diversity(beta, topk):\n",
    "        num_topics = beta.shape[0]\n",
    "        list_w = np.zeros((num_topics, topk))\n",
    "        for k in range(num_topics):\n",
    "            idx = beta[k, :].argsort()[-topk:][::-1]\n",
    "            list_w[k, :] = idx\n",
    "        n_unique = len(np.unique(list_w))\n",
    "        TD = n_unique / (topk * num_topics)\n",
    "        return TD\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topic_coherence(beta, data):\n",
    "        D = len(data)  ## number of docs...data is list of documents\n",
    "        TC = []\n",
    "        num_topics = len(beta)\n",
    "        counter = 0\n",
    "        for k in range(num_topics):\n",
    "            top_10 = list(beta[k].argsort()[-11:][::-1])\n",
    "            TC_k = 0\n",
    "            for i, word in enumerate(top_10):\n",
    "                # get D(w_i)\n",
    "                D_wi = EvaluationUtils.get_document_frequency(data, word)\n",
    "                j = i + 1\n",
    "                tmp = 0\n",
    "                while j < len(top_10) and j > i:\n",
    "                    # get D(w_j) and D(w_i, w_j)\n",
    "                    D_wj, D_wi_wj = EvaluationUtils.get_document_frequency(data, word, top_10[j])\n",
    "                    # get f(w_i, w_j)\n",
    "                    if D_wi_wj == 0:\n",
    "                        f_wi_wj = -1\n",
    "                    else:\n",
    "                        f_wi_wj = -1 + (np.log(D_wi) + np.log(D_wj) - 2.0 * np.log(D)) / (np.log(D_wi_wj) - np.log(D))\n",
    "                    # update tmp:\n",
    "                    tmp += f_wi_wj\n",
    "                    j += 1\n",
    "                    counter += 1\n",
    "                # update TC_k\n",
    "                TC_k += tmp\n",
    "            TC.append(TC_k)\n",
    "        TC = np.mean(TC) / counter\n",
    "        TC = (TC + 1) / 2\n",
    "        return TC\n",
    "\n",
    "    @staticmethod\n",
    "    def get_document_frequency(data, wi, wj=None):\n",
    "        if wj is None:\n",
    "            D_wi = 0\n",
    "            for l in range(len(data)):\n",
    "                doc = data[l]\n",
    "                if wi in doc:\n",
    "                    D_wi += 1\n",
    "            return D_wi\n",
    "        D_wj = 0\n",
    "        D_wi_wj = 0\n",
    "        for l in range(len(data)):\n",
    "            doc = data[l]\n",
    "            if wj in doc:\n",
    "                D_wj += 1\n",
    "                if wi in doc:\n",
    "                    D_wi_wj += 1\n",
    "        return D_wj, D_wi_wj\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.525242Z"
    }
   },
   "id": "db2e242d3215f5e3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "\n",
    "class MadeService:\n",
    "\n",
    "    def run_made(self,\n",
    "                 model, opt,\n",
    "                 x_train, x_test,\n",
    "                 split, batch_size,\n",
    "                 samples, resample_every,\n",
    "                 device):\n",
    "\n",
    "        # enable/disable grad for efficiency of forwarding test batches\n",
    "        torch.set_grad_enabled(split == 'train')\n",
    "        model.train() if split == 'train' else model.eval()\n",
    "        nsamples = 1 if split == 'train' else samples\n",
    "        x = x_train if split == 'train' else x_test\n",
    "\n",
    "        # if batch_size <= 0 or batch_size > len(x):\n",
    "        #     raise ValueError(\n",
    "        #         \"Batch size must be larger than 0 and smaller than sample size\")\n",
    "        \n",
    "        # get the logits, potentially run the same batch a number of times, resampling each time\n",
    "        xbhat = torch.zeros_like(x)\n",
    "        for step in range(nsamples):\n",
    "            # perform order/connectivity-agnostic training by resampling the masks\n",
    "            if step % resample_every == 0:  # if in test, cycle masks every time\n",
    "                model.update_masks()\n",
    "            # forward the model\n",
    "            xbhat += model(x)\n",
    "        xbhat /= nsamples\n",
    "        \n",
    "        pred = xbhat\n",
    "        loss_sample = EvaluationUtils.mean_ce(pred, x, reduction='none')  # batch_size x 1\n",
    "        \n",
    "        num_drugs_taken = torch.sum(x, dim=1) # batch_size x 1\n",
    "        loss_sample = loss_sample / num_drugs_taken # batch_size x 1\n",
    "        loss_mean = torch.mean(loss_sample)  # 1 x 1\n",
    "\n",
    "        # backward/update\n",
    "        if split == 'train':\n",
    "            opt.zero_grad()\n",
    "            loss_mean.backward()\n",
    "            opt.step()\n",
    "\n",
    "        return loss_mean, loss_sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.527211Z"
    }
   },
   "id": "42676a8a970f712d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_made(made, made_opt, made_scheduler, density_x, made_batch_size, samples, resample_every, device, hospital_id, made_epochs, made_service):\n",
    "\n",
    "    # Train MADE\n",
    "    # hidden_list = list(map(int, made_hiddens.split(',')))\n",
    "    # made = MADE(density_x.size(1), hidden_list,\n",
    "    #             density_x.size(1), num_masks=self._num_masks,\n",
    "    #             natural_ordering=self._natural_ordering)\n",
    "    # made.to(self._device)\n",
    "    # made_opt = torch.optim.Adam(made.parameters(),\n",
    "    #                             lr=made_learning_rate,\n",
    "    #                             weight_decay=made_weight_decay)\n",
    "    # made_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    #     made_opt, step_size=45, gamma=0.1)\n",
    "\n",
    "    epochs_without_improvement = 0\n",
    "    best_train_loss = float('inf')\n",
    "    for epoch in range(made_epochs):\n",
    "        made_train_loss, _ = made_service.run_made(made, made_opt,\n",
    "                                                   density_x, None,\n",
    "                                                   'train', made_batch_size,\n",
    "                                                   samples,\n",
    "                                                   resample_every,\n",
    "                                                   device)\n",
    "\n",
    "        made_scheduler.step()\n",
    "\n",
    "        if made_train_loss < best_train_loss:\n",
    "            best_train_loss = made_train_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        print(f\"Train MADE on hospital {hospital_id} - epoch: {epoch}, train loss: {made_train_loss}\")\n",
    "\n",
    "    # _, loss_for_samples = made_service.run_made(made, None,\n",
    "    #                                             None, density_x, 'test',\n",
    "    #                                             made_batch_size,\n",
    "    #                                             samples, \n",
    "    #                                             resample_every,\n",
    "    #                                             device)\n",
    "    # return loss_for_samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.529313Z"
    }
   },
   "id": "78dd0938f18a7248",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def federated_learning(client_models, client_optimizers, rounds=10, epochs=1, enc_drop=0.5, beta_=0.05, lambda_=0.1):\n",
    "    \n",
    "    elbo_hist = {}\n",
    "    kld_hist = {}\n",
    "    recon_hist = {}\n",
    "    \n",
    "    tc_hist = {}\n",
    "    td_hist = {}\n",
    "    tq_hist = {}\n",
    "    \n",
    "    made_service = MadeService()\n",
    "    \n",
    "    # Train client MADE\n",
    "    client_mades = {}\n",
    "    for client_id in hospital_ids:\n",
    "        \n",
    "        elbo_hist[client_id] = []\n",
    "        kld_hist[client_id] = []\n",
    "        recon_hist[client_id] = []\n",
    "        \n",
    "        tc_hist[client_id] = []\n",
    "        td_hist[client_id] = []\n",
    "        tq_hist[client_id] = []\n",
    "        \n",
    "        client_loader = train_loaders[client_id]\n",
    "        \n",
    "        made_hiddens_str = total_made_hiddens[client_id]\n",
    "        hidden_list = list(map(int, made_hiddens_str.split(',')))\n",
    "        client_made = MADE(client_loader.dataset.tensors[0].size(1), hidden_list,\n",
    "                    client_loader.dataset.tensors[0].size(1), num_masks=num_masks,\n",
    "                    natural_ordering=natural_ordering)\n",
    "        client_made.to(device)\n",
    "        \n",
    "        made_lr = total_made_learning_rate[client_id]\n",
    "        client_made_opt = torch.optim.Adam(client_made.parameters(),\n",
    "                                           lr=made_lr,\n",
    "                                           weight_decay=made_weight_decay)\n",
    "        client_made_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            client_made_opt, step_size=45, gamma=0.1)\n",
    "        \n",
    "        made_epochs = total_made_epochs[client_id]\n",
    "        train_made(client_made, client_made_opt, client_made_scheduler, client_loader.dataset.tensors[0], made_batch_size, samples, resample_every, device, client_id, made_epochs, made_service)\n",
    "        \n",
    "        client_mades[client_id] = client_made\n",
    "        \n",
    "    client_batch_reweight_ratios = {}\n",
    "    for client_id in hospital_ids:\n",
    "        \n",
    "        target_id = target_hospital_ids[client_id]\n",
    "        target_made = client_mades[target_id]\n",
    "        client_made = client_mades[client_id]\n",
    "        \n",
    "        client_loader = train_loaders[client_id]\n",
    "        \n",
    "        batch_reweight_ratio = {}\n",
    "        for batch_idx, (bows, normalized_bows) in enumerate(client_loader):\n",
    "            bows = bows.to(device)\n",
    "            \n",
    "            _, source_loss_for_samples = made_service.run_made(client_made, None,\n",
    "                                                None, bows, 'test',\n",
    "                                                made_batch_size,\n",
    "                                                samples, \n",
    "                                                resample_every,\n",
    "                                                device)\n",
    "                    \n",
    "            _, target_loss_for_samples = made_service.run_made(target_made, None,\n",
    "                                        None, bows, 'test',\n",
    "                                        made_batch_size,\n",
    "                                        samples, \n",
    "                                        resample_every,\n",
    "                                        device)\n",
    "            \n",
    "            loss_diff = source_loss_for_samples - target_loss_for_samples  # N x 1\n",
    "            # reweight = p_t(x) / p_s(x) = exp(-target_loss) / exp(-source_loss) = exp(source_loss - target_loss)\n",
    "            reweight = torch.exp(loss_diff)  # N x 1\n",
    "            batch_reweight_ratio[batch_idx] = reweight\n",
    "            \n",
    "        client_batch_reweight_ratios[client_id] = batch_reweight_ratio\n",
    "    \n",
    "    \n",
    "    for round in range(rounds):\n",
    "\n",
    "        client_model_states = []\n",
    "        client_data_sizes = []\n",
    "\n",
    "        for client_id in hospital_ids:\n",
    "            \n",
    "            client_loader = train_loaders[client_id]\n",
    "            client_model = client_models[client_id]\n",
    "            client_optimizer = client_optimizers[client_id]\n",
    "            \n",
    "            client_recon_likelihood = 0.0\n",
    "            client_kld = 0.0\n",
    "            client_elbo = 0.0\n",
    "            \n",
    "            client_model.train()\n",
    "            \n",
    "            target_id = target_hospital_ids[client_id]\n",
    "            if target_id is None:\n",
    "                raise ValueError(f\"Client {client_id} does not have a target.\")\n",
    "            \n",
    "            batch_reweight_ratio = client_batch_reweight_ratios[client_id]\n",
    "\n",
    "            for local_epoch in range(epochs):\n",
    "                \n",
    "                epoch_recon_likelihood = 0.0\n",
    "                epoch_kld = 0.0\n",
    "                epoch_elbo = 0.0\n",
    "                \n",
    "                for batch_idx, (bows, normalized_bows) in enumerate(client_loader):\n",
    "                    bows = bows.to(device)\n",
    "                    normalized_bows = normalized_bows.to(device)\n",
    "\n",
    "                    client_optimizer.zero_grad()\n",
    "                    \n",
    "                    recon_loss_sample, kld_theta = client_model(bows, normalized_bows)\n",
    "                    \n",
    "                    torch.set_grad_enabled(True)\n",
    "                    \n",
    "                    reweight = batch_reweight_ratio[batch_idx]\n",
    "                    reweight = reweight / reweight.sum()  # N x 1\n",
    "                    \n",
    "                    recon_loss = torch.sum(torch.mul(reweight, recon_loss_sample))\n",
    "                    \n",
    "                    # Increase topic diversity\n",
    "                    # Based on paper: https://arxiv.org/pdf/1706.00359\n",
    "                    diversity_penalty = topic_diversity_regularizer(client_model.get_beta())\n",
    "                    \n",
    "                    # Beta-VAE KL Annealing to prevent posterior collapse\n",
    "                    kl_weight = min(1.0, round * beta_)\n",
    "                    loss = recon_loss + kl_weight * kld_theta + lambda_ * diversity_penalty\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    client_optimizer.step()\n",
    "                    \n",
    "                    epoch_recon_likelihood += -recon_loss.item()\n",
    "                    epoch_kld += kld_theta.item()\n",
    "                    epoch_elbo += -loss.item()\n",
    "                    \n",
    "                epoch_recon_likelihood /= len(client_loader)\n",
    "                epoch_kld /= len(client_loader)\n",
    "                epoch_elbo /= len(client_loader)\n",
    "                \n",
    "                client_recon_likelihood += epoch_recon_likelihood\n",
    "                client_kld += epoch_kld\n",
    "                client_elbo += epoch_elbo\n",
    "                \n",
    "            client_recon_likelihood /= epochs\n",
    "            client_kld /= epochs\n",
    "            client_elbo /= epochs\n",
    "\n",
    "            client_model_states.append(client_model.state_dict())  # Save client model state\n",
    "            client_data_sizes.append(len(client_loader.dataset))\n",
    "            \n",
    "            elbo_hist[client_id].append(client_elbo)\n",
    "            kld_hist[client_id].append(client_kld)\n",
    "            recon_hist[client_id].append(client_recon_likelihood)\n",
    "\n",
    "        # Aggregate the parameters from each client\n",
    "        avg_model_state = average_weights(client_model_states, client_data_sizes)\n",
    "\n",
    "        for client_id in hospital_ids:\n",
    "            \n",
    "            # Load the averaged model back into the client model\n",
    "            client_model = client_models[client_id]\n",
    "            load_weights(client_model, avg_model_state)\n",
    "        \n",
    "            # Evaluate\n",
    "            client_model.eval()\n",
    "            \n",
    "            beta = client_model.get_beta()\n",
    "            beta = beta.data.cpu().numpy()\n",
    "            \n",
    "            x_bow_test = x_bow_tests[client_id]\n",
    "            coherence = get_topic_coherence(beta, x_bow_test, 5)\n",
    "            diversity = get_topic_diversity(beta, 5)\n",
    "            quality = coherence * diversity\n",
    "            \n",
    "            tc_hist[client_id].append(coherence)\n",
    "            td_hist[client_id].append(diversity)\n",
    "            tq_hist[client_id].append(quality)\n",
    "    \n",
    "            print(f\"Round {round + 1}/{rounds} - Client: {client_id} - ELBO: {elbo_hist[client_id][-1]:.4f} - Recon likelihood: {recon_hist[client_id][-1]:.4f} - KLD: {kld_hist[client_id][-1]:.4f}, TC: {coherence:.4f}, TD: {diversity:.4f}, TQ: {quality:.4f}\")\n",
    "        \n",
    "    return client_models, elbo_hist, kld_hist, recon_hist, tc_hist, td_hist, tq_hist"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.531327Z"
    }
   },
   "id": "e3490fe0a4099e1d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "client_models = {}\n",
    "client_optimizers = {}\n",
    "\n",
    "for client_id in hospital_ids:\n",
    "\n",
    "    client_model = ETM(num_topics=num_topics,\n",
    "                       vocab_size=len(icd_code_names),\n",
    "                       t_hidden_size=t_hidden_size,\n",
    "                       rho_size=rho_size,\n",
    "                       enc_drop=enc_drop).to(device)\n",
    "    client_models[client_id] = client_model\n",
    "\n",
    "    # Since FedWeight applies a ratio < 1 to the loss function, we add a larger learning rate\n",
    "    optimizer_fn = torch.optim.Adam(client_model.parameters(), lr=1e-4, weight_decay=5e-6)\n",
    "    client_optimizers[client_id] = optimizer_fn\n",
    "\n",
    "client_models, elbo_hist, kld_hist, recon_hist, tc_hist, td_hist, tq_hist = federated_learning(client_models, client_optimizers, rounds=20, epochs=5, enc_drop=enc_drop, beta_=0.05, lambda_=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.532872Z"
    }
   },
   "id": "b02edfbf4b2154d3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot ELBO\n",
    "plt.title(\"ELBO of ETM FedWeight (eICU)\")\n",
    "plt.plot(elbo_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.534672Z"
    }
   },
   "id": "2c1f83d2ef0c3580",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot ELBO\n",
    "plt.title(\"ELBO of ETM FedWeight of (MIMIC-III)\")\n",
    "plt.plot(elbo_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"ELBO\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.537234Z"
    }
   },
   "id": "21cb2da47cab5adc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot Reconstruction Likelihood\n",
    "plt.title(\"Reconstruction Likelihood of ETM FedWeight (eICU)\")\n",
    "plt.plot(recon_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"E[logp(x|z)]\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.539100Z"
    }
   },
   "id": "b240e07c805059c5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot Reconstruction Likelihood\n",
    "plt.title(\"Reconstruction Likelihood of ETM FedWeight (MIMIC-III)\")\n",
    "plt.plot(recon_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"E[logp(x|z)]\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.541037Z"
    }
   },
   "id": "9fab0dca2c0529ae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot KL\n",
    "plt.title(\"KL[q(z|x) || p(z)] of ETM FedWeight (eICU)\")\n",
    "plt.plot(kld_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"KL[q(z|x) || p(z)]\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.542933Z"
    }
   },
   "id": "47a1ef09b66e01b4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot KL\n",
    "plt.title(\"KL[q(z|x) || p(z)] of ETM FedWeight (MIMIC-III)\")\n",
    "plt.plot(kld_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"KL[q(z|x) || p(z)]\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.544935Z"
    }
   },
   "id": "f5bb329fee5c27f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot Topic Coherence\n",
    "plt.title(\"Topic Coherence of ETM FedWeight (eICU)\")\n",
    "plt.plot(tc_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Coherence\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.546609Z"
    }
   },
   "id": "54b0c6e903050f24",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot Topic Coherence\n",
    "plt.title(\"Topic Coherence of ETM FedWeight (MIMIC-III)\")\n",
    "plt.plot(tc_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Coherence\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.548600Z"
    }
   },
   "id": "dc85686d5a94b99f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot Topic Diversity\n",
    "plt.title(\"Topic Diversity of ETM FedWeight (eICU)\")\n",
    "plt.plot(td_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Diversity\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.549715Z"
    }
   },
   "id": "caf33d730fb2c6b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot Topic Diversity\n",
    "plt.title(\"Topic Diversity of ETM FedWeight (MIMIC-III)\")\n",
    "plt.plot(td_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Diversity\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.550886Z"
    }
   },
   "id": "645331c995db62a1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot Topic Quality\n",
    "plt.title(\"Topic Quality (Coherence x Diversity) of ETM FedWeight (eICU)\")\n",
    "plt.plot(tq_hist[1001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Quality\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.551999Z"
    }
   },
   "id": "4bc5147575f4c646",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot Topic Quality\n",
    "plt.title(\"Topic Quality (Coherence x Diversity) of ETM FedWeight (MIMIC-III)\")\n",
    "plt.plot(tq_hist[2001])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Topic Quality\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.552943Z"
    }
   },
   "id": "62ae8626078abb31",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "eicu_client_model = client_models[1001]\n",
    "eicu_topic_word_distribution = eicu_client_model.get_beta()\n",
    "eicu_topic_word_distribution = eicu_topic_word_distribution.data.cpu().numpy()\n",
    "\n",
    "total_top_icd_idx = np.zeros((eicu_topic_word_distribution.shape[0], 5))  # K x 5\n",
    "\n",
    "for topic in range(eicu_topic_word_distribution.shape[0]):\n",
    "    topic_icds = eicu_topic_word_distribution[topic, :]\n",
    "    top_icd_idx = np.flip(np.argsort(topic_icds))[:5]  # Top 5 ICD codes\n",
    "    total_top_icd_idx[topic] = top_icd_idx\n",
    "\n",
    "total_top_icd_idx = np.ravel(total_top_icd_idx).astype(int) # 5K\n",
    "\n",
    "eicu_total_top_icd = eicu_topic_word_distribution[:, total_top_icd_idx] # K x 5K\n",
    "eicu_total_top_icd = eicu_total_top_icd.T # 5K x K\n",
    "\n",
    "total_top_icd_names = icd_code_names[total_top_icd_idx] # 5K\n",
    "disease = [convert_icd9_to_disease(x) for x in total_top_icd_names]\n",
    "disease_label = [f\"{disease[i]} - {total_top_icd_names[i]}\" for i in range(len(disease))]\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "\n",
    "# Plot heatmap\n",
    "plt.title(\"Heatmap of the Top 5 ICD Codes per Topic using ETM FedWeight (eICU)\")\n",
    "ax = sns.heatmap(eicu_total_top_icd,\n",
    "            yticklabels=disease_label,\n",
    "            cmap='Reds', vmax=0.2)\n",
    "\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "y_labels = plt.gca().get_yticklabels()\n",
    "for i, label in enumerate(y_labels):\n",
    "    color = disease_color_map[disease[i]]\n",
    "    label.set_color(color)\n",
    "    \n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.553961Z"
    }
   },
   "id": "13d26299f20868ca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "mimic_client_model = client_models[2001]\n",
    "mimic_topic_word_distribution = mimic_client_model.get_beta()\n",
    "mimic_topic_word_distribution = mimic_topic_word_distribution.data.cpu().numpy()\n",
    "\n",
    "total_top_icd_idx = np.zeros((mimic_topic_word_distribution.shape[0], 5))  # K x 5\n",
    "\n",
    "for topic in range(mimic_topic_word_distribution.shape[0]):\n",
    "    topic_icds = mimic_topic_word_distribution[topic, :]\n",
    "    top_icd_idx = np.flip(np.argsort(topic_icds))[:5]  # Top 5 ICD codes\n",
    "    total_top_icd_idx[topic] = top_icd_idx\n",
    "\n",
    "total_top_icd_idx = np.ravel(total_top_icd_idx).astype(int)\n",
    "\n",
    "mimic_total_top_icd = mimic_topic_word_distribution[:, total_top_icd_idx]\n",
    "mimic_total_top_icd = mimic_total_top_icd.T\n",
    "\n",
    "total_top_icd_names = icd_code_names[total_top_icd_idx]\n",
    "disease = [convert_icd9_to_disease(x) for x in total_top_icd_names]\n",
    "disease_label = [f\"{disease[i]} - {total_top_icd_names[i]}\" for i in range(len(disease))]\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "\n",
    "# Plot heatmap\n",
    "plt.title(\"Heatmap of the Top 5 ICD Codes per Topic using ETM FedWeight (MIMIC-III)\")\n",
    "ax = sns.heatmap(mimic_total_top_icd,\n",
    "            yticklabels=disease_label,\n",
    "            cmap='Reds', vmax=0.2)\n",
    "\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)\n",
    "\n",
    "y_labels = plt.gca().get_yticklabels()\n",
    "for i, label in enumerate(y_labels):\n",
    "    color = disease_color_map[disease[i]]\n",
    "    label.set_color(color)\n",
    "    \n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.554826Z"
    }
   },
   "id": "f41eabec82d23ce",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "eicu_x_bow_test = x_bow_tests[1001]\n",
    "eicu_coherence = get_topic_coherence(eicu_topic_word_distribution, eicu_x_bow_test, 3)\n",
    "eicu_diversity = get_topic_diversity(eicu_topic_word_distribution, 3)\n",
    "eicu_quality = eicu_coherence * eicu_diversity\n",
    "\n",
    "print(\"eICU ETM FedWeight Topic Coherence: \", eicu_coherence)\n",
    "print(\"eICU ETM FedWeight Topic Diversity: \", eicu_diversity)\n",
    "print(\"eICU ETM FedWeight Topic Quality: \", eicu_quality)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.556050Z"
    }
   },
   "id": "1bfc0a16d973f76a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mimic_x_bow_test = x_bow_tests[2001]\n",
    "mimic_coherence = get_topic_coherence(mimic_topic_word_distribution, mimic_x_bow_test, 3)\n",
    "mimic_diversity = get_topic_diversity(mimic_topic_word_distribution, 3)\n",
    "mimic_quality = mimic_coherence * mimic_diversity\n",
    "\n",
    "print(\"MIMIC ETM FedWeight Topic Coherence: \", mimic_coherence)\n",
    "print(\"MIMIC ETM FedWeight Topic Diversity: \", mimic_diversity)\n",
    "print(\"MIMIC ETM FedWeight Topic Quality: \", mimic_quality)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.557419Z"
    }
   },
   "id": "4ec1be8674b6355f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "icd_code_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.558456Z"
    }
   },
   "id": "e0cfd6b84e8e795",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "disease_labels = [convert_icd9_to_disease(x) for x in icd_code_names]\n",
    "disease_labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.559511Z"
    }
   },
   "id": "139833425f0437dd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "mimic_client_model = client_models[2001]\n",
    "eicu_x_test_tensor = test_icds[1001]\n",
    "\n",
    "_, eicu_test_theta_weighted, _ = mimic_client_model.get_theta(eicu_x_test_tensor)\n",
    "eicu_test_theta_weighted = eicu_test_theta_weighted.data.cpu().numpy()\n",
    "\n",
    "eicu_test_readmit_row_ids = test_readmit_row_ids[1001]\n",
    "X_eicu_test = eicu_test_theta_weighted[eicu_test_readmit_row_ids]\n",
    "eicu_icd_input = eicu_x_test_tensor[eicu_test_readmit_row_ids]\n",
    "\n",
    "eicu_most_common_icd_names, eicu_least_common_icd_names = find_common_icds(eicu_icd_input, feature_sums_tensor)\n",
    "\n",
    "unique_diseases = np.unique(eicu_least_common_icd_names)\n",
    "row_colors = pd.Series(eicu_least_common_icd_names).map(disease_color_map).to_numpy()\n",
    "\n",
    "# Create a seaborn clustermap\n",
    "plt.clf()\n",
    "row_clusters = linkage(X_eicu_test, method='ward')\n",
    "col_clusters = linkage(X_eicu_test.T, method='ward')\n",
    "g = sns.clustermap(X_eicu_test, row_linkage=row_clusters, col_linkage=col_clusters, \n",
    "                   figsize=(5, 6),\n",
    "                   yticklabels=False, cmap='rocket_r',\n",
    "                   cbar_kws={'orientation': 'horizontal', 'pad': 0.1, 'shrink': 0.6},\n",
    "                   cbar_pos=(0.45, -0.05, 0.3, 0.02),\n",
    "                   row_colors=row_colors)\n",
    "\n",
    "g.fig.suptitle(f'Heatmap FedWeight ETM Patient-Topic Mixture (MIMIC on eICU)',\n",
    "               fontsize=12, x=0.6, y=1.02)\n",
    "g.ax_heatmap.set_xlabel('Latent Dimension')\n",
    "g.ax_heatmap.set_ylabel('Patients')\n",
    "\n",
    "legend_patches = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=disease_color_map[disease],\n",
    "                             markersize=10, label=disease) for disease in unique_diseases]\n",
    "plt.legend(handles=legend_patches, title='Disease', bbox_to_anchor=(2.0, 12), loc='lower left', borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.560710Z"
    }
   },
   "id": "c3592d587e464992",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "\n",
    "unique_diseases = icd_code_dict.keys()\n",
    "eicu_disease_p_values = {}\n",
    "\n",
    "eicu_disease_topic_p_values = np.zeros((len(unique_diseases), num_topics))\n",
    "\n",
    "for disease_idx, disease in enumerate(unique_diseases):\n",
    "    \n",
    "    # ICD codes for disease category\n",
    "    icd_codes = icd_code_dict[disease]\n",
    "    icd_code_idx = [np.where(icd_code_names == icd_code)[0][0] for icd_code in icd_codes]\n",
    "    eicu_icd_input_disease = eicu_icd_input[:, icd_code_idx]\n",
    "    \n",
    "    # Find the patients if the disease is present\n",
    "    patient_has_disease_indices = (eicu_icd_input_disease.sum(dim=1) > 0).nonzero(as_tuple=True)[0]\n",
    "    patient_no_disease_indices = (eicu_icd_input_disease.sum(dim=1) == 0).nonzero(as_tuple=True)[0]\n",
    "    patient_has_disease_indices = patient_has_disease_indices.cpu().numpy()\n",
    "    patient_no_disease_indices = patient_no_disease_indices.cpu().numpy()\n",
    "    \n",
    "    if len(patient_has_disease_indices) == 0 or len(patient_no_disease_indices) == 0:\n",
    "        continue\n",
    "    \n",
    "    smallest_p_value = np.inf\n",
    "    for topic in range(num_topics):\n",
    "        theta_topic = X_eicu_test[:, topic]\n",
    "        theta_topic_has_disease = theta_topic[patient_has_disease_indices]\n",
    "        theta_topic_no_disease = theta_topic[patient_no_disease_indices]\n",
    "        _, p_value = ranksums(theta_topic_has_disease, theta_topic_no_disease)\n",
    "        eicu_disease_topic_p_values[disease_idx][topic] = -np.log10(p_value)\n",
    "        if p_value < smallest_p_value:\n",
    "            smallest_p_value = p_value\n",
    "    \n",
    "    eicu_disease_p_values[disease] = -np.log10(smallest_p_value)\n",
    "    \n",
    "# MIMIC on eICU\n",
    "eicu_disease_p_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.561785Z"
    }
   },
   "id": "69eca9d9cd53fded",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preg_other_idx = [idx for idx in range(len(unique_diseases)) if list(unique_diseases)[idx] == \"Pregnancy\" or list(unique_diseases)[idx] == \"Others\"]\n",
    "\n",
    "eicu_disease_topic_p_values = np.delete(eicu_disease_topic_p_values, preg_other_idx, axis=0)\n",
    "unique_diseases = np.delete(list(unique_diseases), preg_other_idx)\n",
    "\n",
    "plt.figure(figsize=(5, 6))\n",
    "ax = sns.heatmap(eicu_disease_topic_p_values, cmap='Reds', cbar=True, cbar_kws={'orientation': 'horizontal', 'pad': 0.12, 'shrink': 0.8})\n",
    "\n",
    "ax.set_yticklabels(unique_diseases, rotation=0)\n",
    "\n",
    "colorbar = ax.collections[0].colorbar\n",
    "colorbar.set_label(\"-log10(p-value)\")\n",
    "\n",
    "plt.xlabel('Topics')\n",
    "plt.title('FedWeight ETM Significance of Disease-Topic Associations (MIMIC on eICU)')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.563150Z"
    }
   },
   "id": "1916725626da9a4e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "eicu_client_model = client_models[1001]\n",
    "mimic_x_test_tensor = test_icds[2001]\n",
    "\n",
    "_, mimic_test_theta_weighted, _ = eicu_client_model.get_theta(mimic_x_test_tensor)\n",
    "mimic_test_theta_weighted = mimic_test_theta_weighted.data.cpu().numpy()\n",
    "mimic_test_readmit_row_ids = test_readmit_row_ids[2001]\n",
    "\n",
    "X_mimic_test = mimic_test_theta_weighted[mimic_test_readmit_row_ids]\n",
    "mimic_icd_input = mimic_x_test_tensor[mimic_test_readmit_row_ids]\n",
    "\n",
    "mimic_most_common_icd_names, mimic_least_common_icd_names = find_common_icds(mimic_icd_input, feature_sums_tensor)\n",
    "\n",
    "unique_diseases = np.unique(mimic_least_common_icd_names)\n",
    "row_colors = pd.Series(mimic_least_common_icd_names).map(disease_color_map).to_numpy()\n",
    "\n",
    "# Create a seaborn clustermap\n",
    "plt.clf()\n",
    "row_clusters = linkage(X_mimic_test, method='ward')\n",
    "col_clusters = linkage(X_mimic_test.T, method='ward')\n",
    "g = sns.clustermap(X_mimic_test, row_linkage=row_clusters, col_linkage=col_clusters, \n",
    "                   figsize=(5, 6),\n",
    "                   yticklabels=False, cmap='rocket_r',\n",
    "                   cbar_kws={'orientation': 'horizontal', 'pad': 0.1, 'shrink': 0.6},\n",
    "                   cbar_pos=(0.45, -0.05, 0.3, 0.02),\n",
    "                   row_colors=row_colors)\n",
    "\n",
    "g.fig.suptitle(f'Heatmap FedWeight ETM Patient-Topic Mixture (eICU on MIMIC)',\n",
    "               fontsize=12, x=0.6, y=1.02)\n",
    "g.ax_heatmap.set_xlabel('Latent Dimension')\n",
    "g.ax_heatmap.set_ylabel('Patients')\n",
    "\n",
    "legend_patches = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=disease_color_map[disease],\n",
    "                             markersize=10, label=disease) for disease in unique_diseases]\n",
    "plt.legend(handles=legend_patches, title='Disease', bbox_to_anchor=(2.0, 12), loc='lower left', borderaxespad=0.)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.565697Z"
    }
   },
   "id": "ef0a04b844c8e85e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums\n",
    "\n",
    "unique_diseases = icd_code_dict.keys()\n",
    "mimic_disease_p_values = {}\n",
    "\n",
    "mimic_disease_topic_p_values = np.zeros((len(unique_diseases), num_topics))\n",
    "\n",
    "for disease_idx, disease in enumerate(unique_diseases):\n",
    "    \n",
    "    # ICD codes for disease category\n",
    "    icd_codes = icd_code_dict[disease]\n",
    "    icd_code_idx = [np.where(icd_code_names == icd_code)[0][0] for icd_code in icd_codes]\n",
    "    mimic_icd_input_disease = mimic_icd_input[:, icd_code_idx]\n",
    "    \n",
    "    # Find the patients if the disease is present\n",
    "    patient_has_disease_indices = (mimic_icd_input_disease.sum(dim=1) > 0).nonzero(as_tuple=True)[0]\n",
    "    patient_no_disease_indices = (mimic_icd_input_disease.sum(dim=1) == 0).nonzero(as_tuple=True)[0]\n",
    "    patient_has_disease_indices = patient_has_disease_indices.cpu().numpy()\n",
    "    patient_no_disease_indices = patient_no_disease_indices.cpu().numpy()\n",
    "    \n",
    "    if len(patient_has_disease_indices) == 0 or len(patient_no_disease_indices) == 0:\n",
    "        continue\n",
    "    \n",
    "    smallest_p_value = np.inf\n",
    "    for topic in range(num_topics):\n",
    "        theta_topic = X_mimic_test[:, topic]\n",
    "        theta_topic_has_disease = theta_topic[patient_has_disease_indices]\n",
    "        theta_topic_no_disease = theta_topic[patient_no_disease_indices]\n",
    "        _, p_value = ranksums(theta_topic_has_disease, theta_topic_no_disease)\n",
    "        mimic_disease_topic_p_values[disease_idx][topic] = -np.log10(p_value)\n",
    "        if p_value < smallest_p_value:\n",
    "            smallest_p_value = p_value\n",
    "    \n",
    "    mimic_disease_p_values[disease] = -np.log10(smallest_p_value)\n",
    "    \n",
    "# eICU on MIMIC\n",
    "mimic_disease_p_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.566636Z"
    }
   },
   "id": "c8a4041b8af2e3d9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "preg_other_idx = [idx for idx in range(len(unique_diseases)) if list(unique_diseases)[idx] == \"Pregnancy\" or list(unique_diseases)[idx] == \"Others\"]\n",
    "\n",
    "mimic_disease_topic_p_values = np.delete(mimic_disease_topic_p_values, preg_other_idx, axis=0)\n",
    "unique_diseases = np.delete(list(unique_diseases), preg_other_idx)\n",
    "\n",
    "plt.figure(figsize=(5, 6))\n",
    "ax = sns.heatmap(mimic_disease_topic_p_values, cmap='Reds', cbar=True, cbar_kws={'orientation': 'horizontal', 'pad': 0.12, 'shrink': 0.8})\n",
    "\n",
    "ax.set_yticklabels(unique_diseases, rotation=0)\n",
    "\n",
    "colorbar = ax.collections[0].colorbar\n",
    "colorbar.set_label(\"-log10(p-value)\")\n",
    "\n",
    "plt.xlabel('Topics')\n",
    "plt.title('FedWeight ETM Significance of Disease-Topic Associations (eICU on MIMIC)')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.568174Z"
    }
   },
   "id": "407eccd93ed9003b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, average_precision_score, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "mimic_client_model = client_models[2001]\n",
    "\n",
    "mimic_x_train_tensor = train_icds[2001]\n",
    "eicu_x_test_tensor = test_icds[1001]\n",
    "\n",
    "_, mimic_train_theta_weighted, _ = mimic_client_model.get_theta(mimic_x_train_tensor)\n",
    "_, eicu_test_theta_weighted, _ = mimic_client_model.get_theta(eicu_x_test_tensor)\n",
    "\n",
    "mimic_train_theta_weighted = mimic_train_theta_weighted.data.cpu().numpy()\n",
    "eicu_test_theta_weighted = eicu_test_theta_weighted.data.cpu().numpy()\n",
    "\n",
    "mimic_train_readmit_row_ids = train_readmit_row_ids[2001]\n",
    "eicu_test_readmit_row_ids = test_readmit_row_ids[1001]\n",
    "\n",
    "X_mimic_train = mimic_train_theta_weighted[mimic_train_readmit_row_ids]\n",
    "X_eicu_test = eicu_test_theta_weighted[eicu_test_readmit_row_ids]\n",
    "\n",
    "y_mimic_death_train = train_label_deaths[2001]\n",
    "y_eicu_death_test = test_label_deaths[1001]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=120)\n",
    "knn.fit(X_mimic_train, y_mimic_death_train)\n",
    "\n",
    "y_eicu_death_cross_knn_scores = knn.predict_proba(X_eicu_test)[:, 1]\n",
    "\n",
    "precision_eicu_death_cross_knn, recall_eicu_death_cross_knn, _ = precision_recall_curve(y_eicu_death_test, y_eicu_death_cross_knn_scores)\n",
    "auprc_eicu_death_cross_knn = average_precision_score(y_eicu_death_test, y_eicu_death_cross_knn_scores)\n",
    "top_k_precision_death_cross_knn = top_k_precision(y_eicu_death_test, y_eicu_death_cross_knn_scores, k=100)\n",
    "\n",
    "fpr_eicu_death_cross_knn, tpr_eicu_death_cross_knn, _ = roc_curve(y_eicu_death_test, y_eicu_death_cross_knn_scores)\n",
    "auroc_eicu_death_cross_knn = roc_auc_score(y_eicu_death_test, y_eicu_death_cross_knn_scores)\n",
    "\n",
    "print(f\"AUPRC of Mortality Prediction after Re-admission (MIMIC on eICU): {auprc_eicu_death_cross_knn:.4f}\")\n",
    "print(f\"AUROC of Mortality Prediction after Re-admission (MIMIC on eICU): {auroc_eicu_death_cross_knn:.4f}\")\n",
    "print(f\"Top k Precision of Mortality Prediction after Re-admission (MIMIC on eICU): {top_k_precision_death_cross_knn:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.568278Z"
    }
   },
   "id": "c8bafc4ea73f14d9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, average_precision_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "eicu_client_model = client_models[1001]\n",
    "\n",
    "eicu_x_train_tensor = train_icds[1001]\n",
    "mimic_x_test_tensor = test_icds[2001]\n",
    "\n",
    "_, eicu_train_theta_weighted, _ = eicu_client_model.get_theta(eicu_x_train_tensor)\n",
    "_, mimic_test_theta_weighted, _ = eicu_client_model.get_theta(mimic_x_test_tensor)\n",
    "\n",
    "eicu_train_theta_weighted = eicu_train_theta_weighted.data.cpu().numpy()\n",
    "mimic_test_theta_weighted = mimic_test_theta_weighted.data.cpu().numpy()\n",
    "\n",
    "eicu_train_readmit_row_ids = train_readmit_row_ids[1001]\n",
    "mimic_test_readmit_row_ids = test_readmit_row_ids[2001]\n",
    "\n",
    "X_eicu_train = eicu_train_theta_weighted[eicu_train_readmit_row_ids]\n",
    "X_mimic_test = mimic_test_theta_weighted[mimic_test_readmit_row_ids]\n",
    "\n",
    "y_eicu_death_train = train_label_deaths[1001]\n",
    "y_mimic_death_test = test_label_deaths[2001]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=120)\n",
    "knn.fit(X_eicu_train, y_eicu_death_train)\n",
    "\n",
    "y_mimic_death_cross_knn_scores = knn.predict_proba(X_mimic_test)[:, 1]\n",
    "\n",
    "precision_mimic_death_cross_knn, recall_mimic_death_cross_knn, _ = precision_recall_curve(y_mimic_death_test, y_mimic_death_cross_knn_scores)\n",
    "auprc_mimic_death_cross_knn = average_precision_score(y_mimic_death_test, y_mimic_death_cross_knn_scores)\n",
    "top_k_precision_mimic_cross_knn = top_k_precision(y_mimic_death_test, y_mimic_death_cross_knn_scores, k=100)\n",
    "\n",
    "fpr_mimic_death_cross_knn, tpr_mimic_death_cross_knn, _ = roc_curve(y_mimic_death_test, y_mimic_death_cross_knn_scores)\n",
    "auroc_mimic_death_cross_knn = roc_auc_score(y_mimic_death_test, y_mimic_death_cross_knn_scores)\n",
    "\n",
    "print(f\"AUPRC of Mortality Prediction after Re-admission (eICU on MIMIC): {auprc_mimic_death_cross_knn:.4f}\")\n",
    "print(f\"AUROC of Mortality Prediction after Re-admission (eICU on MIMIC): {auroc_mimic_death_cross_knn:.4f}\")\n",
    "print(f\"Top k Precision of Mortality Prediction after Re-admission (eICU on MIMIC): {top_k_precision_mimic_cross_knn:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.569130Z"
    }
   },
   "id": "5c68809642d02b2a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"weighted_recall_eicu_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for r in recall_eicu_death_cross_knn:\n",
    "        f.write(f\"{r}\\n\")\n",
    "        \n",
    "with open(\"weighted_precision_eicu_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for p in precision_eicu_death_cross_knn:\n",
    "        f.write(f\"{p}\\n\")\n",
    "    \n",
    "with open(\"weighted_auprc_eicu_death_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{auprc_eicu_death_cross_knn}\\n\")\n",
    "\n",
    "with open(\"weighted_top_k_precision_eicu_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{top_k_precision_death_cross_knn}\\n\")\n",
    "    \n",
    "with open(\"weighted_fpr_eicu_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for fp in fpr_eicu_death_cross_knn:\n",
    "        f.write(f\"{fp}\\n\")\n",
    "        \n",
    "with open(\"weighted_tpr_eicu_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for tp in tpr_eicu_death_cross_knn:\n",
    "        f.write(f\"{tp}\\n\")\n",
    "        \n",
    "with open(\"weighted_auroc_eicu_death_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{auroc_eicu_death_cross_knn}\\n\")\n",
    "    \n",
    "with open(\"weighted_eicu_disease_p_values.txt\", 'w') as file:\n",
    "    for key, value in eicu_disease_p_values.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "        \n",
    "with open(\"weighted_eicu_disease_topic_p_values.txt\", 'w') as file:\n",
    "    for row in eicu_disease_topic_p_values:\n",
    "        file.write(\" \".join(f\"{value:.5f}\" for value in row) + \"\\n\")\n",
    "        \n",
    "with open(\"weighted_recall_mimic_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for r in recall_mimic_death_cross_knn:\n",
    "        f.write(f\"{r}\\n\")\n",
    "        \n",
    "with open(\"weighted_precision_mimic_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for p in precision_mimic_death_cross_knn:\n",
    "        f.write(f\"{p}\\n\")\n",
    "    \n",
    "with open(\"weighted_auprc_mimic_death_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{auprc_mimic_death_cross_knn}\\n\")\n",
    "    \n",
    "with open(\"weighted_top_k_precision_mimic_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{top_k_precision_mimic_cross_knn}\\n\")\n",
    "    \n",
    "with open(\"weighted_mimic_disease_p_values.txt\", 'w') as file:\n",
    "    for key, value in mimic_disease_p_values.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "        \n",
    "with open(\"weighted_fpr_mimic_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for fp in fpr_mimic_death_cross_knn:\n",
    "        f.write(f\"{fp}\\n\")\n",
    "        \n",
    "with open(\"weighted_tpr_mimic_death_cross_knn_values.txt\", \"w\") as f:\n",
    "    for tp in tpr_mimic_death_cross_knn:\n",
    "        f.write(f\"{tp}\\n\")\n",
    "        \n",
    "with open(\"weighted_auroc_mimic_death_cross_knn.txt\", \"w\") as f:\n",
    "    f.write(f\"{auroc_mimic_death_cross_knn}\\n\")\n",
    "\n",
    "with open(\"weighted_mimic_disease_topic_p_values.txt\", 'w') as file:\n",
    "    for row in mimic_disease_topic_p_values:\n",
    "        file.write(\" \".join(f\"{value:.5f}\" for value in row) + \"\\n\")\n",
    "        \n",
    "with open(\"weighted_unique_disease_names.txt\", 'w') as file:\n",
    "    for disease in unique_diseases:\n",
    "        file.write(f'{disease}\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.570080Z"
    }
   },
   "id": "23f45facfe1f273e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    " \n",
    "torch.save(client_models[2001].state_dict(), 'weighted_mimic_client_model.pth')\n",
    "torch.save(client_models[1001].state_dict(), 'weighted_eicu_client_model.pth')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "mimic_train_icds = train_icds[2001].cpu().numpy()\n",
    "np.save(\"weighted_mimic_train_icds.npy\", mimic_train_icds)\n",
    "\n",
    "mimic_test_icds = test_icds[2001].cpu().numpy()\n",
    "np.save(\"weighted_mimic_test_icds.npy\", mimic_test_icds)\n",
    "\n",
    "mimic_train_readmit_row_ids = train_readmit_row_ids[2001]\n",
    "np.save(\"weighted_mimic_train_readmit_row_ids.npy\", mimic_train_readmit_row_ids)\n",
    "\n",
    "mimic_test_readmit_row_ids = test_readmit_row_ids[2001]\n",
    "np.save(\"weighted_mimic_test_readmit_row_ids.npy\", mimic_test_readmit_row_ids)\n",
    "\n",
    "mimic_train_label_deaths = train_label_deaths[2001]\n",
    "np.save(\"weighted_mimic_train_label_deaths.npy\", mimic_train_label_deaths)\n",
    "\n",
    "mimic_test_label_deaths = test_label_deaths[2001]\n",
    "np.save(\"weighted_mimic_test_label_deaths.npy\", mimic_test_label_deaths)\n",
    "\n",
    "eicu_train_icds = train_icds[1001].cpu().numpy()\n",
    "np.save(\"weighted_eicu_train_icds.npy\", eicu_train_icds)\n",
    "\n",
    "eicu_test_icds = test_icds[1001].cpu().numpy()\n",
    "np.save(\"weighted_eicu_test_icds.npy\", eicu_test_icds)\n",
    "\n",
    "eicu_train_readmit_row_ids = train_readmit_row_ids[1001]\n",
    "np.save(\"weighted_eicu_train_readmit_row_ids.npy\", eicu_train_readmit_row_ids)\n",
    "\n",
    "eicu_test_readmit_row_ids = test_readmit_row_ids[1001]\n",
    "np.save(\"weighted_eicu_test_readmit_row_ids.npy\", eicu_test_readmit_row_ids)\n",
    "\n",
    "eicu_train_label_deaths = train_label_deaths[1001]\n",
    "np.save(\"weighted_eicu_train_label_deaths.npy\", eicu_train_label_deaths)\n",
    "\n",
    "eicu_test_label_deaths = test_label_deaths[1001]\n",
    "np.save(\"weighted_eicu_test_label_deaths.npy\", eicu_test_label_deaths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.571115Z"
    }
   },
   "id": "5cc934982841c033",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# with open(\"unweighted_recall_eicu_death_cross_knn_values.txt\", \"r\") as f:\n",
    "#     unweighted_recall_eicu_death_cross_knn_txt = [float(line.strip()) for line in f]\n",
    "# \n",
    "# unweighted_recall_eicu_death_cross_knn = np.array(unweighted_recall_eicu_death_cross_knn_txt)\n",
    "# \n",
    "# with open(\"unweighted_precision_eicu_death_cross_knn_values.txt\", \"r\") as f:\n",
    "#     unweighted_precision_eicu_death_cross_knn_txt = [float(line.strip()) for line in f]\n",
    "#     \n",
    "# unweighted_precision_eicu_death_cross_knn = np.array(unweighted_precision_eicu_death_cross_knn_txt)\n",
    "#     \n",
    "# with open(\"unweighted_auprc_eicu_death_cross_knn.txt\", \"r\") as f:\n",
    "#     unweighted_auprc_eicu_death_cross_knn = float(f.readline())\n",
    "# \n",
    "# with open(\"unweighted_top_k_precision_eicu_cross_lr.txt\", \"r\") as f:\n",
    "#     unweighted_top_k_precision_eicu_cross_knn = float(f.readline())\n",
    "#     \n",
    "# unweighted_eicu_disease_p_values = {}\n",
    "# with open(\"unweighted_eicu_disease_p_values.txt\", 'r') as file:\n",
    "#     for line in file:\n",
    "#         key, value = line.strip().split(': ')\n",
    "#         unweighted_eicu_disease_p_values[key] = float(value)\n",
    "#         \n",
    "# with open(\"unweighted_recall_mimic_death_cross_knn_values.txt\", \"r\") as f:\n",
    "#     unweighted_recall_mimic_death_cross_knn_txt = [float(line.strip()) for line in f]\n",
    "# \n",
    "# unweighted_recall_mimic_death_cross_knn = np.array(unweighted_recall_mimic_death_cross_knn_txt)\n",
    "# \n",
    "# with open(\"unweighted_precision_mimic_death_cross_knn_values.txt\", \"r\") as f:\n",
    "#     unweighted_precision_mimic_death_cross_knn_txt = [float(line.strip()) for line in f]\n",
    "# \n",
    "# unweighted_precision_mimic_death_cross_knn = np.array(unweighted_precision_mimic_death_cross_knn_txt)\n",
    "#     \n",
    "# with open(\"unweighted_auprc_mimic_death_cross_knn.txt\", \"r\") as f:\n",
    "#     unweighted_auprc_mimic_death_cross_knn = float(f.readline())\n",
    "#     \n",
    "# with open(\"unweighted_top_k_precision_mimic_cross_knn.txt\", \"r\") as f:\n",
    "#     unweighted_top_k_precision_mimic_cross_knn = float(f.readline())\n",
    "#     \n",
    "# unweighted_mimic_disease_p_values = {}\n",
    "# with open(\"unweighted_mimic_disease_p_values.txt\", 'r') as file:\n",
    "#     for line in file:\n",
    "#         key, value = line.strip().split(': ')\n",
    "#         unweighted_mimic_disease_p_values[key] = float(value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.572725Z"
    }
   },
   "id": "c5dfa45c5ec44506",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 5))\n",
    "# \n",
    "# plt.plot(unweighted_fpr_eicu_death_cross_lr, unweighted_tpr_eicu_death_cross_lr, label=f'FedAvg = {unweighted_auroc_eicu_death_cross_lr:.4f}')\n",
    "# plt.plot(fpr_eicu_death_cross_lr, tpr_eicu_death_cross_lr, label=f'FedWeight = {auroc_eicu_death_cross_lr:.4f}')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve of Mortality Prediction after Re-admission using LR (MIMIC on eICU)')\n",
    "# plt.legend()\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.573998Z"
    }
   },
   "id": "88063533a2f9c817",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 5))\n",
    "# \n",
    "# plt.plot(unweighted_recall_eicu_death_cross_lr, unweighted_precision_eicu_death_cross_lr, label=f'FedAvg AUPRC = {unweighted_auprc_eicu_death_cross_lr:.4f} \\n Top 100 Precision = {unweighted_top_k_precision_eicu_cross_lr:.4f}')\n",
    "# plt.plot(recall_eicu_death_cross_lr, precision_eicu_death_cross_lr, label=f'FedWeight AUPRC = {auprc_eicu_death_cross_lr:.4f} \\n Top 100 Precision = {top_k_precision_death_cross_lr:.4f}')\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision-Recall Curve of Mortality Prediction after Re-admission using LR (MIMIC on eICU)')\n",
    "# plt.legend()\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.575212Z"
    }
   },
   "id": "254f6925a9454a79",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 5))\n",
    "# \n",
    "# plt.plot(unweighted_fpr_eicu_death_cross_knn, unweighted_tpr_eicu_death_cross_knn, label=f'FedAvg = {unweighted_auroc_eicu_death_cross_knn:.4f}')\n",
    "# plt.plot(fpr_eicu_death_cross_knn, tpr_eicu_death_cross_knn, label=f'FedWeight = {auroc_eicu_death_cross_knn:.4f}')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve of Mortality Prediction after Re-admission using KNN (MIMIC on eICU)')\n",
    "# plt.legend()\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.576307Z"
    }
   },
   "id": "2c17ee711db675f6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 5))\n",
    "# \n",
    "# plt.plot(unweighted_fpr_mimic_death_cross_lr, unweighted_tpr_mimic_death_cross_lr, label=f'FedAvg = {unweighted_auroc_mimic_death_cross_lr:.4f}')\n",
    "# plt.plot(fpr_mimic_death_cross_lr, tpr_mimic_death_cross_lr, label=f'FedWeight = {auroc_mimic_death_cross_lr:.4f}')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve of Mortality Prediction after Re-admission using LR (eICU on MIMIC)')\n",
    "# plt.legend()\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.577596Z"
    }
   },
   "id": "e852df8b22ba7c2d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 5))\n",
    "# \n",
    "# plt.plot(unweighted_fpr_mimic_death_cross_knn, unweighted_tpr_mimic_death_cross_knn, label=f'FedAvg = {unweighted_auroc_mimic_death_cross_knn:.4f}')\n",
    "# plt.plot(fpr_mimic_death_cross_knn, tpr_mimic_death_cross_knn, label=f'FedWeight = {auroc_mimic_death_cross_knn:.4f}')\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve of Mortality Prediction after Re-admission using KNN (eICU on MIMIC)')\n",
    "# plt.legend()\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.578958Z"
    }
   },
   "id": "a47a334724c9a5f5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(6, 5))\n",
    "# \n",
    "# plt.plot(unweighted_recall_mimic_death_cross_lr, unweighted_precision_mimic_death_cross_lr, label=f'FedAvg AUPRC = {unweighted_auprc_mimic_death_cross_lr:.4f} \\n Top 100 Precision = {unweighted_top_k_precision_mimic_cross_lr:.4f}')\n",
    "# plt.plot(recall_mimic_death_cross_lr, precision_mimic_death_cross_lr, label=f'FedWeight AUPRC = {auprc_mimic_death_cross_lr:.4f} \\n Top 100 Precision = {top_k_precision_mimic_cross_lr:.4f}')\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.title('Precision-Recall Curve of Mortality Prediction after Re-admission using LR (eICU on MIMIC)')\n",
    "# plt.legend()\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-29T20:49:45.579915Z"
    }
   },
   "id": "1af031ca7fa2b497",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
