{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63642, 1416)\n",
      "Total hospitals:  164\n",
      "Total drugs:  1399\n",
      "Total features:  1408\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>patientunitstayid</th>\n",
       "      <th>hospitalid</th>\n",
       "      <th>Death</th>\n",
       "      <th>unitdischargeoffset</th>\n",
       "      <th>ventilation</th>\n",
       "      <th>sepsis</th>\n",
       "      <th>cardiovascular</th>\n",
       "      <th>2 ML  -  METOCLOPRAMIDE HCL 5 MG/ML IJ SOLN</th>\n",
       "      <th>3 ML VIAL : INSULIN REGULAR HUMAN 100 UNIT/ML IJ SOLN</th>\n",
       "      <th>...</th>\n",
       "      <th>SODIUM CHLORIDE BACTERIOSTATIC 0.9 % INJ SOLN</th>\n",
       "      <th>Gender</th>\n",
       "      <th>&lt; 30</th>\n",
       "      <th>30 - 39</th>\n",
       "      <th>40 - 49</th>\n",
       "      <th>50 - 59</th>\n",
       "      <th>60 - 69</th>\n",
       "      <th>70 - 79</th>\n",
       "      <th>80 - 89</th>\n",
       "      <th>&gt; 89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>141168.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3596.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>141194.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4813.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>141233.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15685.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>141244.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3835.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>141265.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6068.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1416 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  patientunitstayid  hospitalid  Death  unitdischargeoffset  \\\n",
       "0           0           141168.0        59.0    1.0               3596.0   \n",
       "1           1           141194.0        73.0    0.0               4813.0   \n",
       "2           2           141233.0        73.0    0.0              15685.0   \n",
       "3           3           141244.0        73.0    0.0               3835.0   \n",
       "4           4           141265.0        63.0    0.0               6068.0   \n",
       "\n",
       "   ventilation  sepsis  cardiovascular  \\\n",
       "0          0.0     0.0             0.0   \n",
       "1          0.0     0.0             0.0   \n",
       "2          0.0     0.0             0.0   \n",
       "3          0.0     0.0             0.0   \n",
       "4          0.0     0.0             0.0   \n",
       "\n",
       "   2 ML  -  METOCLOPRAMIDE HCL 5 MG/ML IJ SOLN  \\\n",
       "0                                          0.0   \n",
       "1                                          1.0   \n",
       "2                                          0.0   \n",
       "3                                          1.0   \n",
       "4                                          0.0   \n",
       "\n",
       "   3 ML VIAL : INSULIN REGULAR HUMAN 100 UNIT/ML IJ SOLN  ...  \\\n",
       "0                                                0.0      ...   \n",
       "1                                                1.0      ...   \n",
       "2                                                0.0      ...   \n",
       "3                                                1.0      ...   \n",
       "4                                                0.0      ...   \n",
       "\n",
       "   SODIUM CHLORIDE BACTERIOSTATIC 0.9 % INJ SOLN  Gender  < 30  30 - 39  \\\n",
       "0                                            0.0     0.0   0.0      0.0   \n",
       "1                                            0.0     1.0   0.0      0.0   \n",
       "2                                            0.0     0.0   0.0      0.0   \n",
       "3                                            0.0     1.0   0.0      0.0   \n",
       "4                                            0.0     1.0   0.0      0.0   \n",
       "\n",
       "   40 - 49  50 - 59  60 - 69  70 - 79  80 - 89  > 89  \n",
       "0      0.0      0.0      0.0      1.0      0.0   0.0  \n",
       "1      0.0      0.0      1.0      0.0      0.0   0.0  \n",
       "2      0.0      0.0      0.0      0.0      1.0   0.0  \n",
       "3      0.0      1.0      0.0      0.0      0.0   0.0  \n",
       "4      0.0      0.0      1.0      0.0      0.0   0.0  \n",
       "\n",
       "[5 rows x 1416 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# eicu = pd.read_csv(\"../data/one_hot_age_gender_region_eicu_data.csv\")\n",
    "eicu = pd.read_csv(\"../data/one_hot_age_eicu_data_2.csv\")\n",
    "print(eicu.shape)\n",
    "print(\"Total hospitals: \", len(eicu.hospitalid.unique()))\n",
    "print(\"Total drugs: \", len(eicu.columns[8:-9]))\n",
    "print(\"Total features: \", len(eicu.columns[8:]))\n",
    "eicu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "task = \"death\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63642, 1408])\n",
      "torch.Size([44549, 1408])\n",
      "torch.Size([19093, 1408])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = eicu.iloc[:, 8:]\n",
    "x = np.asarray(x)\n",
    "x = x.astype(np.float32)\n",
    "x = torch.Tensor(x).to(device)\n",
    "print(x.shape)\n",
    "\n",
    "x_train, x_test = train_test_split(x, test_size=0.3)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EvaluationUtils:\n",
    "    \"\"\" Utility class to evaluate models \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_bce(pred, y, reduction='mean'):\n",
    "        criterion = nn.BCELoss(reduction=reduction)\n",
    "        return criterion(pred, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_ce(pred, y, reduction='mean'):\n",
    "        criterion = nn.CrossEntropyLoss(reduction=reduction)\n",
    "        return criterion(pred, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_mse(pred, y, reduction='mean'):\n",
    "        criterion = nn.MSELoss(reduction=reduction)\n",
    "        return criterion(pred, y)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_accuracy(pred, y):\n",
    "        pred = (pred > 0.5).float()\n",
    "        return ((pred - y).abs() < 1e-2).float().mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_roc_auc(pred, y):\n",
    "        y = y.detach().cpu().numpy()\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        return roc_auc_score(y, pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_pr_auc(pred, y):\n",
    "        y = y.detach().cpu().numpy()\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        precision, recall, _ = precision_recall_curve(y, pred)\n",
    "        return auc(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "DATASET_DIR_CONFIG_KEY = \"dataset_dir_path\"\n",
    "DATASET_DIR_DEFAULT = \"../data/\"\n",
    "EICU_PATH = \"one_hot_age_gender_eicu_data.csv\"\n",
    "\n",
    "# FL\n",
    "LR = 0.0001\n",
    "WEIGHT_DECAY = 0.001\n",
    "MIN_HOSPITAL_DEATH_COUNT = 150\n",
    "TOTAL_FL_ROUND = 1501\n",
    "LOCAL_FL_EPOCHS = 1\n",
    "TEST_SIZE = 0.3\n",
    "VAL_SIZE = 0.2\n",
    "TOTAL_FEATURE = 1420\n",
    "FL_HIDDEN_LAYER_UNITS = \"256,128,64\"\n",
    "BIAS_INIT_PRIOR_PROB = None\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# MADE\n",
    "MADE_EPOCHS = 50\n",
    "MADE_HIDDEN_LAYER_UNITS = \"500\"\n",
    "MADE_NUM_MASKS = 1\n",
    "MADE_SAMPLES = 1\n",
    "MADE_RESAMPLE_EVERY = 20\n",
    "\n",
    "# Weight\n",
    "REWEIGHT_LAMBDA = 1.0\n",
    "\n",
    "# Task\n",
    "VENTILATOR = \"ventilator\"  # Ventilator usage in hospitals\n",
    "SEPSIS = \"sepsis\"  # Sepsis diagnosis in hospitals\n",
    "DEATH = \"death\"  # Death prediction in hospitals\n",
    "LENGTH = \"length\"  # Length of stay in hospitals\n",
    "CARDIOVASCULAR = \"cardiovascular\" # Cardiovascular\n",
    "REGION_VENTILATOR = \"region_ventilator\"  # Ventilator usage in regions\n",
    "REGION_SEPSIS = \"region_sepsis\"  # Sepsis diagnosis in regions\n",
    "REGION_DEATH = \"region_death\"  # Death prediction in regions\n",
    "REGION_LENGTH = \"region_length\"  # Length of stay in regions\n",
    "REGION_TASK_PREFIX = \"region\"\n",
    "SIMULATION = \"simulation\"  # Simulation dataset\n",
    "SIMULATION_BY_DIR = \"simulation_by_dir\"  # Simulation dataset load by directory\n",
    "COLOR_MNIST = \"color_mnist\"  # Color MNIST\n",
    "BINARIZED_MNIST = \"binarized_mnist\"  # Binarized MNIST\n",
    "\n",
    "# Algorithm\n",
    "WEIGHTED = \"weighted\"  # Weighted\n",
    "UNWEIGHTED = \"unweighted\"  # Unweighted\n",
    "BOTH = \"both\"  # Both, used by paired\n",
    "\n",
    "# Stage\n",
    "GRID_SEARCH = \"grid_search\"\n",
    "RETRAIN = \"retrain\"\n",
    "\n",
    "# Label index\n",
    "LABEL_IDX = {\n",
    "    VENTILATOR: 5,\n",
    "    REGION_VENTILATOR: 5,\n",
    "    SEPSIS: 6,\n",
    "    REGION_SEPSIS: 6,\n",
    "    CARDIOVASCULAR: 7,\n",
    "    DEATH: 3,\n",
    "    REGION_DEATH: 3,\n",
    "    LENGTH: 4,\n",
    "    REGION_LENGTH: 4,\n",
    "    COLOR_MNIST: 0\n",
    "}\n",
    "\n",
    "# Log\n",
    "LOG_PATH_CONFIG_KEY = \"log_dir_path\"\n",
    "LOG_PATH_DEFAULT = \"../log/\"\n",
    "LOGGER_DEFAULT = \"logger_default\"\n",
    "LOGGER_MADE = \"logger_made\"\n",
    "\n",
    "# Output\n",
    "OUTPUT_PATH_CONFIG_KEY = \"output_dir_path\"\n",
    "OUTPUT_PATH_DEFAULT = \"../output/\"\n",
    "\n",
    "# Simulate\n",
    "SIMULATE_SOURCE_HOSPITAL_ID = 420\n",
    "SIMULATE_TARGET_HOSPITAL_ID = 449\n",
    "\n",
    "SIMULATE_X_SOURCE_PATH = \"simulate_x_source.csv\"\n",
    "SIMULATE_Y_SOURCE_PATH = \"simulate_y_source.csv\"\n",
    "SIMULATE_X_TARGET_PATH = \"simulate_x_target.csv\"\n",
    "SIMULATE_Y_TARGET_PATH = \"simulate_y_target.csv\"\n",
    "\n",
    "\n",
    "SIMULATE_DATA_DIR = \"simulation/\"\n",
    "\n",
    "TARGET_HOSPITAL_ID = \"Northeast\"\n",
    "TOTAL_SEED = 10\n",
    "\n",
    "FED_WEIGHT_METHOD_SGD = \"fed_weight_method_sgd\"\n",
    "FED_WEIGHT_METHOD_AVG = \"fed_weight_method_avg\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features: int,\n",
    "                 hidden_sizes: str):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        hidden_list = list(map(int, hidden_sizes.split(',')))\n",
    "\n",
    "        # Encoder layers\n",
    "        self._encoder = []\n",
    "        encoder_hiddens = [in_features] + hidden_list\n",
    "        for h0, h1 in zip(encoder_hiddens, encoder_hiddens[1:]):\n",
    "            self._encoder.extend([\n",
    "                nn.Linear(h0, h1),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self._encoder = nn.Sequential(*self._encoder)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self._decoder = []\n",
    "        decoder_hiddens = encoder_hiddens[::-1]\n",
    "        for h0, h1 in zip(decoder_hiddens, decoder_hiddens[1:]):\n",
    "            self._decoder.extend([\n",
    "                nn.Linear(h0, h1),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self._decoder.pop()  # pop the last ReLU for the output layer\n",
    "        self._decoder = nn.Sequential(*self._decoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self._encoder(x)\n",
    "        decoded = self._decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "class AutoEncoderService:\n",
    "\n",
    "    def __init__(self, task: str) -> None:\n",
    "        self._task = task\n",
    "\n",
    "    def run_auto_encoder(self,\n",
    "                         model, opt,\n",
    "                         x_train, x_test,\n",
    "                         split, batch_size, device):\n",
    "\n",
    "        torch.set_grad_enabled(split == 'train')\n",
    "        model.train() if split == 'train' else model.eval()\n",
    "        x = x_train if split == 'train' else x_test\n",
    "\n",
    "        if batch_size <= 0 or batch_size > len(x):\n",
    "            raise ValueError(\n",
    "                \"Batch size must be larger than 0 and smaller than sample size\")\n",
    "\n",
    "        N, D = x.size()\n",
    "        B = 64  # batch size\n",
    "        nsteps = math.ceil(N/B)\n",
    "\n",
    "        loss_for_samples = torch.full((N,), torch.nan).to(device)  # N x 1\n",
    "        loss_total = []\n",
    "\n",
    "        total_samples = 0\n",
    "        for step in range(nsteps):\n",
    "            # fetch the next batch of data\n",
    "            xb = Variable(x[step * B: step * B + B])\n",
    "            self._run_batch(step, B, N, model, opt, xb, split,\n",
    "                            loss_total, loss_for_samples)\n",
    "            total_samples += B\n",
    "\n",
    "        if total_samples < N:\n",
    "            # fetch the remaining data\n",
    "            xb = Variable(x[total_samples:])\n",
    "            self._run_batch(step + 1, B, N, model, opt, xb, split,\n",
    "                            loss_total, loss_for_samples)\n",
    "\n",
    "        assert not torch.isnan(loss_for_samples).any()\n",
    "        return sum(loss_total) / len(loss_total), loss_for_samples\n",
    "    \n",
    "    def _run_batch(self, step, B, N,\n",
    "                   model, opt,\n",
    "                   xb, split,\n",
    "                   loss_total, \n",
    "                   loss_for_samples):\n",
    "\n",
    "        pred = model(xb)\n",
    "\n",
    "        if self._task == COLOR_MNIST:\n",
    "            # Gaussian\n",
    "            loss = EvaluationUtils.mean_mse(pred, xb, reduction='mean')  # batch_size x D\n",
    "\n",
    "        else:\n",
    "            # Multinomial\n",
    "            loss = EvaluationUtils.mean_ce(pred, xb, reduction='mean')  # batch_size x 1\n",
    "\n",
    "        loss_total.append(loss.item())\n",
    "        # probs_sample = torch.exp(-1 * loss_sample)\n",
    "        # if step * B + B > N:\n",
    "        #     loss_for_samples[step * B:] = loss_sample\n",
    "        # else:\n",
    "        #     loss_for_samples[step * B: step * B + B] = loss_sample\n",
    "\n",
    "        # backward/update\n",
    "        if split == 'train':\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_epochs = 150\n",
    "ae_hiddens = \"1024,512,256\"\n",
    "ae_learning_rate = 0.0001\n",
    "ae_weight_decay = 0.001\n",
    "ae_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m ae_train_hist \u001b[39m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ae_epochs):\n\u001b[0;32m---> 14\u001b[0m     train_loss, _ \u001b[39m=\u001b[39m ae_service\u001b[39m.\u001b[39;49mrun_auto_encoder(ae, ae_opt,\n\u001b[1;32m     15\u001b[0m                          x_train, \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     16\u001b[0m                          \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, ae_batch_size,\n\u001b[1;32m     17\u001b[0m                          device)\n\u001b[1;32m     19\u001b[0m     ae_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m     ae_train_hist\u001b[39m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[43], line 34\u001b[0m, in \u001b[0;36mAutoEncoderService.run_auto_encoder\u001b[0;34m(self, model, opt, x_train, x_test, split, batch_size, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nsteps):\n\u001b[1;32m     32\u001b[0m     \u001b[39m# fetch the next batch of data\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     xb \u001b[39m=\u001b[39m Variable(x[step \u001b[39m*\u001b[39m B: step \u001b[39m*\u001b[39m B \u001b[39m+\u001b[39m B])\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_batch(step, B, N, model, opt, xb, split,\n\u001b[1;32m     35\u001b[0m                     loss_total, loss_for_samples)\n\u001b[1;32m     36\u001b[0m     total_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m B\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m total_samples \u001b[39m<\u001b[39m N:\n\u001b[1;32m     39\u001b[0m     \u001b[39m# fetch the remaining data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[43], line 72\u001b[0m, in \u001b[0;36mAutoEncoderService._run_batch\u001b[0;34m(self, step, B, N, model, opt, xb, split, loss_total, loss_for_samples)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m# probs_sample = torch.exp(-1 * loss_sample)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# if step * B + B > N:\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m#     loss_for_samples[step * B:] = loss_sample\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \n\u001b[1;32m     70\u001b[0m \u001b[39m# backward/update\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m     opt\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m     73\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     74\u001b[0m     opt\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/Project/FedWeight/FedWeight_eICU/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:249\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    247\u001b[0m     p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m foreach \u001b[39mor\u001b[39;00m p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mis_sparse):\n\u001b[0;32m--> 249\u001b[0m     p\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mzero_()\n\u001b[1;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     per_device_and_dtype_grads[p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdevice][p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdtype]\u001b[39m.\u001b[39mappend(p\u001b[39m.\u001b[39mgrad)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "ae = Autoencoder(x.size(1), ae_hiddens)\n",
    "ae_service = AutoEncoderService(task)\n",
    "ae_opt = torch.optim.Adam(ae.parameters(),\n",
    "                            lr=ae_learning_rate,\n",
    "                            weight_decay=ae_weight_decay)\n",
    "ae_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    ae_opt, step_size=45, gamma=0.1)\n",
    "\n",
    "ae_train_hist = []\n",
    "\n",
    "for epoch in range(ae_epochs):\n",
    "    train_loss, _ = ae_service.run_auto_encoder(ae, ae_opt,\n",
    "                         x_train, None,\n",
    "                         'train', ae_batch_size,\n",
    "                         device)\n",
    "    \n",
    "    ae_scheduler.step()\n",
    "    ae_train_hist.append(train_loss)\n",
    "\n",
    "    print(\"epoch: {}, train loss: {}\".format(epoch, train_loss))\n",
    "\n",
    "test_loss, _ = ae_service.run_auto_encoder(ae, ae_opt,\n",
    "                                           None, x_test,\n",
    "                                           'test', ae_batch_size,\n",
    "                                           device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial MADE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "made_hiddens = \"1408\"\n",
    "num_masks = 1\n",
    "natural_ordering = False\n",
    "made_learning_rate = 0.0001\n",
    "made_weight_decay = 0.001\n",
    "made_epochs = 50\n",
    "made_batch_size = 64\n",
    "made_samples = 10\n",
    "resample_every = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MaskedLinear(nn.Linear):\n",
    "    \"\"\" same as Linear except has a configurable mask on the weights \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
    "\n",
    "    def set_mask(self, mask):\n",
    "        self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.mask * self.weight, self.bias)\n",
    "\n",
    "\n",
    "class MADE(nn.Module):\n",
    "\n",
    "    def __init__(self, nin, hidden_sizes, nout, num_masks=1, natural_ordering=False):\n",
    "        \"\"\"\n",
    "        nin: integer; number of inputs\n",
    "        hidden sizes: a list of integers; number of units in hidden layers\n",
    "        nout: integer; number of outputs, which usually collectively parameterize some kind of 1D distribution\n",
    "              note: if nout is e.g. 2x larger than nin (perhaps the mean and std), then the first nin\n",
    "              will be all the means and the second nin will be stds. i.e. output dimensions depend on the\n",
    "              same input dimensions in \"chunks\" and should be carefully decoded downstream appropriately.\n",
    "              the output of running the tests for this file makes this a bit more clear with examples.\n",
    "        num_masks: can be used to train ensemble over orderings/connections\n",
    "        natural_ordering: force natural ordering of dimensions, don't use random permutations\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        assert self.nout % self.nin == 0, \"nout must be integer multiple of nin\"\n",
    "\n",
    "        # define a simple MLP neural net\n",
    "        self.net = []\n",
    "        hs = [nin] + hidden_sizes + [nout]\n",
    "        for h0, h1 in zip(hs, hs[1:]):\n",
    "            self.net.extend([\n",
    "                MaskedLinear(h0, h1),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self.net.pop()  # pop the last ReLU for the output layer\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "        # seeds for orders/connectivities of the model ensemble\n",
    "        self.natural_ordering = natural_ordering\n",
    "        self.num_masks = num_masks\n",
    "        self.seed = 0  # for cycling through num_masks orderings\n",
    "\n",
    "        self.m = {}\n",
    "        self.update_masks()  # builds the initial self.m connectivity\n",
    "        # note, we could also precompute the masks and cache them, but this\n",
    "        # could get memory expensive for large number of masks.\n",
    "\n",
    "    def update_masks(self):\n",
    "        \n",
    "        L = len(self.hidden_sizes)\n",
    "\n",
    "        # fetch the next seed and construct a random stream\n",
    "        rng = np.random.RandomState(self.seed)\n",
    "        self.seed = (self.seed + 1) % self.num_masks\n",
    "\n",
    "        # sample the order of the inputs and the connectivity of all neurons\n",
    "        self.m[-1] = np.arange(\n",
    "            self.nin) if self.natural_ordering else rng.permutation(self.nin)\n",
    "        for l in range(L):\n",
    "            self.m[l] = rng.randint(\n",
    "                self.m[l - 1].min(), self.nin - 1, size=self.hidden_sizes[l])\n",
    "\n",
    "        # construct the mask matrices\n",
    "        masks = [self.m[l - 1][:, None] <= self.m[l][None, :]\n",
    "                 for l in range(L)]\n",
    "        masks.append(self.m[L - 1][:, None] < self.m[-1][None, :])\n",
    "\n",
    "        # handle the case where nout = nin * k, for integer k > 1\n",
    "        if self.nout > self.nin:\n",
    "            k = int(self.nout / self.nin)\n",
    "            # replicate the mask across the other outputs\n",
    "            masks[-1] = np.concatenate([masks[-1]] * k, axis=1)\n",
    "\n",
    "        # set the masks in all MaskedLinear layers\n",
    "        layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)]\n",
    "        for l, m in zip(layers, masks):\n",
    "            l.set_mask(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "\n",
    "class MadeService:\n",
    "\n",
    "    def __init__(self, task: str) -> None:\n",
    "        self._task = task\n",
    "\n",
    "    def run_made(self,\n",
    "                 model, opt,\n",
    "                 x_train, x_test,\n",
    "                 split, batch_size,\n",
    "                 samples, resample_every,\n",
    "                 device):\n",
    "\n",
    "        # enable/disable grad for efficiency of forwarding test batches\n",
    "        torch.set_grad_enabled(split == 'train')\n",
    "        model.train() if split == 'train' else model.eval()\n",
    "        nsamples = 1 if split == 'train' else samples\n",
    "        x = x_train if split == 'train' else x_test\n",
    "\n",
    "        if batch_size <= 0 or batch_size > len(x):\n",
    "            raise ValueError(\n",
    "                \"Batch size must be larger than 0 and smaller than sample size\")\n",
    "\n",
    "        N, D = x.size()\n",
    "        B = 64  # batch size\n",
    "        nsteps = math.ceil(N/B)\n",
    "\n",
    "        loss_for_samples = torch.full((N,), torch.nan).to(device)  # N x 1\n",
    "        loss_total = []\n",
    "\n",
    "        total_samples = 0\n",
    "        for step in range(nsteps):\n",
    "            # fetch the next batch of data\n",
    "            xb = Variable(x[step * B: step * B + B])\n",
    "            self._run_batch(step, B, N, model, opt, xb, split,\n",
    "                            nsamples, resample_every, loss_total, loss_for_samples)\n",
    "            total_samples += B\n",
    "\n",
    "        if total_samples < N:\n",
    "            # fetch the remaining data\n",
    "            xb = Variable(x[total_samples:])\n",
    "            self._run_batch(step + 1, B, N, model, opt, xb, split,\n",
    "                            nsamples, resample_every, loss_total, loss_for_samples)\n",
    "\n",
    "        assert not torch.isnan(loss_for_samples).any()\n",
    "        return sum(loss_total) / len(loss_total), loss_for_samples\n",
    "\n",
    "    def _run_batch(self, step, B, N,\n",
    "                   model, opt,\n",
    "                   xb, split,\n",
    "                   nsamples, resample_every,\n",
    "                   loss_total, loss_for_samples):\n",
    "\n",
    "        # get the logits, potentially run the same batch a number of times, resampling each time\n",
    "        xbhat = torch.zeros_like(xb)\n",
    "        for _ in range(nsamples):\n",
    "            # perform order/connectivity-agnostic training by resampling the masks\n",
    "            if step % resample_every == 0 or split == 'test':  # if in test, cycle masks every time\n",
    "                model.update_masks()\n",
    "            # forward the model\n",
    "            xbhat += model(xb)\n",
    "        xbhat /= nsamples\n",
    "\n",
    "        # evaluate the binary cross entropy loss\n",
    "        if self._task == COLOR_MNIST:\n",
    "            # Gaussian\n",
    "            pred = xbhat\n",
    "            loss_each = EvaluationUtils.mean_mse(pred, xb,\n",
    "                                                 reduction='none')  # batch_size x D\n",
    "            loss_sample = torch.mean(loss_each, dim=1)  # batch_size x 1\n",
    "            loss_mean = EvaluationUtils.mean_mse(pred, xb)  # 1 x 1\n",
    "        \n",
    "        # elif self._task == BINARIZED_MNIST:\n",
    "        else:\n",
    "\n",
    "            # Binary\n",
    "            # pred = torch.sigmoid(xbhat)\n",
    "            # loss_each = EvaluationUtils.mean_bce(pred, xb,\n",
    "            #                                      reduction='none')  # batch_size x D\n",
    "            # loss_sample = torch.mean(loss_each, dim=1)  # batch_size x 1\n",
    "            # loss_mean = EvaluationUtils.mean_bce(pred, xb)  # 1 x 1\n",
    "        \n",
    "        # else:\n",
    "            # Multinomial\n",
    "            pred = xbhat\n",
    "            loss_sample = EvaluationUtils.mean_ce(pred, xb,\n",
    "                                                  reduction='none')  # batch_size x 1\n",
    "            num_drugs_taken = torch.sum(xb, dim=1) # batch_size x 1\n",
    "            loss_sample = loss_sample / num_drugs_taken # batch_size x 1\n",
    "            loss_mean = torch.mean(loss_sample)  # 1 x 1\n",
    "\n",
    "        loss_total.append(loss_mean.item())\n",
    "        # probs_sample = torch.exp(-1 * loss_sample)\n",
    "        if step * B + B > N:\n",
    "            loss_for_samples[step * B:] = loss_sample\n",
    "        else:\n",
    "            loss_for_samples[step * B: step * B + B] = loss_sample\n",
    "\n",
    "        # backward/update\n",
    "        if split == 'train':\n",
    "            opt.zero_grad()\n",
    "            loss_mean.backward()\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m made_train_hist \u001b[39m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(made_epochs):\n\u001b[0;32m---> 17\u001b[0m     made_train_loss, _ \u001b[39m=\u001b[39m made_service\u001b[39m.\u001b[39;49mrun_made(made, made_opt,\n\u001b[1;32m     18\u001b[0m                                                x, \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     19\u001b[0m                                                \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, made_batch_size,\n\u001b[1;32m     20\u001b[0m                                                made_samples, resample_every, device)\n\u001b[1;32m     22\u001b[0m     made_scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m     made_train_hist\u001b[39m.\u001b[39mappend(made_train_loss)\n",
      "Cell \u001b[0;32mIn[17], line 39\u001b[0m, in \u001b[0;36mMadeService.run_made\u001b[0;34m(self, model, opt, x_train, x_test, split, batch_size, samples, resample_every, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nsteps):\n\u001b[1;32m     37\u001b[0m     \u001b[39m# fetch the next batch of data\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     xb \u001b[39m=\u001b[39m Variable(x[step \u001b[39m*\u001b[39m B: step \u001b[39m*\u001b[39m B \u001b[39m+\u001b[39m B])\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_batch(step, B, N, model, opt, xb, split,\n\u001b[1;32m     40\u001b[0m                     nsamples, resample_every, loss_total, loss_for_samples)\n\u001b[1;32m     41\u001b[0m     total_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m B\n\u001b[1;32m     43\u001b[0m \u001b[39mif\u001b[39;00m total_samples \u001b[39m<\u001b[39m N:\n\u001b[1;32m     44\u001b[0m     \u001b[39m# fetch the remaining data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 107\u001b[0m, in \u001b[0;36mMadeService._run_batch\u001b[0;34m(self, step, B, N, model, opt, xb, split, nsamples, resample_every, loss_total, loss_for_samples)\u001b[0m\n\u001b[1;32m    105\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    106\u001b[0m loss_mean\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> 107\u001b[0m opt\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[0;32m~/Documents/Project/FedWeight/FedWeight_eICU/venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Project/FedWeight/FedWeight_eICU/venv/lib/python3.8/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Project/FedWeight/FedWeight_eICU/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/Project/FedWeight/FedWeight_eICU/venv/lib/python3.8/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Documents/Project/FedWeight/FedWeight_eICU/venv/lib/python3.8/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/Documents/Project/FedWeight/FedWeight_eICU/venv/lib/python3.8/site-packages/torch/optim/adam.py:263\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    262\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 263\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n\u001b[1;32m    266\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "hidden_list = list(map(int, made_hiddens.split(',')))\n",
    "made = MADE(x.size(1), hidden_list,\n",
    "            x.size(1), num_masks=num_masks,\n",
    "            natural_ordering=natural_ordering)\n",
    "made_service = MadeService(task)\n",
    "made_opt = torch.optim.Adam(made.parameters(),\n",
    "                            lr=made_learning_rate,\n",
    "                            weight_decay=made_weight_decay)\n",
    "made_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    made_opt, step_size=45, gamma=0.1)\n",
    "\n",
    "made_train_hist = []\n",
    "\n",
    "for epoch in range(made_epochs):\n",
    "    made_train_loss, _ = made_service.run_made(made, made_opt,\n",
    "                                               x_train, None,\n",
    "                                               'train', made_batch_size,\n",
    "                                               made_samples, resample_every, device)\n",
    "    \n",
    "    made_scheduler.step()\n",
    "    made_train_hist.append(made_train_loss)\n",
    "\n",
    "test_loss, _ = made_service.run_made(made, made_opt,\n",
    "                                     None, x_test,\n",
    "                                     'test', made_batch_size,\n",
    "                                     made_samples, resample_every, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_epochs: 150\n",
    "vae_latent_dim: 8\n",
    "vae_hiddens: \"1024,512,256\"\n",
    "vae_learning_rate: 0.0001\n",
    "vae_weight_decay: 0.001\n",
    "vae_batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "\n",
    "    \"\"\" Initialize \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int,\n",
    "                 hidden_sizes: str,\n",
    "                 latent_dim: int) -> None:\n",
    "\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        hidden_list = list(map(int, hidden_sizes.split(',')))\n",
    "\n",
    "        # Encoder layers\n",
    "        self._encoder = []\n",
    "        encoder_hiddens = [in_features] + hidden_list\n",
    "        for h0, h1 in zip(encoder_hiddens, encoder_hiddens[1:]):\n",
    "            self._encoder.extend([\n",
    "                nn.Linear(h0, h1),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self._encoder = nn.Sequential(*self._encoder)\n",
    "\n",
    "        # Latent space layers\n",
    "        self._mu_layer = nn.Linear(encoder_hiddens[-1], latent_dim)\n",
    "        self._logvar_layer = nn.Linear(encoder_hiddens[-1], latent_dim)\n",
    "\n",
    "        # Decoder layers\n",
    "        self._decoder = []\n",
    "        # decoder_hiddens = [latent_dim] + encoder_hiddens[::-1]\n",
    "\n",
    "        decoder_hiddens = [latent_dim, 64, 1408]\n",
    "\n",
    "        for h0, h1 in zip(decoder_hiddens, decoder_hiddens[1:]):\n",
    "            self._decoder.extend([\n",
    "                nn.Linear(h0, h1),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self._decoder.pop()  # pop the last ReLU for the output layer\n",
    "        self._decoder.pop()  # pop the last Dropout for the output layer\n",
    "        self._decoder = nn.Sequential(*self._decoder)\n",
    "\n",
    "        print(\"Encoder: {}\".format(self._encoder))\n",
    "        print(\"Mu layer: {}\".format(self._mu_layer))\n",
    "        print(\"Log var layer: {}\".format(self._logvar_layer))\n",
    "        print(\"Decoder: {}\".format(self._decoder))\n",
    "\n",
    "    \"\"\" Public method \"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        mu, logvar = self._encode(x)\n",
    "        z = self._reparameterize(mu, logvar)\n",
    "        reconstructed = self._decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "    \"\"\" Private method \"\"\"\n",
    "\n",
    "    def _encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        hidden = self._encoder(x)\n",
    "        mu = self._mu_layer(hidden)\n",
    "        logvar = self._logvar_layer(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "    def _reparameterize(self, mu: torch.Tensor,\n",
    "                        logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def _decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        reconstructed = self._decoder(z)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "\n",
    "class VaeService:\n",
    "\n",
    "    def __init__(self, task: str) -> None:\n",
    "        self._task = task\n",
    "\n",
    "    def run_vae(self,\n",
    "                model, opt,\n",
    "                x_train, x_test,\n",
    "                split, batch_size,\n",
    "                beta, device):\n",
    "\n",
    "        torch.set_grad_enabled(split == 'train')\n",
    "        model.train() if split == 'train' else model.eval()\n",
    "        x = x_train if split == 'train' else x_test\n",
    "\n",
    "        if batch_size <= 0 or batch_size > len(x):\n",
    "            raise ValueError(\n",
    "                \"Batch size must be larger than 0 and smaller than sample size\")\n",
    "\n",
    "        N, D = x.size()\n",
    "        B = 64  # batch size\n",
    "        nsteps = math.ceil(N/B)\n",
    "\n",
    "        loss_for_samples = torch.full((N,), torch.nan).to(device)  # N x 1\n",
    "        loss_total = []\n",
    "\n",
    "        total_samples = 0\n",
    "        for step in range(nsteps):\n",
    "            # fetch the next batch of data\n",
    "            xb = Variable(x[step * B: step * B + B])\n",
    "            self._run_batch(step, B, N, model, opt, xb, split, beta,\n",
    "                            loss_total, loss_for_samples)\n",
    "            total_samples += B\n",
    "\n",
    "        if total_samples < N:\n",
    "            # fetch the remaining data\n",
    "            xb = Variable(x[total_samples:])\n",
    "            self._run_batch(step + 1, B, N, model, opt, xb, split, beta,\n",
    "                            loss_total, loss_for_samples)\n",
    "\n",
    "        assert not torch.isnan(loss_for_samples).any()\n",
    "        return sum(loss_total) / len(loss_total), loss_for_samples\n",
    "\n",
    "    def _run_batch(self, step, B, N,\n",
    "                   model, opt,\n",
    "                   xb, split, beta,\n",
    "                   loss_total, loss_for_samples):\n",
    "\n",
    "        pred, mu, logvar = model(xb)\n",
    "\n",
    "        if self._task == COLOR_MNIST:\n",
    "            # Gaussian\n",
    "            loss_each = EvaluationUtils.mean_mse(pred, xb,\n",
    "                                                 reduction='none')  # batch_size x D\n",
    "            loss_sample = torch.mean(loss_each, dim=1)  # batch_size x 1\n",
    "            kl_sample = -0.5 * \\\n",
    "                torch.sum(1 + logvar - mu.pow(2) -\n",
    "                          logvar.exp(), dim=1)  # batch_size x 1\n",
    "            \n",
    "            loss_vae = torch.sum(loss_sample + beta * kl_sample)  # 1 x 1\n",
    "\n",
    "            loss_sample += kl_sample  # batch_size x 1\n",
    "\n",
    "        else:\n",
    "            # Multinomial\n",
    "            loss_sample = EvaluationUtils.mean_ce(pred, xb,\n",
    "                                                  reduction='none')  # batch_size x 1\n",
    "            num_drugs_taken = torch.sum(xb, dim=1)  # batch_size x 1\n",
    "            loss_sample = loss_sample / num_drugs_taken # batch_size x 1\n",
    "            \n",
    "            # batch_size x 1\n",
    "            kl_sample = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)  # batch_size x 1\n",
    "\n",
    "            loss_vae = torch.sum(loss_sample + beta * kl_sample) # 1 x 1\n",
    "\n",
    "            loss_sample += kl_sample  # batch_size x 1\n",
    "\n",
    "        loss_total.append(loss_vae.item())\n",
    "        # probs_sample = torch.exp(-1 * loss_sample)\n",
    "        if step * B + B > N:\n",
    "            loss_for_samples[step * B:] = loss_sample\n",
    "        else:\n",
    "            loss_for_samples[step * B: step * B + B] = loss_sample\n",
    "\n",
    "        # backward/update\n",
    "        if split == 'train':\n",
    "            opt.zero_grad()\n",
    "            loss_vae.backward()\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE\n",
    "vae = VAE(x.size(1),\n",
    "          vae_hiddens,\n",
    "          vae_latent_dim)\n",
    "vae.to(device)\n",
    "vae_opt = torch.optim.Adam(vae.parameters(),\n",
    "                            lr=vae_learning_rate,\n",
    "                            weight_decay=vae_weight_decay)\n",
    "vae_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    vae_opt, step_size=45, gamma=0.1)\n",
    "\n",
    "vae_service = VaeService(task)\n",
    "\n",
    "vae_train_hist = []\n",
    "\n",
    "for epoch in range(vae_epochs):\n",
    "    beta = 0.002 * epoch\n",
    "    beta = beta if beta < 1.0 else 1.0\n",
    "    vae_train_loss, _ = vae_service.run_vae(vae, vae_opt,\n",
    "                                            x_train, None,\n",
    "                                            'train', vae_batch_size,\n",
    "                                            beta, device)\n",
    "\n",
    "    vae_scheduler.step()\n",
    "    vae_train_hist.append(vae_train_loss)\n",
    "\n",
    "vae_test_loss, _ = vae_service.run_vae(vae, vae_opt,\n",
    "                                       None, x_test,\n",
    "                                       'test', vae_batch_size,\n",
    "                                       beta, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings: int,\n",
    "                 embedding_dim: int,\n",
    "                 beta: float,\n",
    "                 device: torch.device) -> None:\n",
    "\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "\n",
    "        self._num_embeddings = num_embeddings  # K\n",
    "        self._embedding_dim = embedding_dim  # D\n",
    "        self._beta = beta  # Commitment loss coefficient\n",
    "        self._decay = 0.99\n",
    "        self._epsilon = 1e-5\n",
    "        self.register_buffer('_ema_cluster_size',\n",
    "                             torch.zeros(num_embeddings))  # 1 x K\n",
    "        self._ema_w = nn.Parameter(torch.Tensor(\n",
    "            num_embeddings, self._embedding_dim))  # K x D\n",
    "        self._ema_w.data.normal_()\n",
    "        self._device = device\n",
    "\n",
    "        # Codebook\n",
    "        self._embedding = nn.Embedding(self._num_embeddings,  # K x D\n",
    "                                       self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1 / self._num_embeddings,  # 1/K to make sure the integral of PDF is 1\n",
    "                                             1 / self._num_embeddings)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "\n",
    "        # Euclidean distances\n",
    "        # print(inputs.shape)  # N x D\n",
    "\n",
    "        distances = (torch.sum(inputs**2, dim=1, keepdim=True)  # N x K\n",
    "                     + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                     - 2 * torch.matmul(inputs, self._embedding.weight.t()))\n",
    "\n",
    "        # Latent representation z = q(z|x) e.g. [0 0 1 0 0]\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)  # N x 1\n",
    "        latent = torch.zeros(encoding_indices.shape[0],  # N x K\n",
    "                             self._num_embeddings,\n",
    "                             device=self._device)\n",
    "        latent.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Codeword\n",
    "        codeword = torch.matmul(latent, self._embedding.weight)  # N x D\n",
    "\n",
    "        # EMA\n",
    "        if self.training:\n",
    "            # torch.sum(latent, 0): observed cluster size per codeword (1 x K)\n",
    "            # self._ema_cluster_size: smoothed cluster size per codeword\n",
    "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
    "                (1 - self._decay) * torch.sum(latent, 0)  # 1 x K\n",
    "\n",
    "            # Laplace smoothing of the cluster size\n",
    "            n = torch.sum(self._ema_cluster_size.data)\n",
    "            self._ema_cluster_size = (\n",
    "                (self._ema_cluster_size + self._epsilon)\n",
    "                / (n + self._num_embeddings * self._epsilon) * n)\n",
    "\n",
    "            # Row of latent.T: Binary vector for codeword neighbours\n",
    "            # 1: the encoder output is close to codeword k (inside cluster of centroid k)\n",
    "            # 0: the encoder output is not close to codeword k\n",
    "            # Column of inputs: Encoder output by different samples\n",
    "            # dw: Sum of encoder outputs close to codeword k\n",
    "            # self._ema_w: Smoothed sum of encoder outputs close to codeword k\n",
    "            dw = latent.T @ inputs  # K x D\n",
    "            self._ema_w = nn.Parameter(\n",
    "                self._ema_w * self._decay + (1 - self._decay) * dw)\n",
    "            self._embedding.weight = nn.Parameter(\n",
    "                self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
    "\n",
    "        # Loss\n",
    "        # Freeze encoder output, train codebook to make sure codeword close to encoder output\n",
    "        # codebook_loss = F.mse_loss(codeword, inputs.detach()) # No need if using EMA!\n",
    "        # Freeze codeword, train encoder to make sure encoder output close to codeword\n",
    "        commitment_loss = F.mse_loss(codeword.detach(), inputs)\n",
    "        # loss = codebook_loss + self._beta * commitment_loss # No need if using EMA!\n",
    "        loss = self._beta * commitment_loss\n",
    "\n",
    "        # Copy gradients of codeword to inputs\n",
    "        codeword = inputs + (codeword - inputs).detach()\n",
    "\n",
    "        return loss, codeword\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features: int,\n",
    "                 hidden_sizes: str,\n",
    "                 latent_dim: int,\n",
    "                 beta: float,\n",
    "                 device: torch.device) -> None:\n",
    "\n",
    "        super(VQVAE, self).__init__()\n",
    "\n",
    "        hidden_list = list(map(int, hidden_sizes.split(',')))\n",
    "\n",
    "        # Encoder layers\n",
    "        self._encoder = []\n",
    "        encoder_hiddens = [in_features] + hidden_list\n",
    "        for h0, h1 in zip(encoder_hiddens, encoder_hiddens[1:]):\n",
    "            self._encoder.extend([\n",
    "                nn.Linear(h0, h1),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self._encoder = nn.Sequential(*self._encoder)\n",
    "\n",
    "        self._vq = VectorQuantizer(latent_dim,\n",
    "                                   encoder_hiddens[-1],\n",
    "                                   beta, device)\n",
    "\n",
    "        # Decoder layers\n",
    "        self._decoder = []\n",
    "        decoder_hiddens = encoder_hiddens[::-1]\n",
    "        for h0, h1 in zip(decoder_hiddens, decoder_hiddens[1:]):\n",
    "            self._decoder.extend([\n",
    "                nn.Linear(h0, h1),\n",
    "                nn.ReLU(),\n",
    "            ])\n",
    "        self._decoder.pop()  # pop the last ReLU for the output layer\n",
    "        self._decoder = nn.Sequential(*self._decoder)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        z = self._encoder(x)\n",
    "        loss, codeword = self._vq(z)\n",
    "        reconstructed = self._decoder(codeword)\n",
    "\n",
    "        return reconstructed, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "\n",
    "class VqVaeService:\n",
    "\n",
    "    def __init__(self, task: str) -> None:\n",
    "        super(VqVaeService, self).__init__()\n",
    "        self._task = task\n",
    "\n",
    "    def run_vqvae(self,\n",
    "                  model, opt,\n",
    "                  x_train, x_test,\n",
    "                  split, batch_size,\n",
    "                  device):\n",
    "\n",
    "        torch.set_grad_enabled(split == 'train')\n",
    "        model.train() if split == 'train' else model.eval()\n",
    "        x = x_train if split == 'train' else x_test\n",
    "\n",
    "        if batch_size <= 0 or batch_size > len(x):\n",
    "            raise ValueError(\n",
    "                \"Batch size must be larger than 0 and smaller than sample size\")\n",
    "\n",
    "        N, D = x.size()\n",
    "        B = 64  # batch size\n",
    "        nsteps = math.ceil(N/B)\n",
    "\n",
    "        loss_for_samples = torch.full((N,), torch.nan).to(device)  # N x 1\n",
    "        loss_total = []\n",
    "\n",
    "        total_samples = 0\n",
    "        for step in range(nsteps):\n",
    "            # fetch the next batch of data\n",
    "            xb = Variable(x[step * B: step * B + B])\n",
    "            self._run_batch(step, B, N, model, opt, xb, split,\n",
    "                            loss_total, loss_for_samples)\n",
    "            total_samples += B\n",
    "\n",
    "        if total_samples < N:\n",
    "            # fetch the remaining data\n",
    "            xb = Variable(x[total_samples:])\n",
    "            self._run_batch(step + 1, B, N, model, opt, xb, split,\n",
    "                            loss_total, loss_for_samples)\n",
    "\n",
    "        assert not torch.isnan(loss_for_samples).any()\n",
    "        return sum(loss_total) / len(loss_total), loss_for_samples\n",
    "\n",
    "    def _run_batch(self, step, B, N,\n",
    "                   model, opt,\n",
    "                   xb, split,\n",
    "                   loss_total,\n",
    "                   loss_for_samples):\n",
    "\n",
    "        pred, vq_loss = model(xb)\n",
    "\n",
    "        if self._task == COLOR_MNIST:\n",
    "            # Gaussian\n",
    "            loss_each = EvaluationUtils.mean_mse(pred, xb,\n",
    "                                                 reduction='none')  # batch_size x D\n",
    "            loss_sample = torch.mean(loss_each, dim=1)  # batch_size x 1\n",
    "\n",
    "            reconstruction_loss = EvaluationUtils.mean_mse(pred, xb,\n",
    "                                                           reduction='sum')\n",
    "            loss_vae = reconstruction_loss + vq_loss  # 1 x 1\n",
    "\n",
    "        else:\n",
    "            # Multinomial\n",
    "            loss_sample = EvaluationUtils.mean_ce(pred, xb,\n",
    "                                                  reduction='none')  # batch_size x 1\n",
    "            num_drugs_taken = torch.sum(xb, dim=1)  # batch_size x 1\n",
    "            loss_sample = loss_sample / num_drugs_taken  # batch_size x 1\n",
    "\n",
    "            reconstruction_loss = EvaluationUtils.mean_ce(pred, xb,\n",
    "                                                          reduction='sum')\n",
    "            loss_vae = reconstruction_loss + vq_loss  # 1 x 1\n",
    "\n",
    "        loss_total.append(loss_vae.item())\n",
    "        # probs_sample = torch.exp(-1 * loss_sample)\n",
    "        if step * B + B > N:\n",
    "            loss_for_samples[step * B:] = loss_sample\n",
    "        else:\n",
    "            loss_for_samples[step * B: step * B + B] = loss_sample\n",
    "\n",
    "        # backward/update\n",
    "        if split == 'train':\n",
    "            opt.zero_grad()\n",
    "            loss_vae.backward()\n",
    "            opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqvae = VQVAE(x.size(1), vae_hiddens, vae_latent_dim, 0.25, device)\n",
    "\n",
    "vqvae.to(device)\n",
    "vqvae_opt = torch.optim.Adam(vqvae.parameters(),\n",
    "                             lr=vae_learning_rate,\n",
    "                             weight_decay=vae_weight_decay)\n",
    "vqvae_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    vqvae_opt, step_size=45, gamma=0.1)\n",
    "\n",
    "vqvae_service = VqVaeService(task)\n",
    "\n",
    "vqvae_train_hist = []\n",
    "for epoch in range(vae_epochs):\n",
    "    vqvae_train_loss, _ = vqvae_service.run_vqvae(vqvae, vqvae_opt,\n",
    "                                                  x_train, None,\n",
    "                                                  'train', vae_batch_size,\n",
    "                                                  device)\n",
    "\n",
    "    vqvae_scheduler.step()\n",
    "    vqvae_train_hist.append(vqvae_train_loss)\n",
    "\n",
    "vqvae_test_loss, _ = vqvae_service.run_vqvae(vqvae, vqvae_opt,\n",
    "                                             x_test, None,\n",
    "                                             'test', vae_batch_size,\n",
    "                                             device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "16a4581469c1582d47fb7c1169ef3415a3b6f89c53cd78841fa7057d11ccb36f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
